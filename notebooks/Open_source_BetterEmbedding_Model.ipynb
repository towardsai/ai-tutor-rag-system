{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D7VHrzOaj6zn"
   },
   "source": [
    "**Install Packages and Setup Variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AymSbJC_2WS-",
    "outputId": "6da0c155-7994-442c-e9d1-20d135ae21e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/56.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m40.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m337.0/337.0 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.4/177.4 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m38.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m584.3/584.3 kB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m60.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.5/15.5 MB\u001b[0m \u001b[31m62.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m171.5/171.5 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m273.8/273.8 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.2/139.2 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.5/93.5 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m63.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.7/150.7 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m679.1/679.1 kB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m44.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m41.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.1/211.1 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m57.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.0/64.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.5/52.5 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.7/149.7 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m60.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.9/37.9 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m68.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.5/12.5 MB\u001b[0m \u001b[31m60.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.7/410.7 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m180.2/180.2 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m861.9/861.9 kB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.3/216.3 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.8/295.8 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m229.9/229.9 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.7/82.7 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.4/71.4 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.6/97.6 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m476.0/476.0 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m49.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m425.7/425.7 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m157.3/157.3 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.6/137.6 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for html2text (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for tinysegmenter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for spider-client (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for feedfinder2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for jieba3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "!pip install -q llama-index==0.10.57 openai==1.37.0 llama-index-finetuning llama-index-embeddings-huggingface llama-index-embeddings-cohere llama-index-readers-web cohere==5.6.2 tiktoken==0.7.0 chromadb==0.5.5 html2text sentence_transformers pydantic llama-index-vector-stores-chroma==0.1.10 llama-index-llms-gemini==0.1.11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3IW-a9ql3FkD"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set the following API Keys in the Python environment. Will be used later.\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"<YOUR_OPENAI_KEY>\"\n",
    "os.environ[\"CO_API_KEY\"] = \"<YOUR_COHERE_KEY>\"\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"<YOUR_OPENAI_KEY>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9TUa1W2H3K47"
   },
   "outputs": [],
   "source": [
    "# Allows running asyncio in environments with an existing event loop, like Jupyter notebooks.\n",
    "\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FAt2v0Qik7sY"
   },
   "source": [
    "**Load a Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Myy9yddv4mzH"
   },
   "outputs": [],
   "source": [
    "from llama_index.llms.gemini import Gemini\n",
    "\n",
    "llm = Gemini(model=\"models/gemini-1.5-flash\", temperature=1, max_tokens=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CD0capZYk-pR"
   },
   "source": [
    "**Create a VectoreStore**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OYoQPGt13OcZ"
   },
   "outputs": [],
   "source": [
    "import chromadb\n",
    "# create client and a new collection\n",
    "# chromadb.EphemeralClient saves data in-memory.\n",
    "chroma_client = chromadb.PersistentClient(path=\"./mini-llama-articles\")\n",
    "chroma_collection = chroma_client.create_collection(\"mini-llama-articles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fbcJORQ33QnC"
   },
   "outputs": [],
   "source": [
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "\n",
    "# Define a storage context object using the created vector database.\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jVkdYWSFlCEC"
   },
   "source": [
    "**Load the Dataset (CSV)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FIgXvo8alD_F"
   },
   "source": [
    "Download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Ag4Bbj1lGf-"
   },
   "source": [
    "The dataset includes several articles from the TowardsAI blog, which provide an in-depth explanation of the LLaMA2 model. Read the dataset as a long string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N8kT2wE-Cv-N",
    "outputId": "a1f8dd1f-47d2-48f3-8c69-0b0585e76a2b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  169k  100  169k    0     0   225k      0 --:--:-- --:--:-- --:--:--  225k\n"
     ]
    }
   ],
   "source": [
    "!curl -o ./mini-llama-articles.csv https://raw.githubusercontent.com/AlaFalaki/tutorial_notebooks/main/data/mini-llama-articles.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-wYChJNylJ_W"
   },
   "source": [
    "**Read File**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jZR7v8er3RIc",
    "outputId": "e9ad0d15-df47-4c41-9699-27acc894f2af"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "rows = []\n",
    "\n",
    "# Load the file as a JSON\n",
    "with open(\"./mini-llama-articles.csv\", mode=\"r\", encoding=\"utf-8\") as file:\n",
    "    csv_reader = csv.reader(file)\n",
    "\n",
    "    for idx, row in enumerate(csv_reader):\n",
    "        if idx == 0:\n",
    "            continue\n",
    "            # Skip header row\n",
    "        rows.append(row)\n",
    "\n",
    "# The number of characters in the dataset.\n",
    "len(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WPjA1Bh1lM2_"
   },
   "source": [
    "**Convert to Document obj**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J9K7W4Tq3TRL"
   },
   "outputs": [],
   "source": [
    "from llama_index.core import Document\n",
    "\n",
    "# Convert the chunks to Document objects so the LlamaIndex framework can process them.\n",
    "documents = [\n",
    "    Document(\n",
    "        text=row[1], metadata={\"title\": row[0], \"url\": row[2], \"source_name\": row[3]}\n",
    "    )\n",
    "    for row in rows\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "njTHI-UHlPHX"
   },
   "source": [
    "**Transforming**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vUklZVhl3WC0"
   },
   "outputs": [],
   "source": [
    "from llama_index.core.text_splitter import TokenTextSplitter\n",
    "\n",
    "# Define the splitter object that split the text into segments with 512 tokens,\n",
    "# with a 128 overlap between the segments.\n",
    "text_splitter = TokenTextSplitter(separator=\" \", chunk_size=512, chunk_overlap=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hc7vMQxOlR27"
   },
   "source": [
    "There are two options to use the Cohere embeddings:\n",
    "\n",
    "- input_type=\"search_document\": Employ this option for texts (documents) intended for storage in your vector database.\n",
    "\n",
    "- input_type=\"search_query\": Use this when issuing search queries to locate the most related documents within your vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AJOW_6NW3WzP"
   },
   "outputs": [],
   "source": [
    "from llama_index.core.extractors import (\n",
    "    SummaryExtractor,\n",
    "    QuestionsAnsweredExtractor,\n",
    "    KeywordExtractor,\n",
    ")\n",
    "\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core.ingestion import IngestionPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 577,
     "referenced_widgets": [
      "b659b80e7f674e39adefecc9a9403dba",
      "6ff3c217c0074d98bb0936be96b8b2ae",
      "eecae025607d4da9879110167e2f41f5",
      "cb3d3cb50ff44faa8b0107fa2fdfc131",
      "f2ff6a7a51a84e2893d8e56d934345cd",
      "aee62996089d4a2d886f53600729d31a",
      "23a0d6da485440c09a79b52cf7b87c92",
      "558821f69c924a929bb4aa7bc672db2f",
      "50b48dd96d3d4244a00e7e3ff978d270",
      "b5b4f9cdfadd46559583dd96c43f40b8",
      "27ba3924a8714ab49d537dd35084948c",
      "58ad3cc6f74646a090ee3165b0c5abfb",
      "1d63c804c97a48cca5f297e9f153c8c7",
      "65a202d7565d4ee1b97e99c1ff032f9d",
      "a50fd31c1d604b549b1e0c79b2308f4b",
      "86ee7fc2b860429b93b0f043bdbf956a",
      "0896f40eaeb247e386dfc7641389f68b",
      "1937ebfef6ce4f3eb6510a8db1a3e69e",
      "77439b18ca5d43f0965fbf80aa3ecff4",
      "8c07108924f5432f9575e1b153a05138",
      "41b08666b17641cca4c493a59bdc3b38",
      "e7d300bd11fe46b79af81139557971a5",
      "ab576db24a2347fc995bb7cdbc1634b1",
      "e104bc0faecb46ebb81273dc207ec0df",
      "7dbd7a14b6fd4a6a9054bffac67e8cbd",
      "8015eec933114eb38acb3c31ed9da533",
      "5c43b5893f2c4f8b826ffffe9c72184f",
      "e3ed1bed0a514359969d9a99309f475b",
      "3c33216225d1463e9240c947a2e723a3",
      "bdec7a54ce0c4622b23d525d89445684",
      "8d18a30d9401477bab76201df30d264e",
      "353a2e0df97e4ff3b65f651389ac3841",
      "8ee2be75e52945918c9b35ea46501dea",
      "24503f7d0fe44791a801662e4d97a810",
      "faf37ab2c4654a0db8b61e02a8d3fd3e",
      "8ea046399e814210ba1dbd9e261d6c25",
      "a300029c0d7c4cf181a3c0f920f3b22b",
      "b4a16ca31f4c4d8aac0aead5ac0c037c",
      "165eb83a7e674371a9c5bbdaa4bd99b4",
      "ed882b7013c24ee49081749705ebdc83",
      "c614c894f8904dfb9785ac29a64a6468",
      "a2750034472e415db7ac2c1b738100e8",
      "e6faa7457a964a72b257553300f4b19e",
      "68294689e4bf4f8c958a5125054cd0ef",
      "f0db8353e7f945e7b3afd2385239e9e3",
      "3a669bc68f084bae840ba66217da8e78",
      "e6353eeda07c4fdaa1691d3f3af4a930",
      "57597f6fc0db4d2cb43db42b80db69cf",
      "35286fea7e51491ea8511c08f6093fb4",
      "c203cd454ca840a9a3e31e19ee3785da",
      "541419ee534c435e883ac4814f6c7422",
      "940c40c269bc4731a1713f44ffc1152d",
      "7a892bfbd88848558704b49eb2238354",
      "1864164e6f0b4e18a76860dda3c9b62d",
      "d887265ce2d342ee96722ac676ec6784",
      "78b9b58488bb4590827083cd12e6a6b8",
      "96cec49d344a4ca69283971b3b074925",
      "b49c0c8d52db482484704cf936eb12b6",
      "4e3fe6b8ed614a44a4d69c2259828c5f",
      "bbe98d82ef32476281251150591f4e8d",
      "1597316e09fe4c20b955fd2b258937e0",
      "4fff5e3fc9494a868afbb175dbed05cc",
      "107741c768c94e0ca503964e190e7616",
      "e487e71e5b4044388df8afde46f841a2",
      "10cbcc7b69554b1e8322c1ae18155ec6",
      "7e4a7c1de8514cf0ab71789f0cbeb042",
      "0a118b3677094b629cc159f95ae6d1d2",
      "6526c8663eae41288f9052b6a6ce7a3f",
      "e124c0e79241479db3c479339955dc3e",
      "0555e085c9a1483f8e668114624136ec",
      "30495514d63f4c19831a2660f4f92c61",
      "f91e906f9b70420e9567badf70dd803e",
      "51cb60f2eec049eeb038176aa04a5e00",
      "6d13319d2ba648d3b19a8e1997907996",
      "3409630b23114cd2b2783a0689d2053d",
      "f4f30accf0884e04baed7121c1201def",
      "5a46117550d7463387ac44d5f61a46ef",
      "81ab95b448ff4e858f590d405408be21",
      "22684a7ebaf44084968d589cb9db65b8",
      "f1d1f0afbc384762997dcf30d4d5c7b3",
      "63f32d8f11884321bf1c9998ff8056c9",
      "a0cee90b71104ed696779b6a3504d90e",
      "5325bbe592754971bf6ab9e5e3279134",
      "5df8f72e8d28419c9e96d68b111a556d",
      "a6f54756222c4e949291d8f9fdf87d1d",
      "fe6fa7990928443a8756100611b9d8f8",
      "5237bc589cb649639648465c8c699e3c",
      "6885ca0dab2149299c50a61e743b3b2e",
      "20cb16df4fef48d1ab2bdd4a2c9739ad",
      "4302f47996ce4ff6bcc50e44352a3ed4",
      "caee96435b4948a895ed43296beb1b25",
      "f63075eb4d9c4a46bb029dd298868488",
      "94cd5cf0c4a940b586bd2767828afae7",
      "fc43e8b46a244e4cb2ac94e148b9d37a",
      "8dd4e02f49ba40e49d2021833975a1b7",
      "7d96a7651a9b447cab72e0350271637c",
      "eb4b621067944da0aadfc4531dea294f",
      "1d5d8472e80b41468d462cbfdaac3c3e",
      "f114a6865e1b481ba2e6bed8e7e8f682",
      "bf659ceb43c341dea36f76c061a6408f",
      "9b350b7530664e6291670299236c91b6",
      "f56f1e80db4244c28f6f1a238ad8f0d2",
      "75e13a5e6a534ea1afca50a50d51d6d1",
      "9c3854f0889841da92f711d2903e61fd",
      "da8c5a92bd534ed399da486daed2211f",
      "07f9cd2796ac4bbdb8975b984cb46aff",
      "be4ec6778f6b4bda95aefff3a2f1b2fa",
      "d63ee69f099b4266b8ce7b5a1dc53848",
      "b35fce1993904794b21c451c8cae223b",
      "73cb867b4fe14b05a433957edbb8d64d",
      "5adc3bc1c5da4d0dae4aa53c2eb69cfb",
      "0a93707ba3174d66a2f70d8dc3cea285",
      "e48f095ad218416fa1d81af61d22fbfc",
      "b7d12e349c8b4b709116512a4b478e05",
      "cb8560abe5354693b170a0a759bdfeb2",
      "5ae96baffffb4eb4ac7d2c30cae1c61e",
      "5e18c5e00102433ca44745bf6e5a50ef",
      "33378395f484405b993d38ab5cd18d51",
      "6de4b3939dd448e2850ed54a58e9463f",
      "4ae46893f7324e02a08e086a21d2f2aa",
      "85c43506c5ce4ca383675ec8cf03c920",
      "e5b454fabede441a838e5175fb626860",
      "659cb635cdbf4a02a51b848e41ea1307",
      "cb330b5bd53048c3a215fd765f09d8eb",
      "75a19dd9b6e8443ea306eb69d34bbafa",
      "8ff2f535c3ca4db7a0c2129823326fda",
      "e05f2d0f5a4f4f93a419cc8572770f90",
      "86f38f66ad7c4792be73939160c3f5f9",
      "05bf5c13a32f45fc88d12e0ce910776e",
      "bd6097865f0947ae8b187b9d76d67fda",
      "5e901763f6b84c7a91f03f733b7b1af6",
      "c3f191c8a3554e2ab616c893bc0f5548"
     ]
    },
    "id": "XUPLSGEW3Y-N",
    "outputId": "b604e81d-c07f-478c-86d0-ca938d8d6c3f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b659b80e7f674e39adefecc9a9403dba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/387 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58ad3cc6f74646a090ee3165b0c5abfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/67.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab576db24a2347fc995bb7cdbc1634b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/57.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24503f7d0fe44791a801662e4d97a810",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/615 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0db8353e7f945e7b3afd2385239e9e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/133M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78b9b58488bb4590827083cd12e6a6b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/314 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a118b3677094b629cc159f95ae6d1d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81ab95b448ff4e858f590d405408be21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20cb16df4fef48d1ab2bdd4a2c9739ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf659ceb43c341dea36f76c061a6408f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/200 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5adc3bc1c5da4d0dae4aa53c2eb69cfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 108/108 [05:11<00:00,  2.89s/it] \n",
      "100%|██████████| 108/108 [05:09<00:00,  2.87s/it] \n",
      "100%|██████████| 108/108 [02:20<00:00,  1.30s/it] \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5b454fabede441a838e5175fb626860",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/108 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create the pipeline to apply the transformation on each chunk,\n",
    "# and store the transformed text in the chroma vector store.\n",
    "\n",
    "pipeline = IngestionPipeline(\n",
    "    transformations=[\n",
    "        text_splitter,\n",
    "        QuestionsAnsweredExtractor(questions=3, llm=llm),\n",
    "        SummaryExtractor(summaries=[\"prev\", \"self\"], llm=llm),\n",
    "        KeywordExtractor(keywords=10, llm=llm),\n",
    "        HuggingFaceEmbedding(model_name=\"intfloat/e5-small-v2\"),\n",
    "    ],\n",
    "    vector_store=vector_store,\n",
    ")\n",
    "\n",
    "# Run the transformation pipeline.\n",
    "nodes = pipeline.run(documents=documents, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QHESGfky3a2U",
    "outputId": "89c703d6-ecf7-4cf3-aacb-2e3f1b93aaf9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "108"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nO3OOO62WvzK",
    "outputId": "1243f620-fa1c-44ae-b927-6793e66bc364"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nodes[0].embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MHW9joSgW1Ib",
    "outputId": "55c956aa-deca-43ef-a04c-926b6a76105d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: mini-llama-articles/ (stored 0%)\n",
      "  adding: mini-llama-articles/20b69b3f-4c2f-4475-8b85-8b715af14bef/ (stored 0%)\n",
      "  adding: mini-llama-articles/20b69b3f-4c2f-4475-8b85-8b715af14bef/length.bin (deflated 59%)\n",
      "  adding: mini-llama-articles/20b69b3f-4c2f-4475-8b85-8b715af14bef/header.bin (deflated 61%)\n",
      "  adding: mini-llama-articles/20b69b3f-4c2f-4475-8b85-8b715af14bef/data_level0.bin (deflated 7%)\n",
      "  adding: mini-llama-articles/20b69b3f-4c2f-4475-8b85-8b715af14bef/link_lists.bin (stored 0%)\n",
      "  adding: mini-llama-articles/chroma.sqlite3 (deflated 74%)\n"
     ]
    }
   ],
   "source": [
    "!zip -r vectorstore_cohere.zip mini-llama-articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KQ6LLEGulhmo"
   },
   "source": [
    "**Load Indexes**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k8AefCpTljUi"
   },
   "source": [
    "If you have already uploaded the zip file for the vector store checkpoint, please uncomment the code in the following cell block to extract its contents. After doing so, you will be able to load the dataset from local storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ce_WKVqZW1lP"
   },
   "outputs": [],
   "source": [
    "# Load the vector store from the local storage.\n",
    "db = chromadb.PersistentClient(path=\"./mini-llama-articles\")\n",
    "chroma_collection = db.get_or_create_collection(\"mini-llama-articles\")\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qx6HTUVmW29f",
    "outputId": "b1b14b5e-f258-44d0-99de-4f05e711aafa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-17-d75efd596876>:5: DeprecationWarning: Call to deprecated class method from_defaults. (ServiceContext is deprecated, please use `llama_index.settings.Settings` instead.) -- Deprecated since version 0.10.0.\n",
      "  service_context = ServiceContext.from_defaults(llm=llm, embed_model=embed_model)\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import ServiceContext\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"intfloat/e5-small-v2\")\n",
    "# Define the ServiceCotext object to tie the LLM for generating final answer,\n",
    "# and the embedding model to help with retrieving related nodes.\n",
    "service_context = ServiceContext.from_defaults(llm=llm, embed_model=embed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SoXtySKVXLCn"
   },
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "# Create the index based on the vector store.\n",
    "index = VectorStoreIndex.from_vector_store(\n",
    "    vector_store, service_context=service_context\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hK45PRsilnfX"
   },
   "source": [
    "**Query Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VLQNrqPcXQsP"
   },
   "outputs": [],
   "source": [
    "# Define a query engine that is responsible for retrieving related pieces of text,\n",
    "# and using a LLM to formulate the final answer.\n",
    "query_engine = index.as_query_engine(llm=llm, similarity_top_k=5)\n",
    "\n",
    "res = query_engine.query(\"How many parameters LLaMA2 model has?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "WtpFsVECXUhT",
    "outputId": "215131d4-80da-45b3-ae1d-7253ce639e99"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'The largest model has 70 billion parameters. \\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cfboDe8CXVnP",
    "outputId": "77eda677-0598-4bfd-bf54-68c3e0903ce8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node ID\t b132a431-6675-4a34-92b3-4ffaf981fb5b\n",
      "Title\t Meta's Llama 2: Revolutionizing Open Source Language Models for Commercial Use\n",
      "Text\t I. Llama 2: Revolutionizing Commercial Use Unlike its predecessor Llama 1, which was limited to research use, Llama 2 represents a major advancement as an open-source commercial model. Businesses can now integrate Llama 2 into products to create AI-powered applications. Availability on Azure and AWS facilitates fine-tuning and adoption. However, restrictions apply to prevent exploitation. Companies with over 700 million active daily users cannot use Llama 2. Additionally, its output cannot be used to improve other language models.  II. Llama 2 Model Flavors Llama 2 is available in four different model sizes: 7 billion, 13 billion, 34 billion, and 70 billion parameters. While 7B, 13B, and 70B have already been released, the 34B model is still awaited. The pretrained variant, trained on a whopping 2 trillion tokens, boasts a context window of 4096 tokens, twice the size of its predecessor Llama 1. Meta also released a Llama 2 fine-tuned model for chat applications that was trained on over 1 million human annotations. Such extensive training comes at a cost, with the 70B model taking a staggering 1720320 GPU hours to train. The context window's length determines the amount of content the model can process at once, making Llama 2 a powerful language model in terms of scale and efficiency.  III. Safety Considerations: A Top Priority for Meta Meta's commitment to safety and alignment shines through in Llama 2's design. The model demonstrates exceptionally low AI safety violation percentages, surpassing even ChatGPT in safety benchmarks. Finding the right balance between helpfulness and safety when optimizing a model poses significant challenges. While a highly helpful model may be capable of answering any question, including sensitive ones like \"How do I build a bomb?\", it also raises concerns about potential misuse. Thus, striking the perfect equilibrium between providing useful information and ensuring safety is paramount. However, prioritizing safety to an extreme extent can lead to a model that struggles to effectively address a diverse range of questions. This limitation could hinder the model's practical applicability and user experience. Thus, achieving\n",
      "Score\t 0.8017737142860696\n",
      "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
      "Node ID\t 1b53d0af-ce21-4821-b679-747ce9ff0c80\n",
      "Title\t Fine-Tuning a Llama-2 7B Model for Python Code Generation\n",
      "Text\t New Llama-2 model In mid-July, Meta released its new family of pre-trained and finetuned models called Llama-2, with an open source and commercial character to facilitate its use and expansion. The base model was released with a chat version and sizes 7B, 13B, and 70B. Together with the models, the corresponding papers were published describing their characteristics and relevant points of the learning process, which provide very interesting information on the subject. For pre-training, 40% more tokens were used, reaching 2T, the context length was doubled and the grouped-query attention (GQA) technique was applied to speed up inference on the heavier 70B model. On the standard transformer architecture, RMSNorm normalization, SwiGLU activation, and rotatory positional embedding are used, the context length reaches 4096 tokens, and an Adam optimizer is applied with a cosine learning rate schedule, a weight decay of 0.1 and gradient clipping.  The dataset for tuning For our tuning process, we will take a dataset containing about 18,000 examples where the model is asked to build a Python code that solves a given task. This is an extraction of the original dataset [2], where only the Python language examples are selected. Each row contains the description of the task to be solved, an example of data input to the task if applicable, and the generated code fragment that solves the task is provided [3].  Creating the prompt To carry out an instruction fine-tuning, we must transform each one of our data examples as if it were an instruction, outlining its main sections as follows: Output:  Fine-tuning the model To carry out this stage, we have used the Google Colab environment, where we have developed a notebook that allows us to run the training in an interactive way and also a Python script to run the training in unattended mode. For the first test runs, a T4 instance with a high RAM capacity is enough, but when it comes to running the whole dataset and epochs, we have opted to use an A100 instance in order to speed up the training and ensure that its execution time is reasonable. In order to be able to\n",
      "Score\t 0.8006135459394733\n",
      "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
      "Node ID\t e144b44b-f151-47c5-b954-0530a6ae49a6\n",
      "Title\t Inside Code Llama: Meta AI's Entrance in the Code LLM Space\n",
      "Text\t Results The evaluate Code Llama, Meta AI engaged two widely acknowledged coding benchmarks: HumanEval and Mostly Basic Python Programming (MBPP). The HumanEval benchmark systematically assesses the model's prowess in code completion via docstrings, while the MBPP benchmark scrutinizes the model's capacity to translate descriptions into executable code.The meticulous benchmarking endeavor unfolded illuminating results: Code Llama outshone open-source, code-centric Large Language Models (LLMs) and even outperformed its predecessor, Llama 2. For instance, in the case of Code Llama 34B, remarkable scores emerged - an impressive 53.7% on the HumanEval benchmark and a formidable 56.2% on the MBPP benchmark. These scores stood as the highest amongst comparable state-of-the-art solutions, positioning Code Llama 34B on par with the notable capabilities of ChatGPT. Code Llama promises to be one of the most important code LLMs in the near future. It certainly contributes to reaffirm the value of open-source foundation models across different domains.\n",
      "Score\t 0.7855226443955338\n",
      "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
      "Node ID\t 05346ad8-d733-4d2e-9f5c-d108c8b55293\n",
      "Title\t Inside Code Llama: Meta AI's Entrance in the Code LLM Space\n",
      "Text\t This strategic methodology enhances the model's capacity to grasp human expectations in prompts. For endeavors involving code generation, it is advised to opt for Code Llama - Instruct versions, as they have been calibrated to yield useful and secure natural language responses. Deep diving into the Code Llama training and fine-tuning, there are a few aspects that are worth highlighting 1) DatasetLlama's training rests on a meticulously curated dataset enriched with publicly available code, offering a near-duplicate-free landscape. The dataset consists of 500B tokens during the initial phase, starting from the 7B, 13B, and 34B versions. A supplementary 8% of sample data is garnered from natural language datasets linked to code domains. 2) InfillingWithin the realm of Code Infilling, a pivotal task revolves around predicting missing segments within a program while being guided by contextual surroundings. Pragmatic applications encompass code completion within Integrated Development Environments (IDEs), type inference, and even the generation of in-code documentation such as docstrings. Operating in alignment with the concept of causal masking, a framework expounded by Aghajanyan et al. (2022) and Fried et al. (2023), Meta AI molds infilling models. The training process entails shifting parts of training sequences to the conclusion, paving the path for autoregressive predictions. In this endeavor, both the versatile 7B and 13B models undergo infilling-oriented training, echoing the strategies advised by Bavarian et al. (2022). 3) Long Context Fine-Tuning:Unraveling the intricacies of handling extensive sequences is a formidable pursuit in the realm of transformer-based language models. The pivotal challenges orbit around extrapolation - delving into sequence lengths beyond those encountered during training - and the quadratic complexity of attention passes that tilts the balance towards short-to-medium inputs for effective training. Meta AI steps forward with a unique solution, introducing the dedicated domain of long context fine-tuning (LCFT). Embracing sequences encompassing 16,384 tokens, a substantial leap from the 4,096 tokens featured in Llama 2's initial code training stages, LCFT\n",
      "Score\t 0.7853219960849772\n",
      "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
      "Node ID\t 268b7452-2392-46a3-9519-f939e3439355\n",
      "Title\t Inside Code Llama: Meta AI's Entrance in the Code LLM Space\n",
      "Text\t Inside Code Llama The release of Code Llama does not include a single model but three different variants, characterized by their parameter sizes of 7B, 13B, and 34B. Each of these models has been trained on an extensive pool of 500B tokens encompassing code and code-related information. Notably, the 7B and 13B base and instruct models have been endowed with fill-in-the-middle (FIM) competence, empowering them to seamlessly insert code into existing code structures. This attribute equips them to handle tasks like code completion right from the outset.The trio of models caters to distinct requisites concerning serving and latency. For instance, the 7B model boasts the ability to operate on a single GPU. While the 34B model stands out for yielding optimal outcomes and elevating coding assistance, the smaller 7B and 13B versions excel in speed, making them fitting for low-latency tasks such as real-time code completion. Meta AI's innovations further extend to two nuanced adaptations of Code Llama: Code Llama - Python and Code Llama - Instruct. Code Llama - Python is a specialized derivation, meticulously honed on a substantial volume of Python code spanning 100B tokens. Given Python's central role in code generation benchmarks and its significance within the AI community, this focused model augments utility.Code Llama - Instruct represents an alignment and refinement of Code Llama through instructional fine-tuning. This novel training approach entails furnishing the model with \"natural language instruction\" inputs paired with anticipated outputs. This strategic methodology enhances the model's capacity to grasp human expectations in prompts. For endeavors involving code generation, it is advised to opt for Code Llama - Instruct versions, as they have been calibrated to yield useful and secure natural language responses. Deep diving into the Code Llama training and fine-tuning, there are a few aspects that are worth highlighting 1) DatasetLlama's training rests on a meticulously curated dataset enriched with publicly available code, offering a near-duplicate-free landscape. The dataset consists of 500B tokens during the initial phase, starting from the 7B, 13B, and 34B\n",
      "Score\t 0.7849947768753771\n",
      "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n"
     ]
    }
   ],
   "source": [
    "# Show the retrieved nodes\n",
    "for src in res.source_nodes:\n",
    "    print(\"Node ID\\t\", src.node_id)\n",
    "    print(\"Title\\t\", src.metadata[\"title\"])\n",
    "    print(\"Text\\t\", src.text)\n",
    "    print(\"Score\\t\", src.score)\n",
    "    print(\"-_\" * 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s1qsVRmZlq9q"
   },
   "source": [
    "**Evaluate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "xDt9uvJNXxDH",
    "outputId": "eda1ec13-1e68-48d3-f70c-5427a4d12fa0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 108/108 [03:58<00:00,  2.21s/it]\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.evaluation import generate_question_context_pairs\n",
    "from llama_index.llms.gemini import Gemini\n",
    "llm = Gemini(model=\"models/gemini-pro\", temperature=1, max_tokens=512)\n",
    "rag_eval_dataset = generate_question_context_pairs(\n",
    "    nodes, llm=llm, num_questions_per_chunk=1\n",
    ")\n",
    "\n",
    "# We can save the evaluation dataset as a json file for later use.\n",
    "rag_eval_dataset.save_json(\"./rag_eval_dataset_os.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BtEMnxeOZ6EX"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "#  A simple function to show the evaluation result.\n",
    "def display_results_retriever(name, eval_results):\n",
    "    \"\"\"Display results from evaluate.\"\"\"\n",
    "\n",
    "    metric_dicts = []\n",
    "    for eval_result in eval_results:\n",
    "        metric_dict = eval_result.metric_vals_dict\n",
    "        metric_dicts.append(metric_dict)\n",
    "\n",
    "    full_df = pd.DataFrame(metric_dicts)\n",
    "\n",
    "    hit_rate = full_df[\"hit_rate\"].mean()\n",
    "    mrr = full_df[\"mrr\"].mean()\n",
    "\n",
    "    metric_df = pd.DataFrame(\n",
    "        {\"Retriever Name\": [name], \"Hit Rate\": [hit_rate], \"MRR\": [mrr]}\n",
    "    )\n",
    "\n",
    "    return metric_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vl-sma4-aGZf",
    "outputId": "b2786060-d01b-4c11-f16d-828c4de14478"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Retriever Name  Hit Rate       MRR\n",
      "0  Retriever top_2  0.407407  0.324074\n",
      "    Retriever Name  Hit Rate       MRR\n",
      "0  Retriever top_4  0.490741  0.347994\n",
      "    Retriever Name  Hit Rate       MRR\n",
      "0  Retriever top_6  0.564815  0.361883\n",
      "    Retriever Name  Hit Rate       MRR\n",
      "0  Retriever top_8  0.583333  0.364198\n",
      "     Retriever Name  Hit Rate       MRR\n",
      "0  Retriever top_10  0.592593  0.365226\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.evaluation import RetrieverEvaluator\n",
    "\n",
    "# We can evaluate the retievers with different top_k values.\n",
    "for i in [2, 4, 6, 8, 10]:\n",
    "    retriever = index.as_retriever(similarity_top_k=i)\n",
    "    retriever_evaluator = RetrieverEvaluator.from_metric_names(\n",
    "        [\"mrr\", \"hit_rate\"], retriever=retriever\n",
    "    )\n",
    "    eval_results = await retriever_evaluator.aevaluate_dataset(rag_eval_dataset)\n",
    "    print(display_results_retriever(f\"Retriever top_{i}\", eval_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 332
    },
    "id": "B0mA7KzaaHyT",
    "outputId": "11c8cc7c-f318-4eb0-c8be-7195d71aa479"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-25-96598bd8895c>:15: DeprecationWarning: Call to deprecated class method from_defaults. (ServiceContext is deprecated, please use `llama_index.settings.Settings` instead.) -- Deprecated since version 0.10.0.\n",
      "  service_context_gpt4 = ServiceContext.from_defaults(llm=llm)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top_2 faithfulness_score: 0.8\n",
      "top_2 relevancy_score: 0.8\n",
      "-_-_-_-_-_-_-_-_-_-_\n",
      "top_4 faithfulness_score: 0.9\n",
      "top_4 relevancy_score: 0.9\n",
      "-_-_-_-_-_-_-_-_-_-_\n",
      "top_6 faithfulness_score: 0.85\n",
      "top_6 relevancy_score: 0.85\n",
      "-_-_-_-_-_-_-_-_-_-_\n",
      "top_8 faithfulness_score: 0.85\n",
      "top_8 relevancy_score: 0.85\n",
      "-_-_-_-_-_-_-_-_-_-_\n",
      "top_10 faithfulness_score: 0.9\n",
      "top_10 relevancy_score: 0.9\n",
      "-_-_-_-_-_-_-_-_-_-_\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.evaluation import (\n",
    "    RelevancyEvaluator,\n",
    "    FaithfulnessEvaluator,\n",
    "    BatchEvalRunner,\n",
    ")\n",
    "from llama_index.core import ServiceContext\n",
    "from llama_index.llms.gemini import Gemini\n",
    "\n",
    "for i in [2, 4, 6, 8, 10]:\n",
    "    # Set Faithfulness and Relevancy evaluators\n",
    "    query_engine = index.as_query_engine(similarity_top_k=i, llm=llm)\n",
    "\n",
    "    # While we use Gemini Pro to answer questions, we can use Gemini flash to evaluate the answers.\n",
    "    llm = Gemini(model=\"models/gemini-1.5-flash\", temperature=1, max_tokens=512)\n",
    "    service_context_gpt4 = ServiceContext.from_defaults(llm=llm)\n",
    "\n",
    "    faithfulness_evaluator = FaithfulnessEvaluator(service_context=service_context_gpt4)\n",
    "    relevancy_evaluator = RelevancyEvaluator(service_context=service_context_gpt4)\n",
    "\n",
    "    # Run evaluation\n",
    "    queries = list(rag_eval_dataset.queries.values())\n",
    "    batch_eval_queries = queries[:20]\n",
    "\n",
    "    runner = BatchEvalRunner(\n",
    "        {\"faithfulness\": faithfulness_evaluator, \"relevancy\": relevancy_evaluator},\n",
    "        workers=8,\n",
    "    )\n",
    "    eval_results = await runner.aevaluate_queries(\n",
    "        query_engine, queries=batch_eval_queries\n",
    "    )\n",
    "    faithfulness_score = sum(\n",
    "        result.passing for result in eval_results[\"faithfulness\"]\n",
    "    ) / len(eval_results[\"faithfulness\"])\n",
    "    print(f\"top_{i} faithfulness_score: {faithfulness_score}\")\n",
    "\n",
    "    relevancy_score = sum(\n",
    "        result.passing for result in eval_results[\"faithfulness\"]\n",
    "    ) / len(eval_results[\"relevancy\"])\n",
    "    print(f\"top_{i} relevancy_score: {relevancy_score}\")\n",
    "    print(\"-_\" * 10)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
