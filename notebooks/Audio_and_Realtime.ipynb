{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/towardsai/ai-tutor-rag-system/blob/main/notebooks/Audio_and_Realtime.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding Speech with OpenAIâ€™s GPT4o Audio\n",
    "\n",
    "In this lesson, we see how to leverage the new audio capabilities of GPT4o with the \"gpt-4o-audio-preview\" model. We'll see how to write code that registers our voices, sends it to the model, and plays back the audio response. We'll also learn how to parse audio streaming output and play it as soon as the first audio chunks arrive. Last, we integrate this with the AI tutor knowledge base, getting to a script that listens to the user query, instructs the LLM to use the knowledge base to retrieve information for answering the query, and plays back the final audio response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries and Environment Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code has been tested with the following libraries installed:\n",
    "\n",
    "```\n",
    "chromadb==0.5.3\n",
    "huggingface-hub==0.26.2\n",
    "llama-index==0.10.49\n",
    "llama-index-embeddings-openai==0.1.11\n",
    "numpy==1.26.4\n",
    "openai==1.54.3\n",
    "PyAudio==0.2.14\n",
    "sounddevice==0.5.1\n",
    "wavio==0.0.9\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"<YOUR-API-KEY>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Knowledge Base and Create Retriever\n",
    "\n",
    "In this section, we download our 500 blog dataset and create a vector retriever with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download 500 blog dataset as knowledge base\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "hf_hub_download(repo_id=\"jaiganesan/ai_tutor_knowledge\", filename=\"vectorstore.zip\", repo_type=\"dataset\", local_dir=\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "# Load the vector store from the local storage\n",
    "db = chromadb.PersistentClient(path=\"/Users/fabio/Desktop/temp/ai_tutor_knowledge\")\n",
    "chroma_collection = db.get_collection(\"ai_tutor_knowledge\")\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "\n",
    "# Create the index based on the vector store\n",
    "vector_index = VectorStoreIndex.from_vector_store(vector_store=vector_store)\n",
    "\n",
    "# Create retriever\n",
    "vector_retriever = vector_index.as_retriever(similarity_top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the retriever with a query\n",
    "nodes = vector_retriever.retrieve(\"How does RAG work?\")\n",
    "for node in nodes:\n",
    "    print(node.metadata[\"title\"])\n",
    "    print(node.metadata[\"url\"])\n",
    "    print(\"-\" * 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Registering Audio and Generating Audio Responses with GPT4o\n",
    "\n",
    "In this section, we see how to (1) register audio from your microphone, (2) send the audio to GPT4o to generate an audio response, and (3) play the audio response and show its transcript."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "import wavio\n",
    "import base64\n",
    "from openai import OpenAI\n",
    "import tempfile\n",
    "import json\n",
    "import simpleaudio as sa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_audio(key=\"q\", sample_rate=44100, channels=1):\n",
    "    \"\"\"Record audio from the microphone until the user sends the \"q\" key.\"\"\"\n",
    "    print(f\"Recording... Press '{key}' to stop.\")\n",
    "    audio_data = []\n",
    "\n",
    "    # Define a callback function to capture audio data\n",
    "    def callback(indata, frames, time, status):\n",
    "        audio_data.append(indata.copy())\n",
    "\n",
    "    # Open audio input stream and start recording\n",
    "    with sd.InputStream(samplerate=sample_rate, channels=channels, callback=callback):\n",
    "        while True:\n",
    "            if input() == key:\n",
    "                break\n",
    "    print(\"Stopped recording.\")\n",
    "\n",
    "    # Combine audio data and return as a numpy array\n",
    "    audio_data = np.concatenate(audio_data, axis=0)\n",
    "\n",
    "    # Save the audio to a temporary file\n",
    "    with tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False) as audio_file:\n",
    "        wavio.write(audio_file.name, audio_data, sample_rate, sampwidth=2)\n",
    "        audio_file_path = audio_file.name\n",
    "\n",
    "    return audio_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_audio_to_llm(audio_file_path, prompt):\n",
    "    \"\"\"Sends an audio file to the OpenAI API and returns the audio completion.\"\"\"\n",
    "    # Read the temp file and encode as base64\n",
    "    with open(audio_file_path, \"rb\") as audio_file:\n",
    "        encoded_audio = base64.b64encode(audio_file.read()).decode('utf-8')\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": prompt\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"input_audio\",\n",
    "                    \"input_audio\": {\n",
    "                        \"data\": encoded_audio,\n",
    "                        \"format\": \"wav\"\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    # Send to OpenAI API\n",
    "    completion = openai_client.chat.completions.create(\n",
    "        model=\"gpt-4o-audio-preview\",\n",
    "        modalities=[\"text\", \"audio\"],\n",
    "        audio={\"voice\": \"alloy\", \"format\": \"pcm16\"},\n",
    "        messages=messages\n",
    "    )\n",
    "\n",
    "    return completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_sound(pcm_bytes, sample_rate=24000, channels=1, sample_width=2):\n",
    "    \"\"\"Plays a sound from PCM bytes using simpleaudio\"\"\"\n",
    "    play_obj = sa.play_buffer(\n",
    "        pcm_bytes,\n",
    "        num_channels=channels,\n",
    "        bytes_per_sample=sample_width,\n",
    "        sample_rate=sample_rate\n",
    "    )\n",
    "    play_obj.wait_done()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record audio until the user presses 'q'\n",
    "audio_file_path = record_audio()\n",
    "\n",
    "# Initialize OpenAI API client\n",
    "openai_client = OpenAI()\n",
    "\n",
    "# Print transcription result\n",
    "prompt = \"Transcribe the attached recording. Write only the transcription and nothing else.\"\n",
    "completion = send_audio_to_llm(audio_file_path, prompt)\n",
    "print(completion.choices[0].message.audio.transcript)\n",
    "\n",
    "# Play the audio response\n",
    "pcm_bytes = base64.b64decode(completion.choices[0].message.audio.data)\n",
    "play_sound(pcm_bytes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Streaming Outputs\n",
    "\n",
    "In this section we see how to leveraging streaming outputs of the OpenAI API to retrieve the audio response chunk by chunk. This allows us to play the response audio with lower latency as we play the first bytes as soon as we receive them intead of waiting for the whole audio output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyaudio\n",
    "import threading\n",
    "import queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_sound_from_queue(pcm_queue, sample_rate=24000, channels=1, sample_width=2):\n",
    "    \"\"\"\n",
    "    Play PCM audio data from a queue that is being filled over time.\n",
    "\n",
    "    Args:\n",
    "        pcm_queue: A Queue object from which PCM data is read.\n",
    "    \"\"\"\n",
    "    p = pyaudio.PyAudio()\n",
    "    format = p.get_format_from_width(sample_width)\n",
    "\n",
    "    # Open a blocking stream\n",
    "    stream = p.open(format=format,\n",
    "                    channels=channels,\n",
    "                    rate=sample_rate,\n",
    "                    output=True)\n",
    "\n",
    "    # Read data from the queue and write to the stream\n",
    "    while True:\n",
    "        data = pcm_queue.get()\n",
    "        if data is None:\n",
    "            break  # No more data to play\n",
    "        stream.write(data)\n",
    "\n",
    "    # Clean up\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    p.terminate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_sound_and_print_transcript(stream):\n",
    "    \"\"\"\n",
    "    Starting from a stream of audio chunks (the response to the LLM call),\n",
    "    plays the response audio and prints its transcript.\n",
    "    \"\"\"\n",
    "    pcm_queue = queue.Queue()\n",
    "    has_playback_started = False\n",
    "    for chunk in stream:\n",
    "        if hasattr(chunk.choices[0].delta, \"audio\"):\n",
    "            chunk_audio = chunk.choices[0].delta.audio\n",
    "            if \"transcript\" in chunk_audio:\n",
    "                print(chunk_audio[\"transcript\"], end=\"\") # Print the transcript\n",
    "            elif \"data\" in chunk_audio:\n",
    "                pcm_bytes = base64.b64decode(chunk_audio[\"data\"])\n",
    "                pcm_queue.put(pcm_bytes) # Add the audio data to the queue\n",
    "                if not has_playback_started:\n",
    "                    # Start the playback thread\n",
    "                    playback_thread = threading.Thread(target=play_sound_from_queue, args=(pcm_queue,))\n",
    "                    playback_thread.start()\n",
    "                    has_playback_started = True\n",
    "    pcm_queue.put(None) # Signal end of data\n",
    "    playback_thread.join() # Wait for playback to finish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get response from GPT4o (i.e. a stream of chunks of audio)\n",
    "with open(audio_file_path, \"rb\") as audio_file:\n",
    "    encoded_audio = base64.b64encode(audio_file.read()).decode('utf-8')\n",
    "\n",
    "# Prepare messages\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": prompt\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"input_audio\",\n",
    "                \"input_audio\": {\n",
    "                    \"data\": encoded_audio,\n",
    "                    \"format\": \"wav\"\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "]\n",
    "\n",
    "# Get streaming response from the LLM\n",
    "stream = openai_client.chat.completions.create(\n",
    "    model=\"gpt-4o-audio-preview\",\n",
    "    modalities=[\"text\", \"audio\"],\n",
    "    audio={\"voice\": \"alloy\", \"format\": \"pcm16\"},\n",
    "    messages=messages,\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "# Play the audio response and print the transcript\n",
    "play_sound_and_print_transcript(stream)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrating Audio Inputs and Outputs with RAG\n",
    "\n",
    "In this section, we see how to (1) define the tool that retrieves relevant information from our knowledge base, (2) send the user query to the LLM specifying the available tools, (3) manage the LLM response if it asks to use a tool, (4) get the final audio response via streaming from the LLM leveraging the tool response, and (5) play the audio response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will be used as tool for the LLM to retrieve resources\n",
    "def retrieve_resources(query: str) -> str:\n",
    "    \"\"\"Given a query, retrieve relevant resources and return them as a formatted string.\"\"\"\n",
    "    nodes = vector_retriever.retrieve(query)\n",
    "\n",
    "    context_text = \"\"\n",
    "    for i, node in enumerate(nodes):\n",
    "        context_text += f\"<resource-{i+1}>\" + \"\\n\"\n",
    "        context_text += \"<resource-title>\" + node.node.metadata[\"title\"] + \"</resource-title>\" + \"\\n\\n\"\n",
    "        context_text += \"<resource-text>\" + \"\\n\" + node.node.text + \"\\n\" + \"</resource-text>\" + \"\\n\"\n",
    "        context_text += f\"</resource-{i+1}>\" + \"\\n\\n\"\n",
    "    context_text = context_text.strip()\n",
    "\n",
    "    return context_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the tools for the LLM\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"retrieve_resources\",\n",
    "            \"description\": \"Given a query, find resources that are relevant to the query and useful for answering it. It leverages an internal knowledge base.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"query\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"A query that will be used (via embeddings similarity search) to find relevant resources.\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"query\"],\n",
    "                \"additionalProperties\": False\n",
    "            },\n",
    "            \"response\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"A textual representation of the resources found that are relevant to the query.\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are a helpful assistant whose job is answering user queries about artificial intelligence topics.\n",
    "Leverage the \"retrieve_resources\" tool to find resources based on the user's query.\n",
    "You can use the tool at most once per user query.\n",
    "Always leverage the retrieved resources to provide a helpful response.\n",
    "If you can't find useful information, don't use your knowledge to make up an answer, just say that you can't find the information in your knowledge base.\n",
    "Speak fast.\n",
    "Be very concise. Answer with at most 50 words.\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_audio_to_llm(audio_file_path, system_prompt):\n",
    "    \"\"\"Sends an audio file to the OpenAI API and returns the audio completion.\"\"\"\n",
    "    # Read the temp file and encode as base64\n",
    "    with open(audio_file_path, \"rb\") as audio_file:\n",
    "        encoded_audio = base64.b64encode(audio_file.read()).decode('utf-8')\n",
    "\n",
    "    # Define the messages to send to the LLM\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": system_prompt\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"input_audio\",\n",
    "                    \"input_audio\": {\n",
    "                        \"data\": encoded_audio,\n",
    "                        \"format\": \"wav\"\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    # Send to OpenAI API\n",
    "    completion = openai_client.chat.completions.create(\n",
    "        model=\"gpt-4o-audio-preview\",\n",
    "        modalities=[\"text\", \"audio\"],\n",
    "        audio={\"voice\": \"alloy\", \"format\": \"pcm16\"},\n",
    "        messages=messages,\n",
    "        tools=tools,\n",
    "    )\n",
    "\n",
    "    return completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = send_audio_to_llm(audio_file_path, system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the response (spoiler: it's a function call)\n",
    "completion.choices[0].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manage_tool_call(completion):\n",
    "    \"\"\"\n",
    "    If the LLM completion contains a tool call, retrieve the resources and continue the conversation.\n",
    "    The returned conversation is in the form of a stream.\n",
    "    \"\"\"\n",
    "    if completion.choices[0].finish_reason == \"tool_calls\":\n",
    "        tool_call_id = completion.choices[0].message.tool_calls[0].id\n",
    "        tool_name = completion.choices[0].message.tool_calls[0].function.name # not used\n",
    "        tool_query = json.loads(completion.choices[0].message.tool_calls[0].function.arguments)[\"query\"]\n",
    "        resources = retrieve_resources(tool_query)\n",
    "\n",
    "        new_messages = messages + [\n",
    "            completion.choices[0].message,\n",
    "            {\n",
    "                \"role\": \"tool\",\n",
    "                \"content\": json.dumps({\n",
    "                    \"query\": tool_query,\n",
    "                    \"resources\": resources,\n",
    "                }),\n",
    "                \"tool_call_id\": tool_call_id\n",
    "            },\n",
    "        ]\n",
    "\n",
    "        stream = openai_client.chat.completions.create(\n",
    "            model=\"gpt-4o-audio-preview\",\n",
    "            modalities=[\"text\", \"audio\"],\n",
    "            audio={\"voice\": \"alloy\", \"format\": \"pcm16\"},\n",
    "            messages=new_messages,\n",
    "            stream=True,\n",
    "        )\n",
    "\n",
    "        return stream\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the tool call and play the audio response\n",
    "stream = manage_tool_call(completion)\n",
    "play_sound_and_print_transcript(stream)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting All Together\n",
    "\n",
    "Last, we put everything together in a single script so that (1) the user registers its question via audio, (2) the LLM generates a final audio response leveraging the retrieval tool, and (3) the audio response is played via streaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Record audio until the user presses 'q'\n",
    "audio_file_path = record_audio()\n",
    "\n",
    "# 2. Send audio to GPT4o\n",
    "completion = send_audio_to_llm(audio_file_path, system_prompt)\n",
    "\n",
    "# 3. Manage tool call\n",
    "# NB: We're assuming that the first LLM response is always a tool call!\n",
    "stream = manage_tool_call(completion)\n",
    "\n",
    "# 4. Play final response\n",
    "play_sound_and_print_transcript(stream)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llamaindexkernel",
   "language": "python",
   "name": "llamaindexkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
