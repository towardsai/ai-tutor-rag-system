{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGCYUKSzG0y0"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/towardsai/ai-tutor-rag-system/blob/main/notebooks/Audio_and_Realtime.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIfyuNrBG0y-"
      },
      "source": [
        "# Adding Speech with OpenAI’s GPT4o Audio\n",
        "\n",
        "In this lesson, we see how to leverage the new audio capabilities of GPT4o with the \"gpt-4o-audio-preview\" model. We'll see how to write code that registers our voices, sends it to the model, and plays back the audio response. We'll also learn how to parse audio streaming output and play it as soon as the first audio chunks arrive. Last, we integrate this with the AI tutor knowledge base, getting to a script that listens to the user query, instructs the LLM to use the knowledge base to retrieve information for answering the query, and plays back the final audio response."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gD0SpjphG0zB"
      },
      "source": [
        "## Libraries and Environment Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "9-Y032tyHDMO",
        "outputId": "8d265df6-1c68-46c9-d1f4-d8f5ee65cd95"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "# Install system dependencies\n",
        "%pip install -q chromadb==1.0.11 huggingface-hub==0.32.0 llama-index==0.12.21 jedi==0.19.2 llama-index-vector-stores-chroma==0.4.2 \\\n",
        "                llama-index-embeddings-openai==0.3.1 openai==1.70.0 sounddevice==0.5.1 wavio==0.0.9 simpleaudio==1.0.4 PyAudio==0.2.14 keyboard==0.13.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PJQyPDbCG0zF"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"Your OpenAI API Key Here\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iRBj8n0wG0zJ"
      },
      "source": [
        "## Load Knowledge Base and Create Retriever\n",
        "\n",
        "In this section, we download our 500 blog dataset and create a vector retriever with it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "1ct76jMLG0zN"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import Settings\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "\n",
        "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\JAI\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from huggingface_hub import login\n",
        "login(token=\"hf_xxx\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68,
          "referenced_widgets": [
            "edeccda33d744e95a6b177ab7bd8e4f3",
            "28c4a30004594110bc1be739d7780a25",
            "ec00072e97854004bc8985d3048d8c92",
            "807b463e35d54358b7671420a0436c86",
            "759715384e194ceb86dff14b60456a53",
            "3487878bc45441b4b73e0eb68c15ce2c",
            "e0a1d0cd2c8e440897f1008630cc3843",
            "46e1c61b99aa4a209486333097be6d3e",
            "0caabe3f01cf4a3d8fa7eef7040d7ec8",
            "e93f72d169774db78b4b2d3c3f30fd9e",
            "3f2358d6339741e89dd0c82a621fd02f"
          ]
        },
        "id": "fSY27jzOG0zP",
        "outputId": "105ec276-3d1c-4b07-dbb6-a8cfad760679"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'vectorstore.zip'"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Download knowledge base\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "hf_hub_download(repo_id=\"jaiganesan/ai_tutor_knowledge\", filename=\"vectorstore.zip\", repo_type=\"dataset\", local_dir=\".\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tqfUFoKUZxH5",
        "outputId": "bac5761c-466a-4a5d-e141-c8d297bb24b9"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Extract the zip file\n",
        "with zipfile.ZipFile('vectorstore.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Path: c:\\Users\\JAI\\Downloads\n"
          ]
        }
      ],
      "source": [
        "print(f\"Path: {os.getcwd()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "rUNIpiOgG0zR"
      },
      "outputs": [],
      "source": [
        "import chromadb\n",
        "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
        "from llama_index.core import VectorStoreIndex\n",
        "\n",
        "# Load the vector store from the local storage\n",
        "db = chromadb.PersistentClient(path=\"c:/Users/JAI/Downloads/ai_tutor_knowledge\")\n",
        "chroma_collection = db.get_collection(\"ai_tutor_knowledge\")\n",
        "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
        "\n",
        "# Create the index based on the vector store\n",
        "vector_index = VectorStoreIndex.from_vector_store(vector_store=vector_store)\n",
        "\n",
        "# Create retriever\n",
        "vector_retriever = vector_index.as_retriever(similarity_top_k=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JkcJZ6VHG0zT",
        "outputId": "b9676e60-84eb-4bfa-c897-b70c98c4c7ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RAGChecker: A Fine-grained Framework for Diagnosing Retrieval-Augmented Generation:Related Work\n",
            "http://arxiv.org/pdf/2408.08067v2\n",
            "-----\n",
            "RAG\n",
            "https://huggingface.co/docs/transformers/model_doc/rag\n",
            "-----\n",
            "Conceptual guide\n",
            "https://python.langchain.com/v0.2/docs/concepts\n",
            "-----\n",
            "RAGChecker: A Fine-grained Framework for Diagnosing Retrieval-Augmented Generation:Comparative Metrics of RAGCHECKER\n",
            "http://arxiv.org/pdf/2408.08067v2\n",
            "-----\n",
            "RAGChecker: A Fine-grained Framework for Diagnosing Retrieval-Augmented Generation:Evaluation of RAG\n",
            "http://arxiv.org/pdf/2408.08067v2\n",
            "-----\n"
          ]
        }
      ],
      "source": [
        "# Test the retriever with a query\n",
        "nodes = vector_retriever.retrieve(\"How does RAG work?\")\n",
        "for node in nodes:\n",
        "    print(node.metadata[\"title\"])\n",
        "    print(node.metadata[\"url\"])\n",
        "    print(\"-\" * 5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9AdoNJQG0zU"
      },
      "source": [
        "## Registering Audio and Generating Audio Responses with GPT4o\n",
        "\n",
        "In this section, we see how to (1) register audio from your microphone, (2) send the audio to GPT4o to generate an audio response, and (3) play the audio response and show its transcript."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Axen3JF4G0zV"
      },
      "outputs": [],
      "source": [
        "import sounddevice as sd\n",
        "import numpy as np\n",
        "import wavio\n",
        "import base64\n",
        "from openai import OpenAI\n",
        "import tempfile\n",
        "import json\n",
        "import simpleaudio as sa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "iZf2lGsuG0zW"
      },
      "outputs": [],
      "source": [
        "import keyboard\n",
        "import time\n",
        "\n",
        "def record_audio(key=\"q\", sample_rate=44100, channels=1):\n",
        "    \"\"\"Record audio from the microphone until the user sends the \"q\" key.\"\"\"\n",
        "    print(f\"Recording... Press '{key}' to stop.\")\n",
        "    audio_data = []\n",
        "    recording = True\n",
        "\n",
        "    # Define a callback function to capture audio data\n",
        "    def callback(indata, frames, time, status):\n",
        "        audio_data.append(indata.copy())\n",
        "    \n",
        "    # Open audio input stream and start recording\n",
        "    with sd.InputStream(samplerate=sample_rate, channels=channels, callback=callback):\n",
        "        while recording:\n",
        "            if keyboard.is_pressed(key):\n",
        "                recording = False\n",
        "                print(\"\\nStopped recording.\")\n",
        "                break\n",
        "            time.sleep(0.1)\n",
        "            \n",
        "    audio_data = np.concatenate(audio_data, axis=0)\n",
        "\n",
        "    # Save the audio to a temporary file\n",
        "    with tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False) as audio_file:\n",
        "        wavio.write(audio_file.name, audio_data, sample_rate, sampwidth=2)\n",
        "        audio_file_path = audio_file.name\n",
        "\n",
        "    return audio_file_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "55vNFqtXG0zX"
      },
      "outputs": [],
      "source": [
        "# Initialize OpenAI API client\n",
        "openai_client = OpenAI()\n",
        "\n",
        "def send_audio_to_llm(audio_file_path, prompt):\n",
        "    \"\"\"Sends an audio file to the OpenAI API and returns the audio completion.\"\"\"\n",
        "    # Read the temp file and encode as base64\n",
        "    with open(audio_file_path, \"rb\") as audio_file:\n",
        "        encoded_audio = base64.b64encode(audio_file.read()).decode('utf-8')\n",
        "\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\n",
        "                    \"type\": \"text\",\n",
        "                    \"text\": prompt\n",
        "                },\n",
        "                {\n",
        "                    \"type\": \"input_audio\",\n",
        "                    \"input_audio\": {\n",
        "                        \"data\": encoded_audio,\n",
        "                        \"format\": \"wav\"\n",
        "                    }\n",
        "                }\n",
        "            ]\n",
        "        },\n",
        "    ]\n",
        "\n",
        "    # Send to OpenAI API\n",
        "    completion = openai_client.chat.completions.create(\n",
        "        model=\"gpt-4o-audio-preview\",\n",
        "        modalities=[\"text\", \"audio\"],\n",
        "        audio={\"voice\": \"alloy\", \"format\": \"pcm16\"},\n",
        "        messages=messages\n",
        "    )\n",
        "    \n",
        "    return completion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "6wPU7Ml5G0zY"
      },
      "outputs": [],
      "source": [
        "def play_sound(pcm_bytes, sample_rate=24000, channels=1, sample_width=2):\n",
        "    \"\"\"Plays audio using sounddevice (safer for Jupyter)\"\"\"\n",
        "    try:\n",
        "        print(f\"Playing audio: {len(pcm_bytes)} bytes\")\n",
        "        \n",
        "        # Convert PCM bytes to numpy array\n",
        "        audio_array = np.frombuffer(pcm_bytes, dtype=np.int16)\n",
        "        \n",
        "        # Convert to float32 and normalize\n",
        "        audio_float = audio_array.astype(np.float32) / 32768.0\n",
        "                \n",
        "        # Play using sounddevice\n",
        "        sd.play(audio_float, samplerate=sample_rate)\n",
        "        sd.wait()  # Wait until playback is finished\n",
        "        print(\"Audio playback completed\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Audio playback failed: {e}\")\n",
        "        # Fallback: save to file\n",
        "        with open(\"response_audio.wav\", \"wb\") as f:\n",
        "            f.write(pcm_bytes)\n",
        "        print(\"Audio saved to response_audio.wav instead\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 894
        },
        "id": "DiDFjvhTG0zZ",
        "outputId": "be5e3245-8777-495d-ebd8-a738bcadd251"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Recording... Press 'q' to stop.\n",
            "\n",
            "Stopped recording.\n",
            "Transcription: What is retrieval-augmented generation?\n",
            "Playing audio: 165600 bytes\n",
            "Audio playback completed\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    # Record audio until the we presses 'q'\n",
        "    audio_file_path = record_audio()\n",
        "\n",
        "    # Print transcription result\n",
        "    prompt = \"Transcribe the attached recording. Write only the transcription and nothing else.\"\n",
        "    completion = send_audio_to_llm(audio_file_path, prompt)\n",
        "    print(\"Transcription:\", completion.choices[0].message.audio.transcript)\n",
        "\n",
        "    # Play the audio response\n",
        "    pcm_bytes = base64.b64decode(completion.choices[0].message.audio.data)\n",
        "    play_sound(pcm_bytes)\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLEm7KzBG0za"
      },
      "source": [
        "## Using Streaming Outputs\n",
        "\n",
        "In this section we see how to leveraging streaming outputs of the OpenAI API to retrieve the audio response chunk by chunk. This allows us to play the response audio with lower latency as we play the first bytes as soon as we receive them intead of waiting for the whole audio output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "mlvJhkFpG0za"
      },
      "outputs": [],
      "source": [
        "import pyaudio\n",
        "import threading\n",
        "import queue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "zHzIKzwbG0za"
      },
      "outputs": [],
      "source": [
        "def play_sound_from_queue(pcm_queue, sample_rate=24000, channels=1, sample_width=2):\n",
        "    \"\"\"\n",
        "    Play PCM audio data from a queue that is being filled over time.\n",
        "\n",
        "    Args:\n",
        "        pcm_queue: A Queue object from which PCM data is read.\n",
        "    \"\"\"\n",
        "    p = pyaudio.PyAudio()\n",
        "    format = p.get_format_from_width(sample_width)\n",
        "\n",
        "    # Open a blocking stream\n",
        "    stream = p.open(format=format,\n",
        "                    channels=channels,\n",
        "                    rate=sample_rate,\n",
        "                    output=True)\n",
        "\n",
        "    # Read data from the queue and write to the stream\n",
        "    while True:\n",
        "        data = pcm_queue.get()\n",
        "        if data is None:\n",
        "            break  # No more data to play\n",
        "        stream.write(data)\n",
        "\n",
        "    # Clean up\n",
        "    stream.stop_stream()\n",
        "    stream.close()\n",
        "    p.terminate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "n0dm8xDFG0zb"
      },
      "outputs": [],
      "source": [
        "def play_sound_and_print_transcript(stream):\n",
        "    \"\"\"\n",
        "    Starting from a stream of audio chunks (the response to the LLM call),\n",
        "    plays the response audio and prints its transcript.\n",
        "    \"\"\"\n",
        "    pcm_queue = queue.Queue()\n",
        "    has_playback_started = False\n",
        "    for chunk in stream:\n",
        "        if hasattr(chunk.choices[0].delta, \"audio\"):\n",
        "            chunk_audio = chunk.choices[0].delta.audio\n",
        "            if \"transcript\" in chunk_audio:\n",
        "                print(chunk_audio[\"transcript\"], end=\"\") # Print the transcript\n",
        "            elif \"data\" in chunk_audio:\n",
        "                pcm_bytes = base64.b64decode(chunk_audio[\"data\"])\n",
        "                pcm_queue.put(pcm_bytes) # Add the audio data to the queue\n",
        "                if not has_playback_started:\n",
        "                    # Start the playback thread\n",
        "                    playback_thread = threading.Thread(target=play_sound_from_queue, args=(pcm_queue,))\n",
        "                    playback_thread.start()\n",
        "                    has_playback_started = True\n",
        "    pcm_queue.put(None) # Signal end of data\n",
        "    playback_thread.join() # Wait for playback to finish"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "XlxAq1mnG0zb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "What is retrieval augmented generation?"
          ]
        }
      ],
      "source": [
        "# Get response from GPT4o (i.e. a stream of chunks of audio)\n",
        "with open(audio_file_path, \"rb\") as audio_file:\n",
        "    encoded_audio = base64.b64encode(audio_file.read()).decode('utf-8')\n",
        "\n",
        "# Prepare messages\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "            {\n",
        "                \"type\": \"text\",\n",
        "                \"text\": prompt\n",
        "            },\n",
        "            {\n",
        "                \"type\": \"input_audio\",\n",
        "                \"input_audio\": {\n",
        "                    \"data\": encoded_audio,\n",
        "                    \"format\": \"wav\"\n",
        "                }\n",
        "            }\n",
        "        ]\n",
        "    },\n",
        "]\n",
        "\n",
        "# Get streaming response from the LLM\n",
        "stream = openai_client.chat.completions.create(\n",
        "    model=\"gpt-4o-audio-preview\",\n",
        "    modalities=[\"text\", \"audio\"],\n",
        "    audio={\"voice\": \"alloy\", \"format\": \"pcm16\"},\n",
        "    messages=messages,\n",
        "    stream=True,\n",
        ")\n",
        "\n",
        "# Play the audio response and print the transcript\n",
        "play_sound_and_print_transcript(stream)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNcX81RjG0zc"
      },
      "source": [
        "## Integrating Audio Inputs and Outputs with RAG\n",
        "\n",
        "In this section, we see how to (1) define the tool that retrieves relevant information from our knowledge base, (2) send the user query to the LLM specifying the available tools, (3) manage the LLM response if it asks to use a tool, (4) get the final audio response via streaming from the LLM leveraging the tool response, and (5) play the audio response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "H3BlOLxbG0zc"
      },
      "outputs": [],
      "source": [
        "# This function will be used as tool for the LLM to retrieve resources\n",
        "def retrieve_resources(query: str) -> str:\n",
        "    \"\"\"Given a query, retrieve relevant resources and return them as a formatted string.\"\"\"\n",
        "    nodes = vector_retriever.retrieve(query)\n",
        "\n",
        "    context_text = \"\"\n",
        "    for i, node in enumerate(nodes):\n",
        "        context_text += f\"<resource-{i+1}>\" + \"\\n\"\n",
        "        context_text += \"<resource-title>\" + node.node.metadata[\"title\"] + \"</resource-title>\" + \"\\n\\n\"\n",
        "        context_text += \"<resource-text>\" + \"\\n\" + node.node.text + \"\\n\" + \"</resource-text>\" + \"\\n\"\n",
        "        context_text += f\"</resource-{i+1}>\" + \"\\n\\n\"\n",
        "    context_text = context_text.strip()\n",
        "\n",
        "    return context_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "FDblhpc0G0zd"
      },
      "outputs": [],
      "source": [
        "# Define the tools for the LLM\n",
        "tools = [\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"retrieve_resources\",\n",
        "            \"description\": \"Given a query, find resources that are relevant to the query and useful for answering it. It leverages an internal knowledge base.\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"query\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"description\": \"A query that will be used (via embeddings similarity search) to find relevant resources.\"\n",
        "                    }\n",
        "                },\n",
        "                \"required\": [\"query\"],\n",
        "                \"additionalProperties\": False\n",
        "            },\n",
        "            \"response\": {\n",
        "                \"type\": \"string\",\n",
        "                \"description\": \"A textual representation of the resources found that are relevant to the query.\"\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "CXdDaiIvG0ze"
      },
      "outputs": [],
      "source": [
        "system_prompt = \"\"\"\n",
        "You are a helpful assistant whose job is answering user queries about artificial intelligence topics.\n",
        "Leverage the \"retrieve_resources\" tool to find resources based on the user's query.\n",
        "You can use the tool at most once per user query.\n",
        "Always leverage the retrieved resources to provide a helpful response.\n",
        "If you can't find useful information, don't use your knowledge to make up an answer, just say that you can't find the information in your knowledge base.\n",
        "Speak fast.\n",
        "Be very concise. Answer with at most 50 words.\n",
        "\"\"\".strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "Z1R13igEG0ze"
      },
      "outputs": [],
      "source": [
        "def send_audio_to_llm(audio_file_path, system_prompt):\n",
        "    \"\"\"Sends an audio file to the OpenAI API and returns the audio completion.\"\"\"\n",
        "    # Read the temp file and encode as base64\n",
        "    with open(audio_file_path, \"rb\") as audio_file:\n",
        "        encoded_audio = base64.b64encode(audio_file.read()).decode('utf-8')\n",
        "\n",
        "    # Define the messages to send to the LLM\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": system_prompt\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\n",
        "                    \"type\": \"input_audio\",\n",
        "                    \"input_audio\": {\n",
        "                        \"data\": encoded_audio,\n",
        "                        \"format\": \"wav\"\n",
        "                    }\n",
        "                }\n",
        "            ]\n",
        "        },\n",
        "    ]\n",
        "\n",
        "    # Send to OpenAI API\n",
        "    completion = openai_client.chat.completions.create(\n",
        "        model=\"gpt-4o-audio-preview\",\n",
        "        modalities=[\"text\", \"audio\"],\n",
        "        audio={\"voice\": \"alloy\", \"format\": \"pcm16\"},\n",
        "        messages=messages,\n",
        "        tools=tools,\n",
        "    )\n",
        "\n",
        "    return completion, messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "Pp7pP0hkG0zf"
      },
      "outputs": [],
      "source": [
        "completion, messages = send_audio_to_llm(audio_file_path, system_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "vGHuR6NwG0zf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'finish_reason': 'tool_calls',\n",
              " 'index': 0,\n",
              " 'message': {'content': None,\n",
              "  'refusal': None,\n",
              "  'role': 'assistant',\n",
              "  'annotations': [],\n",
              "  'tool_calls': [{'id': 'call_8GOxnWzl2ppv7T2SrYmRRZZF',\n",
              "    'function': {'arguments': '{\"query\":\"retrieval-augmented generation\"}',\n",
              "     'name': 'retrieve_resources'},\n",
              "    'type': 'function'}]}}"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Show the response (spoiler: it's a function call)\n",
        "completion.choices[0].to_dict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "kHVt7NDkG0zg"
      },
      "outputs": [],
      "source": [
        "def manage_tool_call(completion, messages):\n",
        "    \"\"\"\n",
        "    If the LLM completion contains a tool call, retrieve the resources and continue the conversation.\n",
        "    The returned conversation is in the form of a stream.\n",
        "    \"\"\"\n",
        "    if completion.choices[0].finish_reason == \"tool_calls\":\n",
        "        tool_call_id = completion.choices[0].message.tool_calls[0].id\n",
        "        tool_name = completion.choices[0].message.tool_calls[0].function.name # not used\n",
        "        tool_query = json.loads(completion.choices[0].message.tool_calls[0].function.arguments)[\"query\"]\n",
        "        resources = retrieve_resources(tool_query)\n",
        "\n",
        "        new_messages = messages + [\n",
        "            completion.choices[0].message,\n",
        "            {\n",
        "                \"role\": \"tool\",\n",
        "                \"content\": json.dumps({\n",
        "                    \"query\": tool_query,\n",
        "                    \"resources\": resources,\n",
        "                }),\n",
        "                \"tool_call_id\": tool_call_id\n",
        "            },\n",
        "        ]\n",
        "\n",
        "        stream = openai_client.chat.completions.create(\n",
        "            model=\"gpt-4o-audio-preview\",\n",
        "            modalities=[\"text\", \"audio\"],\n",
        "            audio={\"voice\": \"alloy\", \"format\": \"pcm16\"},\n",
        "            messages=new_messages,\n",
        "            stream=True,\n",
        "        )\n",
        "\n",
        "        return stream\n",
        "    return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_mLYlGPG0zh"
      },
      "source": [
        "## Putting All Together\n",
        "\n",
        "Last, we put everything together in a single script so that (1) the user registers its question via audio, (2) the LLM generates a final audio response leveraging the retrieval tool, and (3) the audio response is played via streaming."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "upzrddn_G0zh"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Recording... Press 'q' to stop.\n",
            "\n",
            "Stopped recording.\n",
            "Retrieval-Augmented Generation (RAG) systems combine retrieval of knowledge from external sources and generation of responses. They consist of a retrieval component, extracting relevant info, and a generation component, using that info to produce responses. This helps in creating accurate and context-relevant answers for various applications, improving the quality of generated content."
          ]
        }
      ],
      "source": [
        "# 1. Record audio until the user presses 'q'\n",
        "audio_file_path = record_audio()\n",
        "\n",
        "# 2. Send audio to GPT4o\n",
        "completion, messages = send_audio_to_llm(audio_file_path, system_prompt)\n",
        "\n",
        "# 3. Manage tool call\n",
        "# NB: We're assuming that the first LLM response is always a tool call!\n",
        "stream = manage_tool_call(completion, messages)\n",
        "\n",
        "# 4. Play final response\n",
        "play_sound_and_print_transcript(stream)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0caabe3f01cf4a3d8fa7eef7040d7ec8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "28c4a30004594110bc1be739d7780a25": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3487878bc45441b4b73e0eb68c15ce2c",
            "placeholder": "​",
            "style": "IPY_MODEL_e0a1d0cd2c8e440897f1008630cc3843",
            "value": "vectorstore.zip: 100%"
          }
        },
        "3487878bc45441b4b73e0eb68c15ce2c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3f2358d6339741e89dd0c82a621fd02f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "46e1c61b99aa4a209486333097be6d3e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "759715384e194ceb86dff14b60456a53": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "807b463e35d54358b7671420a0436c86": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e93f72d169774db78b4b2d3c3f30fd9e",
            "placeholder": "​",
            "style": "IPY_MODEL_3f2358d6339741e89dd0c82a621fd02f",
            "value": " 97.2M/97.2M [00:01&lt;00:00, 116MB/s]"
          }
        },
        "e0a1d0cd2c8e440897f1008630cc3843": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e93f72d169774db78b4b2d3c3f30fd9e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ec00072e97854004bc8985d3048d8c92": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_46e1c61b99aa4a209486333097be6d3e",
            "max": 97198458,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0caabe3f01cf4a3d8fa7eef7040d7ec8",
            "value": 97198458
          }
        },
        "edeccda33d744e95a6b177ab7bd8e4f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_28c4a30004594110bc1be739d7780a25",
              "IPY_MODEL_ec00072e97854004bc8985d3048d8c92",
              "IPY_MODEL_807b463e35d54358b7671420a0436c86"
            ],
            "layout": "IPY_MODEL_759715384e194ceb86dff14b60456a53"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
