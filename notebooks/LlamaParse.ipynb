{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "3iSJpxmEML0w",
    "outputId": "e71ba052-5ec5-4ace-b01a-70c5ef2e7844"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.0/241.0 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m41.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.6/49.6 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -q llama-index==0.12.12 llama-parse==0.5.19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "n9OKejSAM1Kj"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# API access to llama-cloud\n",
    "os.environ[\"LLAMA_CLOUD_API_KEY\"] = \"llx-\"\n",
    "\n",
    "# from google.colab import userdata\n",
    "# os.environ[\"LLAMA_CLOUD_API_KEY\"] = userdata.get('LlamaParse_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "93f5e3f20b014560920f48fd8a7fe275",
      "14c5e2bb9e36407498d455be8d3afb91",
      "131d08f21cde4d5c9c1fce6e4f74d098",
      "88b0cc7928094e6db03d74b7d9be5aa0",
      "3b26f99d7b904851b82e93fa7a9b8ae8",
      "e191dd741afe40ac8954a8f112105d9d",
      "519d28aadf2442c484c29380ec60efd5",
      "ba17b4049838486ea30346b211753a6e",
      "bab0b4df6bab4c8b88295b0e2e5c7cc3",
      "00689ba2edfb462aa7ab247bbb2358a7",
      "8d10b79bf1f94f4bab952293299c5afb"
     ]
    },
    "id": "Xs5WjC01NCiy",
    "outputId": "da515fa5-cdf8-4849-9dfe-a74fdf7a2612"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93f5e3f20b014560920f48fd8a7fe275",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "research_papers_llamaparse.zip:   0%|          | 0.00/13.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Downloading Research paper dataset from HuggingFace Hub\n",
    "from huggingface_hub import hf_hub_download\n",
    "file_path = hf_hub_download(repo_id=\"jaiganesan/ai_tutor_knowledge\", filename=\"research_papers_llamaparse.zip\",repo_type=\"dataset\",local_dir=\"/content\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tRNqM8cIPQC_",
    "outputId": "71bee7c0-62d0-4eaf-b6ed-292229b36566"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  research_papers_llamaparse.zip\n",
      "   creating: research_papers_llamaparse/\n",
      "  inflating: research_papers_llamaparse/2106.09685v2.pdf  \n",
      "  inflating: research_papers_llamaparse/2404.19756v2.pdf  \n",
      "  inflating: research_papers_llamaparse/2405.07437v2.pdf  \n"
     ]
    }
   ],
   "source": [
    "!unzip research_papers_llamaparse.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "WC9urbdDOABf"
   },
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "itMgf7Dsptw4"
   },
   "source": [
    "## Parse directory to LlamaParse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "-JuFlFt2NcJF"
   },
   "outputs": [],
   "source": [
    "# LlamaParse Implemetation\n",
    "from llama_parse import LlamaParse\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "#Parser\n",
    "parser = LlamaParse(\n",
    "    result_type=\"markdown\",\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CrPv4GtFODxP",
    "outputId": "9c998e6b-2376-4462-eb5a-ba30cb9f74ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started parsing the file under job_id 1ba495f0-0a20-4913-93f9-c951735d2951\n",
      "Started parsing the file under job_id e9459eda-3ac8-4e11-926c-5a311c9b3a98\n",
      ".Started parsing the file under job_id b06b8507-780a-4c41-9293-27212a2f1ec3\n"
     ]
    }
   ],
   "source": [
    "file_extractor = {\".pdf\": parser}\n",
    "\n",
    "documents = SimpleDirectoryReader(\"/content/research_papers_llamaparse\", file_extractor=file_extractor).load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XvWyrtWhPgkZ",
    "outputId": "d1aaa1c5-2884-4f27-f76f-4d715b67bc2a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(id_='417c0e4b-6a25-499e-9f8d-b1d5a3d2d292', embedding=None, metadata={'file_path': '/content/research_papers_llamaparse/2106.09685v2.pdf', 'file_name': '2106.09685v2.pdf', 'file_type': 'application/pdf', 'file_size': 1609513, 'creation_date': '2025-01-21', 'last_modified_date': '2024-07-06'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='# LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS\\n\\nEdward Hu∗ Yelong Shen∗ Phillip Wallis Zeyuan Allen-Zhu Yuanzhi Li Shean Wang Lu Wang Weizhu Chen\\n\\nMicrosoft Corporation\\n\\n{edwardhu, yeshe, phwallis, zeyuana, yuanzhil, swang, luw, wzchen}@microsoft.com\\n\\nyuanzhil@andrew.cmu.edu\\n\\n(Version 2)\\n\\narXiv:2106.09685v2 [cs.CL] 16 Oct 2021\\n\\n# ABSTRACT\\n\\nAn important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example – deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA.\\n\\n# 1 INTRODUCTION\\n\\nMany applications in natural language processing rely on adapting one large-scale, pre-trained language model to multiple downstream applications. Such adaptation is usually done via fine-tuning, which updates all the parameters of the pre-trained model. The major downside of fine-tuning is that the new model contains as many parameters as in the original model. As larger models are trained every few months, this changes from a mere “inconvenience” for GPT-2 (Radford et al., b) or RoBERTa large (Liu et al., 2019) to a critical deployment challenge for GPT-3 (Brown et al., 2020) with 175 billion trainable parameters.\\n\\nMany sought to mitigate this by adapting only some parameters or learning external modules for new tasks. This way, we only need to store and load a small number of task-specific parameters in addition to the pre-trained model for each task, greatly boosting the operational efficiency when deployed. However, existing techniques\\n\\n∗Equal contribution.\\n\\n0Compared to V1, this draft includes better baselines, experiments on GLUE, and more on adapter latency.\\n\\n1While GPT-3 175B achieves non-trivial performance with few-shot learning, fine-tuning boosts its performance significantly as shown in Appendix A.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fCfzTVX0pqhs"
   },
   "source": [
    "## LlamaParse JSON Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VBb4VjgPPiWq",
    "outputId": "b17b865a-191a-4cc3-b313-41875c70752e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started parsing the file under job_id 9dcfabaf-0ec6-40ef-858f-4e8395acdd1e\n",
      "Started parsing the file under job_id 2df49ee5-e60f-4f4c-9c3e-7e49fb267f0a\n",
      "Started parsing the file under job_id ce786afd-1ffe-4c04-bb9b-5fa0f6a61035\n"
     ]
    }
   ],
   "source": [
    "# Using LlamaParse in JSON Mode for PDF Reading\n",
    "\n",
    "import glob\n",
    "pdf_files = glob.glob(\"/content/research_papers_llamaparse/*.pdf\")\n",
    "\n",
    "parser = LlamaParse(verbose=True)\n",
    "\n",
    "json_objs=[]\n",
    "\n",
    "for pdf_file in pdf_files:\n",
    "  json_objs.extend(parser.get_json_result(pdf_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "id": "A9HexIWZWCpV",
    "outputId": "b54aaca9-1fce-465b-99c4-f7e15dc189b6"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'                                                Evaluation of Retrieval-Augmented Generation:\\n                                                                                      A Survey\\n                                           Hao Yu1,2, Aoran Gan3, Kai Zhang3, Shiwei Tong1†, Qi Liu3, and Zhaofeng Liu1\\n                                                                                   1 Tencent Company\\n                                                                                   2 McGill University\\n                                                                  3 State Key Laboratory of Cognitive Intelligence,\\narXiv:2405.07437v2  [cs.CL]  3 Jul 2024University of Science and Technology of Chinahao.yu2@mail.mcgill.ca\\n                                                                              gar@mail.ustc.edu.cn\\n                                                               {shiweitong†,zhaofengliu}@tencent.com\\n                                                                    {kkzhang08,qiliuql}@ustc.edu.cn\\n\\n                                                 Abstract. Retrieval-Augmented Generation (RAG) has recently gained traction\\n                                                 in natural language processing. Numerous studies and real-world applications\\n                                                 are leveraging its ability to enhance generative models through external informa-\\n                                                 tion retrieval. Evaluating these RAG systems, however, poses unique challenges\\n                                                 due to their hybrid structure and reliance on dynamic knowledge sources. To\\n                                                 better understand these challenges, we conduct A Unified Evaluation Process of\\n                                                 RAG (Auepora) and aim to provide a comprehensive overview of the evaluation\\n                                                 and benchmarks of RAG systems. Specifically, we examine and compare several\\n                                                 quantifiable metrics of the Retrieval and Generation components, such as rele-\\n                                                 vance, accuracy, and faithfulness, within the current RAG benchmarks, encom-\\n                                                 passing the possible output and ground truth pairs. We then analyze the various\\n                                                 datasets and metrics, discuss the limitations of current benchmarks, and suggest\\n                                                 potential directions to advance the field of RAG benchmarks.\\n\\n                                        1\\n                                             Introduction\\n                                        Retrieval-Augmented Generation (RAG) [34] efficiently enhances the performance of\\n                                        generative language models through integrating information retrieval techniques. It ad-\\n                                        dresses a critical challenge faced by standalone generative language models: the ten-\\n                                        dency to produce responses that, while plausible, may not be grounded in facts. By\\n                                        retrieving relevant information from external sources, RAG significantly reduces the\\n                                        incidence of hallucinations [23] or factually incorrect outputs, thereby improving the\\n                                        content’s reliability and richness. [73] This fusion of retrieval and generation capabil-\\n                                        ities enables the creation of responses that are not only contextually appropriate but\\n                                        also informed by the most current and accurate information available, making RAG a\\n                                        development in the pursuit of more intelligent and versatile language models [73,64].\\n                                         † Corresponding Author\\n                                           Paper Homepage: https://github.com/YHPeter/Awesome-RAG-Evaluation'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_objs[0]['pages'][0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "id": "fN075z18fAHI",
    "outputId": "0a7a811d-1017-4a5f-98bd-4cc98cf5bab6"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'                                    Evaluation of Retrieval-Augmented Generation: A Survey                         5\\n                         Result         Query       Ground Truth\\n  Retrieval          Relevant Docs                Docs Candidates      Relevant Docs   Query            Relevance\\n                                                                       Relevant Docs   Docs Candidates   Accuracy\\n                       Response                  Sample Response       Response    Query              FaithfulnessRelevance\\n  Generation                                                           Response    Relevant Docs\\n                        Output                         Label           Response    Sample Response     Correctness\\n  Additional Requirements                                     Latency, Noise Robustness Negative Rejection, Diversity,..\\n                              Fig. 2: The Target modular of the Auepora.\\n\\n3.1     Evaluation Target (What to Evaluate?)\\n\\nThe combination of EOs and GTs in the RAG system can generate all possible targets,\\nwhich is the fundamental concept of the Auepora (as shown in Figure 1). Once iden-\\ntified, these targets can be defined based on a specific pair of EOs or EO with GT, as\\nillustrated in Figure 2, and used to analyze all aspects of current RAG benchmarks.\\n\\nRetrieval The EOs are the relevant documents for evaluating the retrieval component\\ndepending on the query. Then we can construct two pairwise relationships for the re-\\ntrieval component, which are Relevant Documents ↔ Query, Relevant Documents ↔\\nDocuments Candidates.\\n\\n   - Relevance (Relevant Documents ↔ Query) evaluates how well the retrieved docu-\\n      ments match the information needed expressed in the query. It measures the preci-\\n      sion and specificity of the retrieval process.\\n   - Accuracy (Relevant Documents ↔ Documents Candidates) assesses how accurate\\n      the retrieved documents are in comparison to a set of candidate documents. It is\\n      a measure of the system’s ability to identify and score relevant documents higher\\n      than less relevant or irrelevant ones.\\n\\nGeneration The similar pairwise relations for the generation components are listed\\nbelow. The EOs are the generated text and phrased structured content. Then we need to\\ncompare these EOs with the provided GTs and labels.\\n\\n   - Relevance (Response ↔ Query) measures how well the generated response aligns\\n      with the intent and content of the initial query. It ensures that the response is related\\n      to the query topic and meets the query’s specific requirements.\\n   - Faithfulness (Response ↔ Relevant Documents) evaluates if the generated re-\\n      sponse accurately reflects the information contained within the relevant documents\\n      and measures the consistency between generated content and the source documents.\\n   - Correctness (Response ↔ Sample Response) Similar to the accuracy in the re-\\n      trieval component, this measures the accuracy of the generated response against a\\n      sample response, which serves as a ground truth. It checks if the response is correct\\n      in terms of factual information and appropriate in the context of the query.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_objs[0]['pages'][4]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "id": "TXGAykARXWdV",
    "outputId": "f7cf6daa-7702-45ad-f827-58105c84a57e"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'                                  Evaluation of Retrieval-Augmented Generation: A Survey   7\\n\\n2024. Table 1 portrays this information, where each evaluation criterion is represented\\nby a different colour. For example, FeB4RAG [57], the fourth from the last, has posited\\nfour standards based on [17] that comprise Consistency, Correctness, Clarity, and Cov-\\nerage. Correctness is equivalent to accuracy in retrieval, and Consistency is tantamount\\nto faithfulness in the generation component. While accuracy in retrieval gauges the\\ncorrectness of the retrieved information, we posit that Coverage pertains to the cover-\\nage rate and is more associated with diversity. Therefore, we consider Coverage to be\\nlinked with diversity and an additional requirement in our proposed evaluation frame-\\nwork, which will be introduced subsequently. The remaining standard, Clarity, is also\\nclassified as an additional requirement in our proposed framework. The other tools and\\nbenchmarks are processed similarly.\\n     Tools and benchmarks offer varying degrees of flexibility in evaluating datasets for\\nRAG systems. Tools, which specify only evaluation targets, provide a versatile frame-\\nwork capable of constructing complete RAG applications and evaluation pipelines, as\\nseen in works like [54,32,33]. Benchmarks, on the other hand, focus on different as-\\npects of RAG evaluation with specific emphasis on either retrieval outputs or genera-\\ntion targets. For instance, RAGAs [14] and ARES [49] assess the relevance of retrieval\\ndocuments, while RGB and MultiHop-RAG [6,52] prioritize accuracy, necessitating\\ncomparison with GTs. The [66] focuses on the Hallucination, which is a combination\\nof faithfulness and correctness. All benchmarks consider generation targets due to their\\ncritical role in RAG systems, though their focus areas vary.\\n\\nAdditional Requirement In addition to evaluating the two primary components out-\\nlined, a portion of the works also addressed some additional requirements of RAG\\n(Black and Italics targets in Table 2). The requirements are as follows:\\n\\n   - Latency [20,32] measures how quickly the system can find information and re-\\n      spond, crucial for user experience.\\n   - Diversity [4,32] checks if the system retrieves a variety of relevant documents and\\n      generates diverse responses.\\n   - Noise Robustness [6] assesses how well the system handles irrelevant information\\n      without affecting response quality.\\n   - Negative Rejection [6] gauges the system’s ability to refrain from providing a\\n      response when the available information is insufficient.\\n   - Counterfactual Robustness [6] evaluates the system’s capacity to identify and\\n      disregard incorrect information, even when alerted about potential misinformation.\\n   - More: For more human preferences considerations, there can be more additional\\n      requirements, such as readability [57,33], toxicity, perplexity [33], etc.\\n\\n     For the exception, CRUD-RAG [39] introduces a comprehensive benchmark ad-\\ndressing the broader spectrum of RAG applications beyond question-answering, cat-\\negorized into Create, Read, Update, and Delete scenarios. This benchmark evaluates\\nRAG systems across diverse tasks, including text continuation, question answering,\\nhallucination modification, and multi-document summarization. It offers insights for\\noptimizing RAG technology across different scenarios. DomainRAG [58] identifies six\\ncomplex abilities for RAG systems: conversational, structural information, faithfulness,'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# KAN 7th page complete extracted information\n",
    "json_objs[0]['pages'][6]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QEnTFCn2pgB7",
    "outputId": "2688033d-24de-4660-ba02-84d92c85623b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'text',\n",
       " 'value': '2024. Table 1 portrays this information, where each evaluation criterion is represented by a different colour. For example, FeB4RAG [57], the fourth from the last, has posited four standards based on [17] that comprise Consistency, Correctness, Clarity, and Coverage. Correctness is equivalent to accuracy in retrieval, and Consistency is tantamount to faithfulness in the generation component. While accuracy in retrieval gauges the correctness of the retrieved information, we posit that Coverage pertains to the coverage rate and is more associated with diversity. Therefore, we consider Coverage to be linked with diversity and an additional requirement in our proposed evaluation framework, which will be introduced subsequently. The remaining standard, Clarity, is also classified as an additional requirement in our proposed framework. The other tools and benchmarks are processed similarly.\\n\\nTools and benchmarks offer varying degrees of flexibility in evaluating datasets for RAG systems. Tools, which specify only evaluation targets, provide a versatile framework capable of constructing complete RAG applications and evaluation pipelines, as seen in works like [54,32,33]. Benchmarks, on the other hand, focus on different aspects of RAG evaluation with specific emphasis on either retrieval outputs or generation targets. For instance, RAGAs [14] and ARES [49] assess the relevance of retrieval documents, while RGB and MultiHop-RAG [6,52] prioritize accuracy, necessitating comparison with GTs. The [66] focuses on the Hallucination, which is a combination of faithfulness and correctness. All benchmarks consider generation targets due to their critical role in RAG systems, though their focus areas vary.',\n",
       " 'md': '2024. Table 1 portrays this information, where each evaluation criterion is represented by a different colour. For example, FeB4RAG [57], the fourth from the last, has posited four standards based on [17] that comprise Consistency, Correctness, Clarity, and Coverage. Correctness is equivalent to accuracy in retrieval, and Consistency is tantamount to faithfulness in the generation component. While accuracy in retrieval gauges the correctness of the retrieved information, we posit that Coverage pertains to the coverage rate and is more associated with diversity. Therefore, we consider Coverage to be linked with diversity and an additional requirement in our proposed evaluation framework, which will be introduced subsequently. The remaining standard, Clarity, is also classified as an additional requirement in our proposed framework. The other tools and benchmarks are processed similarly.\\n\\nTools and benchmarks offer varying degrees of flexibility in evaluating datasets for RAG systems. Tools, which specify only evaluation targets, provide a versatile framework capable of constructing complete RAG applications and evaluation pipelines, as seen in works like [54,32,33]. Benchmarks, on the other hand, focus on different aspects of RAG evaluation with specific emphasis on either retrieval outputs or generation targets. For instance, RAGAs [14] and ARES [49] assess the relevance of retrieval documents, while RGB and MultiHop-RAG [6,52] prioritize accuracy, necessitating comparison with GTs. The [66] focuses on the Hallucination, which is a combination of faithfulness and correctness. All benchmarks consider generation targets due to their critical role in RAG systems, though their focus areas vary.',\n",
       " 'bBox': {'x': 134, 'y': 91, 'w': 346.48, 'h': 274.96}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Table information\n",
    "json_objs[0]['pages'][6]['items'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b2KgsfBKrxME",
    "outputId": "5253aba6-6c20-4fb7-91ae-976da6208f51"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'text',\n",
       " 'value': 'The generation component, powered by LLMs, produces coherent and contextually appropriate responses based on the retrieved content. The challenge here lies in evaluating the faithfulness and accuracy of the generated content to the input data. This involves not only assessing the factual correctness of responses but also their relevance to the original query and the coherence of the generated text [75,49]. The subjective nature of certain tasks, such as creative content generation or open-ended question answering, further complicates the evaluation, as it introduces variability in what constitutes a “correct” or “high-quality” response [48].',\n",
       " 'md': 'The generation component, powered by LLMs, produces coherent and contextually appropriate responses based on the retrieved content. The challenge here lies in evaluating the faithfulness and accuracy of the generated content to the input data. This involves not only assessing the factual correctness of responses but also their relevance to the original query and the coherence of the generated text [75,49]. The subjective nature of certain tasks, such as creative content generation or open-ended question answering, further complicates the evaluation, as it introduces variability in what constitutes a “correct” or “high-quality” response [48].',\n",
       " 'bBox': {'x': 134, 'y': 91, 'w': 345.82, 'h': 119.96}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_objs[0]['pages'][3]['items'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "KKy6jmjLyG3h"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "pTfBbY2l2czn"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
