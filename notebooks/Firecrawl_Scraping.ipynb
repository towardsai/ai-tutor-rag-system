{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9h3lrtQRhYv"
      },
      "source": [
        "## Install the requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "AKBQrqNsjrLB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "cb1a92ff-8875-4044-89d9-8279d7cdb8a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/56.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m48.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.8/454.8 kB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m48.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.2/80.2 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m51.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.4/40.4 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.1/211.1 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.1/45.1 MB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m58.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.4/253.4 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.9/104.9 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m492.9/492.9 kB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for html2text (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for tinysegmenter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for spider-client (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for feedfinder2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for jieba3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q llama-index==0.12.12 openai==1.59.6 tiktoken==0.8.0 llama-index-readers-web==0.3.4 firecrawl-py==1.10.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKh_dKV6Rm9a"
      },
      "source": [
        "### SET THE ENVIRONMENT VARIABLES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "QZgBdtZRJfze"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"<OPENAI_API_KEY>\"\n",
        "FIRECRAWL_API_KEY = \"<FIRECRAWL_API_KEY>\"\n",
        "\n",
        "# from google.colab import userdata\n",
        "# os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY1')\n",
        "# os.environ[\"FIRECRAWL_API_KEY\"] = userdata.get('FIRECRAWL_API_KEY')\n",
        "\n",
        "# FIRECRAWL_API_KEY = userdata.get('FIRECRAWL_API_KEY')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tiBy7Q74Vkt_"
      },
      "source": [
        "# SCRAPE WITH FIRECRAWL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQP8VA_gRr2g"
      },
      "source": [
        "## IMPORT THE FIRECRAWL WEBREADER\n",
        "\n",
        "Firecrawl allows you to turn entire websites into LLM-ready markdown\n",
        "\n",
        "Get the API key here\n",
        "https://www.firecrawl.dev/app/api-keys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "YuLILEoNj2lG"
      },
      "outputs": [],
      "source": [
        "from llama_index.readers.web import FireCrawlWebReader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "x-ls9L0QWC3S"
      },
      "outputs": [],
      "source": [
        "\n",
        "# using firecrawl to crawl a website\n",
        "firecrawl_reader = FireCrawlWebReader(\n",
        "    api_key=FIRECRAWL_API_KEY,  # Replace with your actual API key from https://www.firecrawl.dev/\n",
        "    mode=\"scrape\",\n",
        ")\n",
        "\n",
        "# Load documents from a single page URL\n",
        "documents = firecrawl_reader.load_data(url=\"https://towardsai.net/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "zL1BOBWhWrfe"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import VectorStoreIndex\n",
        "\n",
        "index = VectorStoreIndex.from_documents(documents)\n",
        "query_engine = index.as_query_engine()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "AV4Fg-haWspw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fec32478-6f5b-40e3-85e5-26977f064b72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Towards AI aims to be the world's leading artificial intelligence (AI) and technology publication, catering to thought-leaders and decision-makers globally by sharing high-quality publications, news, articles, and stories on AI and technology-related topics.\n",
            "-----------------\n",
            "Node ID\t 67dc199c-12c8-4207-b09b-c4d77d01b235\n",
            "Title\t Towards AI\n",
            "URL\t https://towardsai.net/\n",
            "Score\t 0.8477040848763497\n",
            "Description\t ['Towards AI is an online publication, which focuses on sharing high-quality publications, news, articles, and stories on AI and technology related topics.', 'Our thoughts and highlights from the latest AI news, models and papers. Read by over 120,000 AI practitioners and students. Click to read Towards AI Newsletter, a Substack publication.']\n",
            "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Node ID\t 1fa123a1-adfe-457a-b546-32b35b741318\n",
            "Title\t Towards AI\n",
            "URL\t https://towardsai.net/\n",
            "Score\t 0.8463061955785798\n",
            "Description\t ['Towards AI is an online publication, which focuses on sharing high-quality publications, news, articles, and stories on AI and technology related topics.', 'Our thoughts and highlights from the latest AI news, models and papers. Read by over 120,000 AI practitioners and students. Click to read Towards AI Newsletter, a Substack publication.']\n",
            "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n"
          ]
        }
      ],
      "source": [
        "res = query_engine.query(\"What is towards AI aim?\")\n",
        "\n",
        "print(res.response)\n",
        "\n",
        "print(\"-----------------\")\n",
        "# Show the retrieved nodes\n",
        "for src in res.source_nodes:\n",
        "  print(\"Node ID\\t\", src.node_id)\n",
        "  print(\"Title\\t\", src.metadata['title'])\n",
        "  print(\"URL\\t\", src.metadata['sourceURL'])\n",
        "  print(\"Score\\t\", src.score)\n",
        "  print(\"Description\\t\", src.metadata.get(\"description\"))\n",
        "  print(\"-_\"*20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRjJrc7VVaNX"
      },
      "source": [
        "# CRAWL A WEBSITE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uj6dqla8SEwd"
      },
      "source": [
        "## Load The CSV\n",
        "\n",
        "CSV contains the list of tools and url of the page which we use to get information about the tool."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "aWTgax6zZpLx"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import csv\n",
        "\n",
        "# Google Sheets file URL (CSV export link)\n",
        "url = 'https://docs.google.com/spreadsheets/d/1gHB-aQJGt9Nl3cyOP2GorAkBI_Us2AqkYnfqrmejStc/export?format=csv'\n",
        "\n",
        "# Send a GET request to fetch the CSV file\n",
        "response = requests.get(url)\n",
        "\n",
        "response_list = []\n",
        "# Check if the request was successful\n",
        "if response.status_code == 200:\n",
        "    # Decode the content to a string\n",
        "    content = response.content.decode('utf-8')\n",
        "\n",
        "    # Use the csv.DictReader to read the content as a dictionary\n",
        "    csv_reader = csv.DictReader(content.splitlines(), delimiter=',')\n",
        "    response_list = [row for row in csv_reader]\n",
        "else:\n",
        "    print(f\"Failed to retrieve the file: {response.status_code}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "atFrRz4MgmaR"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "start_index = random.randint(0, len(response_list) - 3)\n",
        "website_list = response_list[start_index:start_index+10] # Crawling 10 websites only."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "OHWjBFSQMWZk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db8ea8e7-781c-4984-f6cd-d3c40b3af6d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV data\n",
            "[{'': '',\n",
            "  'Category': 'Distributed Training',\n",
            "  'Company': 'Nvidia',\n",
            "  'Description': 'Multi-GPU and multi-node collective communication primitives',\n",
            "  'Is a direct URL company /tool website?': 'Yes',\n",
            "  'Name': 'NVIDIA NCCL',\n",
            "  'Parent': '',\n",
            "  'Tool Type': 'Library',\n",
            "  'URL': 'https://developer.nvidia.com/nccl'},\n",
            " {'': '',\n",
            "  'Category': 'Model Optimization',\n",
            "  'Company': '',\n",
            "  'Description': 'High-performance deep learning inference optimizer and '\n",
            "                 'runtime',\n",
            "  'Is a direct URL company /tool website?': 'Yes',\n",
            "  'Name': 'NVIDIA TensorRT',\n",
            "  'Parent': '',\n",
            "  'Tool Type': 'Library',\n",
            "  'URL': 'https://developer.nvidia.com/tensorrt'},\n",
            " {'': '',\n",
            "  'Category': 'Computer Vision',\n",
            "  'Company': '',\n",
            "  'Description': 'Computer vision and machine learning software library',\n",
            "  'Is a direct URL company /tool website?': 'Yes',\n",
            "  'Name': 'OpenCV',\n",
            "  'Parent': '',\n",
            "  'Tool Type': 'Library',\n",
            "  'URL': 'https://opencv.org/'},\n",
            " {'': '',\n",
            "  'Category': 'Computer Vision',\n",
            "  'Company': '',\n",
            "  'Description': 'Real-time multi-person keypoint detection library',\n",
            "  'Is a direct URL company /tool website?': 'No',\n",
            "  'Name': 'OpenPose',\n",
            "  'Parent': '',\n",
            "  'Tool Type': 'Library',\n",
            "  'URL': 'https://blog.roboflow.com/what-is-openpose/'},\n",
            " {'': '',\n",
            "  'Category': 'Data Processing',\n",
            "  'Company': '',\n",
            "  'Description': 'Data manipulation and analysis python library',\n",
            "  'Is a direct URL company /tool website?': 'Yes',\n",
            "  'Name': 'Pandas',\n",
            "  'Parent': 'Python',\n",
            "  'Tool Type': 'Library',\n",
            "  'URL': 'https://pandas-ai.com/'},\n",
            " {'': '',\n",
            "  'Category': 'Data Analysis',\n",
            "  'Company': '',\n",
            "  'Description': 'Generates profile reports from a pandas DataFrame',\n",
            "  'Is a direct URL company /tool website?': 'No',\n",
            "  'Name': 'Pandas-Profiling',\n",
            "  'Parent': 'Pandas, Python',\n",
            "  'Tool Type': 'Library',\n",
            "  'URL': 'https://pypi.org/project/pandas-profiling/'},\n",
            " {'': '',\n",
            "  'Category': 'Data Validation',\n",
            "  'Company': '',\n",
            "  'Description': 'Statistical data validation for pandas',\n",
            "  'Is a direct URL company /tool website?': 'Yes',\n",
            "  'Name': 'Pandera',\n",
            "  'Parent': 'Pandas, Python',\n",
            "  'Tool Type': 'Library',\n",
            "  'URL': 'https://pandera.readthedocs.io/en/stable/'},\n",
            " {'': '',\n",
            "  'Category': 'Web Development',\n",
            "  'Company': '',\n",
            "  'Description': 'High-level app and dashboarding solution for Python',\n",
            "  'Is a direct URL company /tool website?': 'No',\n",
            "  'Name': 'Panel',\n",
            "  'Parent': 'Python',\n",
            "  'Tool Type': 'Library',\n",
            "  'URL': 'https://pypi.org/project/panel/'},\n",
            " {'': '',\n",
            "  'Category': 'Web Mining',\n",
            "  'Company': '',\n",
            "  'Description': 'Web mining module for Python',\n",
            "  'Is a direct URL company /tool website?': 'No',\n",
            "  'Name': 'Pattern',\n",
            "  'Parent': 'Python',\n",
            "  'Tool Type': 'Library',\n",
            "  'URL': 'https://pypi.org/project/Pattern/'},\n",
            " {'': '',\n",
            "  'Category': 'Reinforcement Learning',\n",
            "  'Company': '',\n",
            "  'Description': 'Python library for conducting research in multi-agent '\n",
            "                 'reinforcement learning',\n",
            "  'Is a direct URL company /tool website?': 'Yes',\n",
            "  'Name': 'PettingZoo',\n",
            "  'Parent': 'Python',\n",
            "  'Tool Type': 'Library',\n",
            "  'URL': 'https://pettingzoo.farama.org/index.html'}]\n"
          ]
        }
      ],
      "source": [
        "import pprint\n",
        "print(\"CSV data\")\n",
        "pprint.pprint(website_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1SO6txzTBT_"
      },
      "source": [
        "## Initialize the Firecrawl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "o8GphTUy7IS_"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from firecrawl import FirecrawlApp\n",
        "app = FirecrawlApp(api_key=FIRECRAWL_API_KEY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "8-G8fB6KyGP7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de379a6b-9ad1-4c45-eab1-8a7aaf34a321"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Crawling: https://developer.nvidia.com/nccl\n",
            "Pausing for 1 minute to comply with crawl limit...\n",
            "Crawling: https://developer.nvidia.com/tensorrt\n",
            "Pausing for 1 minute to comply with crawl limit...\n",
            "Crawling: https://opencv.org/\n",
            "Pausing for 1 minute to comply with crawl limit...\n",
            "Crawling: https://blog.roboflow.com/what-is-openpose/\n",
            "Pausing for 1 minute to comply with crawl limit...\n",
            "Crawling: https://pandas-ai.com/\n",
            "Pausing for 1 minute to comply with crawl limit...\n",
            "Crawling: https://pypi.org/project/pandas-profiling/\n",
            "Pausing for 1 minute to comply with crawl limit...\n",
            "Crawling: https://pandera.readthedocs.io/en/stable/\n",
            "Pausing for 1 minute to comply with crawl limit...\n",
            "Crawling: https://pypi.org/project/panel/\n",
            "Pausing for 1 minute to comply with crawl limit...\n",
            "Crawling: https://pypi.org/project/Pattern/\n",
            "Pausing for 1 minute to comply with crawl limit...\n",
            "Crawling: https://pettingzoo.farama.org/index.html\n",
            "Pausing for 1 minute to comply with crawl limit...\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "# Crawl websites and handle responses\n",
        "url_response = {}\n",
        "crawl_per_min = 1  # Max crawl per minute\n",
        "\n",
        "# Track crawls\n",
        "crawled_websites = 0\n",
        "scraped_pages = 0\n",
        "\n",
        "for i, website_dict in enumerate(website_list):\n",
        "    url = website_dict.get('URL')\n",
        "    print(f\"Crawling: {url}\")\n",
        "\n",
        "    try:\n",
        "        response = app.crawl_url(\n",
        "            url,\n",
        "            params={\n",
        "                'limit': 5,  # Limit pages to scrape per site.\n",
        "                'scrapeOptions': {'formats': ['markdown', 'html']}\n",
        "            }\n",
        "        )\n",
        "        crawled_websites += 1\n",
        "\n",
        "    except Exception as exc:\n",
        "        print(f\"Failed to fetch {url} -> {exc}\")\n",
        "        continue\n",
        "\n",
        "    # Store the scraped data and associated info in the response dict\n",
        "    url_response[url] = {\n",
        "        \"scraped_data\": response.get(\"data\"),\n",
        "        \"csv_data\": website_dict\n",
        "    }\n",
        "\n",
        "    # Pause to comply with crawl per minute limit for free version its 1 crawl per minute\n",
        "    if i!=len(website_list) and (i + 1) % crawl_per_min == 0:\n",
        "        print(\"Pausing for 1 minute to comply with crawl limit...\")\n",
        "        time.sleep(60)  # Pause for 1 minute after every crawl\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwu6VIMvTKp1"
      },
      "source": [
        "## Create  llamaindex documents from the scraped content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "fHSEWg7FBdSS"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import Document\n",
        "documents = []\n",
        "\n",
        "for _, scraped_content in url_response.items():\n",
        "    csv_data = scraped_content.get(\"csv_data\")\n",
        "    scraped_results = scraped_content.get(\"scraped_data\")\n",
        "\n",
        "    for scraped_site_dict in scraped_results:\n",
        "        for result in scraped_results:\n",
        "            markdown_content = result.get(\"markdown\")\n",
        "            title = result.get(\"metadata\").get(\"title\")\n",
        "            url = result.get(\"metadata\").get(\"sourceURL\")\n",
        "            documents.append(\n",
        "                Document(\n",
        "                    text=markdown_content,\n",
        "                    metadata={\n",
        "                        \"title\": title,\n",
        "                        \"url\": url,\n",
        "                        \"description\": csv_data.get(\"Description\"),\n",
        "                        \"category\": csv_data.get(\"Category\")\n",
        "                    }\n",
        "                )\n",
        "            )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwtRhcpQT294"
      },
      "source": [
        "# Create The RAG Pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "iDOgwesPI-Kq"
      },
      "outputs": [],
      "source": [
        "from llama_index.llms.openai import OpenAI\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "\n",
        "llm = OpenAI(model=\"gpt-4o-mini\")\n",
        "embed_model = OpenAIEmbedding(model=\"text-embedding-3-large\")\n",
        "text_splitter = SentenceSplitter(chunk_size=512, chunk_overlap=30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "R8Oi4MiJJQii"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import Settings\n",
        "\n",
        "Settings.llm = llm\n",
        "Settings.embed_model = embed_model\n",
        "Settings.text_splitter = text_splitter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "61ZLk2GoJ4VH"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import VectorStoreIndex\n",
        "\n",
        "index = VectorStoreIndex.from_documents(documents)\n",
        "query_engine = index.as_query_engine()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "R1ImxmsoUNVo"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Markdown, display\n",
        "def display_response(response):\n",
        "    display(Markdown(f\"<b>{response}</b>\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "pcFcFgCvJ7Y8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "outputId": "f5e2b1b2-32ea-41e0-816f-eaf9d3de7bb2"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "<b>Real-time multi-person keypoint detection refers to the process of identifying and tracking specific points on the human body, such as joints and limbs, in a video stream or image. This technology enables the detection of multiple individuals simultaneously, allowing for applications in various fields such as healthcare, entertainment, and sports. By analyzing the positions of these keypoints, systems can interpret human movements and poses in real-time, facilitating interactive experiences and providing valuable insights for monitoring and analysis.</b>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------------\n",
            "Node ID\t b61fc79e-969d-429c-804e-f888272fbd8d\n",
            "Title\t What is OpenPose? A Guide for Beginners.\n",
            "URL\t https://blog.roboflow.com/what-is-openpose/\n",
            "Score\t 0.5726111768661022\n",
            "Description\t Real-time multi-person keypoint detection library\n",
            "Category\t Computer Vision\n",
            "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Node ID\t 578b8dc1-131d-4acb-b9a1-268f02925da9\n",
            "Title\t What is OpenPose? A Guide for Beginners.\n",
            "URL\t https://blog.roboflow.com/what-is-openpose/\n",
            "Score\t 0.5217128971469592\n",
            "Description\t Real-time multi-person keypoint detection library\n",
            "Category\t Computer Vision\n",
            "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n"
          ]
        }
      ],
      "source": [
        "query = \"What is Real-time multi-person keypoint detection\" # Enter your query here, it should be relevant to the crawled websites\n",
        "res = query_engine.query(query)\n",
        "display_response(res)\n",
        "\n",
        "print(\"-----------------\")\n",
        "# Show the retrieved nodes\n",
        "for src in res.source_nodes:\n",
        "  print(\"Node ID\\t\", src.node_id)\n",
        "  print(\"Title\\t\", src.metadata['title'])\n",
        "  print(\"URL\\t\", src.metadata['url'])\n",
        "  print(\"Score\\t\", src.score)\n",
        "  print(\"Description\\t\", src.metadata.get(\"description\"))\n",
        "  print(\"Category\\t\", src.metadata.get(\"category\"))\n",
        "  print(\"-_\"*20)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Querying not relevant to the crawled websites.\n",
        "\n",
        "query = \"What is qdrant?\"\n",
        "res = query_engine.query(query)\n",
        "display_response(res)\n",
        "\n",
        "print(\"-----------------\")\n",
        "# Show the retrieved nodes\n",
        "for src in res.source_nodes:\n",
        "  print(\"Node ID\\t\", src.node_id)\n",
        "  print(\"Title\\t\", src.metadata['title'])\n",
        "  print(\"URL\\t\", src.metadata['url'])\n",
        "  print(\"Score\\t\", src.score)\n",
        "  print(\"Description\\t\", src.metadata.get(\"description\"))\n",
        "  print(\"Category\\t\", src.metadata.get(\"category\"))\n",
        "  print(\"-_\"*20)"
      ],
      "metadata": {
        "id": "8Uw13m2AVFWf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "outputId": "5d4b1146-743d-4709-c2a5-b14c771c36b4"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "<b>The provided information does not include any details about Qdrant. It focuses on a data validation framework called Pandera, which is used for validating data in dataframe-like objects. For information about Qdrant, you may need to consult other sources.</b>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------------\n",
            "Node ID\t b9560b11-f0ca-41c5-95ac-a1afe5ad6c96\n",
            "Title\t pandera documentation\n",
            "URL\t https://pandera.readthedocs.io/en/stable/\n",
            "Score\t 0.1993697291056966\n",
            "Description\t Statistical data validation for pandas\n",
            "Category\t Data Validation\n",
            "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8zQtPrap1miR"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}