{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/towardsai/ai-tutor-rag-system/blob/notebook%2Faman/notebooks/FireCrawl_scraping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9h3lrtQRhYv"
      },
      "source": [
        "## Install the requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AKBQrqNsjrLB"
      },
      "outputs": [],
      "source": [
        "pip install -q llama-index==0.10.30 openai==1.12.0 tiktoken==0.6.0 llama-index-readers-web firecrawl-py==1.2.3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKh_dKV6Rm9a"
      },
      "source": [
        "### SET THE ENVIRONMENT VARIABLES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QZgBdtZRJfze"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"<OPENAI_API_KEY>\"\n",
        "FIRECRAWL_API_KEY = \"<FIRECRAWL_API_KEY>\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tiBy7Q74Vkt_"
      },
      "source": [
        "# SCRAPE WITH FIRECRAWL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQP8VA_gRr2g"
      },
      "source": [
        "## IMPORT THE FIRECRAWL WEBREADER\n",
        "\n",
        "Firecrawl allows you to turn entire websites into LLM-ready markdown\n",
        "\n",
        "Get the API key here\n",
        "https://www.firecrawl.dev/app/api-keys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YuLILEoNj2lG",
        "outputId": "293ad6c5-4859-46bb-e25a-b588c014867a"
      },
      "outputs": [],
      "source": [
        "from llama_index.readers.web import FireCrawlWebReader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x-ls9L0QWC3S"
      },
      "outputs": [],
      "source": [
        "\n",
        "# using firecrawl to crawl a website\n",
        "firecrawl_reader = FireCrawlWebReader(\n",
        "    api_key=FIRECRAWL_API_KEY,  # Replace with your actual API key from https://www.firecrawl.dev/\n",
        "    mode=\"scrape\",\n",
        ")\n",
        "\n",
        "# Load documents from a single page URL\n",
        "documents = firecrawl_reader.load_data(url=\"https://towardsai.net/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zL1BOBWhWrfe"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import VectorStoreIndex\n",
        "\n",
        "index = VectorStoreIndex.from_documents(documents)\n",
        "query_engine = index.as_query_engine()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AV4Fg-haWspw",
        "outputId": "04642e46-1f4f-4e89-972a-326cca793f34"
      },
      "outputs": [],
      "source": [
        "res = query_engine.query(\"What is towards AI aim?\")\n",
        "\n",
        "print(res.response)\n",
        "\n",
        "print(\"-----------------\")\n",
        "# Show the retrieved nodes\n",
        "for src in res.source_nodes:\n",
        "  print(\"Node ID\\t\", src.node_id)\n",
        "  print(\"Title\\t\", src.metadata['title'])\n",
        "  print(\"URL\\t\", src.metadata['sourceURL'])\n",
        "  print(\"Score\\t\", src.score)\n",
        "  print(\"Description\\t\", src.metadata.get(\"description\"))\n",
        "  print(\"-_\"*20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRjJrc7VVaNX"
      },
      "source": [
        "# CRAWL A WEBSITE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uj6dqla8SEwd"
      },
      "source": [
        "## Load The CSV\n",
        "\n",
        "CSV contains the list of tools and url of the page which we use to get information about the tool."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aWTgax6zZpLx"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import csv\n",
        "\n",
        "# Google Sheets file URL (CSV export link)\n",
        "url = 'https://docs.google.com/spreadsheets/d/1gHB-aQJGt9Nl3cyOP2GorAkBI_Us2AqkYnfqrmejStc/export?format=csv'\n",
        "\n",
        "# Send a GET request to fetch the CSV file\n",
        "response = requests.get(url)\n",
        "\n",
        "response_list = []\n",
        "# Check if the request was successful\n",
        "if response.status_code == 200:\n",
        "    # Decode the content to a string\n",
        "    content = response.content.decode('utf-8')\n",
        "\n",
        "    # Use the csv.DictReader to read the content as a dictionary\n",
        "    csv_reader = csv.DictReader(content.splitlines(), delimiter=',')\n",
        "    response_list = [row for row in csv_reader]\n",
        "else:\n",
        "    print(f\"Failed to retrieve the file: {response.status_code}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "atFrRz4MgmaR"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "start_index = random.randint(0, len(response_list) - 3)\n",
        "website_list = response_list[start_index:start_index+2] # crawling 2 website for demo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OHWjBFSQMWZk",
        "outputId": "f589564e-5cc3-4031-8481-55b3843d9380"
      },
      "outputs": [],
      "source": [
        "import pprint\n",
        "print(\"CSV data\")\n",
        "pprint.pprint(website_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1SO6txzTBT_"
      },
      "source": [
        "## Initialize the Firecrawl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o8GphTUy7IS_"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from firecrawl import FirecrawlApp\n",
        "app = FirecrawlApp(api_key=FIRECRAWL_API_KEY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8-G8fB6KyGP7",
        "outputId": "a06cfbb4-a5c0-4a49-8c8e-254943b34c52"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "# Crawl websites and handle responses\n",
        "url_response = {}\n",
        "crawl_per_min = 1  # Max crawl per minute\n",
        "\n",
        "# Track crawls\n",
        "crawled_websites = 0\n",
        "scraped_pages = 0\n",
        "\n",
        "for i, website_dict in enumerate(website_list):\n",
        "    url = website_dict.get('URL')\n",
        "    print(f\"Crawling: {url}\")\n",
        "\n",
        "    try:\n",
        "        response = app.crawl_url(\n",
        "            url,\n",
        "            params={\n",
        "                'limit': 3,  # Limit pages to scrape per site.\n",
        "                'scrapeOptions': {'formats': ['markdown', 'html']}\n",
        "            }\n",
        "        )\n",
        "        crawled_websites += 1\n",
        "\n",
        "    except Exception as exc:\n",
        "        print(f\"Failed to fetch {url} -> {exc}\")\n",
        "        continue\n",
        "\n",
        "    # Store the scraped data and associated info in the response dict\n",
        "    url_response[url] = {\n",
        "        \"scraped_data\": response.get(\"data\"),\n",
        "        \"csv_data\": website_dict\n",
        "    }\n",
        "\n",
        "    # Pause to comply with crawl per minute limit for free version its 1 crawl per minute\n",
        "    if i!=len(website_list) and (i + 1) % crawl_per_min == 0:\n",
        "        print(\"Pausing for 1 minute to comply with crawl limit...\")\n",
        "        time.sleep(60)  # Pause for 1 minute after every crawl\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwu6VIMvTKp1"
      },
      "source": [
        "## Create  llamaindex documents from the scraped content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fHSEWg7FBdSS"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import Document\n",
        "documents = []\n",
        "\n",
        "for _, scraped_content in url_response.items():\n",
        "    csv_data = scraped_content.get(\"csv_data\")\n",
        "    scraped_results = scraped_content.get(\"scraped_data\")\n",
        "\n",
        "    for scraped_site_dict in scraped_results:\n",
        "        for result in scraped_results:\n",
        "            markdown_content = result.get(\"markdown\")\n",
        "            title = result.get(\"metadata\").get(\"title\")\n",
        "            url = result.get(\"metadata\").get(\"sourceURL\")\n",
        "            documents.append(\n",
        "                Document(\n",
        "                    text=markdown_content,\n",
        "                    metadata={\n",
        "                        \"title\": title,\n",
        "                        \"url\": url,\n",
        "                        \"description\": csv_data.get(\"Description\"),\n",
        "                        \"category\": csv_data.get(\"Category\")\n",
        "                    }\n",
        "                )\n",
        "            )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwtRhcpQT294"
      },
      "source": [
        "# Create The RAG Pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iDOgwesPI-Kq"
      },
      "outputs": [],
      "source": [
        "from llama_index.llms.openai import OpenAI\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "\n",
        "llm = OpenAI(model=\"gpt-4o-mini\")\n",
        "embed_model = OpenAIEmbedding(model=\"text-embedding-3-large\")\n",
        "text_splitter = SentenceSplitter(chunk_size=512, chunk_overlap=30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R8Oi4MiJJQii"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import Settings\n",
        "\n",
        "Settings.llm = llm\n",
        "Settings.embed_model = embed_model\n",
        "Settings.text_splitter = text_splitter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "61ZLk2GoJ4VH"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import VectorStoreIndex\n",
        "\n",
        "index = VectorStoreIndex.from_documents(documents)\n",
        "query_engine = index.as_query_engine()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R1ImxmsoUNVo"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Markdown, display\n",
        "def display_response(response):\n",
        "    display(Markdown(f\"<b>{response}</b>\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 616
        },
        "id": "pcFcFgCvJ7Y8",
        "outputId": "84449163-47ed-4d15-a080-2a18600e5bc4"
      },
      "outputs": [],
      "source": [
        "query = \"Compare the best No sql dbs?\" # Enter your query here, it should be relevant to the crawled websites\n",
        "res = query_engine.query(\"I want to use key value store which is the best db?\")\n",
        "display_response(res)\n",
        "\n",
        "print(\"-----------------\")\n",
        "# Show the retrieved nodes\n",
        "for src in res.source_nodes:\n",
        "  print(\"Node ID\\t\", src.node_id)\n",
        "  print(\"Title\\t\", src.metadata['title'])\n",
        "  print(\"URL\\t\", src.metadata['url'])\n",
        "  print(\"Score\\t\", src.score)\n",
        "  print(\"Description\\t\", src.metadata.get(\"description\"))\n",
        "  print(\"Category\\t\", src.metadata.get(\"category\"))\n",
        "  print(\"-_\"*20)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyP4qCoBLDDxEkxyJ9OmPq0w",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
