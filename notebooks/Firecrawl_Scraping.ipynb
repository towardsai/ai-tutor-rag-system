{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AGMuJyTQJm4L"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/towardsai/ai-tutor-rag-system/blob/main/notebooks/Firecrawl_Scraping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z9h3lrtQRhYv"
   },
   "source": [
    "## Install the requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "AKBQrqNsjrLB",
    "outputId": "44dbbeb7-0982-414c-d6a3-a7128053c3bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m50.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.8/96.8 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m951.0/951.0 kB\u001b[0m \u001b[31m40.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m96.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m303.3/303.3 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/41.0 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.1/211.1 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.9/45.9 MB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m87.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.9/43.9 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.5/81.5 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.5/42.5 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.9/63.9 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.2/313.2 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m42.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.4/107.4 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m499.2/499.2 kB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.4/144.4 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for html2text (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for tinysegmenter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for spider-client (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for feedfinder2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for jieba3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "!pip install -q llama-index==0.12.48 llama-index-readers-web==0.3.7 openai==1.107.0 jedi==0.19.2 firecrawl-py==1.15.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TKh_dKV6Rm9a"
   },
   "source": [
    "### SET THE ENVIRONMENT VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QZgBdtZRJfze"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"<OPENAI_API_KEY>\"\n",
    "# FIRECRAWL_API_KEY = \"<FIRECRAWL_API_KEY>\"\n",
    "\n",
    "from google.colab import userdata\n",
    "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
    "os.environ[\"FIRECRAWL_API_KEY\"] = userdata.get('FIRECRAWL_API_KEY')\n",
    "\n",
    "FIRECRAWL_API_KEY = userdata.get('FIRECRAWL_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tiBy7Q74Vkt_"
   },
   "source": [
    "# SCRAPE WITH FIRECRAWL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GQP8VA_gRr2g"
   },
   "source": [
    "## IMPORT THE FIRECRAWL WEBREADER\n",
    "\n",
    "Firecrawl allows you to turn entire websites into LLM-ready markdown\n",
    "\n",
    "Get the API key here\n",
    "https://www.firecrawl.dev/app/api-keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x-ls9L0QWC3S"
   },
   "outputs": [],
   "source": [
    "from llama_index.readers.web import FireCrawlWebReader\n",
    "\n",
    "# using firecrawl to crawl a website\n",
    "firecrawl_reader = FireCrawlWebReader(\n",
    "    api_key=FIRECRAWL_API_KEY,  # Replace with your actual API key from https://www.firecrawl.dev/\n",
    "    mode=\"scrape\",\n",
    "    params={'formats':[\"markdown\", \"html\"]},\n",
    ")\n",
    "\n",
    "# Load documents from a single page URL\n",
    "documents = firecrawl_reader.load_data(url=\"https://towardsai.net/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zL1BOBWhWrfe",
    "outputId": "0e9bf385-c858-4603-b4e9-c6e7612219eb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex, Settings\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "# To increase chunk size globally\n",
    "# Settings.chunk_size = 2048  # or even larger like 4096\n",
    "# Settings.chunk_overlap = 200\n",
    "\n",
    "\n",
    "# node parser with larger chunk size only for this index\n",
    "node_parser = SentenceSplitter(\n",
    "    chunk_size=2048,\n",
    "    chunk_overlap=200,\n",
    ")\n",
    "\n",
    "index = VectorStoreIndex.from_documents(documents, transformations=[node_parser])\n",
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AV4Fg-haWspw",
    "outputId": "35a553cb-f511-4ddb-c6fa-df0792ffbc44"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Towards AI aims to make artificial intelligence (AI) accessible to all by offering practical AI Engineering courses, corporate AI bootcamps, and LLM development consultancy for individuals and companies.\n",
      "-----------------\n",
      "Node ID\t 4dc9135e-c6d5-4150-9689-c2a6c0b3cff5\n",
      "Title\t Towards AI\n",
      "URL\t https://towardsai.net/\n",
      "Score\t 0.8569113365360227\n",
      "Description\t Master AI with Towards AI. We offer practical AI Engineering courses, corporate AI bootcamps, and LLM development consultancy for individuals and companies.\n",
      "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
      "Node ID\t afe6e5e9-d5be-4bf3-bbbc-33ee5c9fa2b0\n",
      "Title\t Towards AI\n",
      "URL\t https://towardsai.net/\n",
      "Score\t 0.8566070222435507\n",
      "Description\t Master AI with Towards AI. We offer practical AI Engineering courses, corporate AI bootcamps, and LLM development consultancy for individuals and companies.\n",
      "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n"
     ]
    }
   ],
   "source": [
    "res = query_engine.query(\"What is towards AI aim?\")\n",
    "\n",
    "print(res.response)\n",
    "\n",
    "print(\"-----------------\")\n",
    "# Show the retrieved nodes\n",
    "for src in res.source_nodes:\n",
    "  print(\"Node ID\\t\", src.node_id)\n",
    "  print(\"Title\\t\", src.metadata['title'])\n",
    "  print(\"URL\\t\", src.metadata['sourceURL'])\n",
    "  print(\"Score\\t\", src.score)\n",
    "  print(\"Description\\t\", src.metadata.get(\"description\"))\n",
    "  print(\"-_\"*20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lRjJrc7VVaNX"
   },
   "source": [
    "# CRAWL A WEBSITE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uj6dqla8SEwd"
   },
   "source": [
    "## Load The CSV\n",
    "\n",
    "CSV contains the list of tools and url of the page which we use to get information about the tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aWTgax6zZpLx"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "\n",
    "# Google Sheets file URL (CSV export link)\n",
    "url = 'https://docs.google.com/spreadsheets/d/1gHB-aQJGt9Nl3cyOP2GorAkBI_Us2AqkYnfqrmejStc/export?format=csv'\n",
    "\n",
    "# Send a GET request to fetch the CSV file\n",
    "response = requests.get(url)\n",
    "\n",
    "response_list = []\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Decode the content to a string\n",
    "    content = response.content.decode('utf-8')\n",
    "\n",
    "    # Use the csv.DictReader to read the content as a dictionary\n",
    "    csv_reader = csv.DictReader(content.splitlines(), delimiter=',')\n",
    "    response_list = [row for row in csv_reader]\n",
    "else:\n",
    "    print(f\"Failed to retrieve the file: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "atFrRz4MgmaR"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "start_index = random.randint(0, len(response_list) - 3)\n",
    "website_list = response_list[start_index:start_index+10] # Crawling 10 websites only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OHWjBFSQMWZk",
    "outputId": "2c91cdf6-4a64-4b2c-e998-842f7fc314d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV data\n",
      "[{'': '',\n",
      "  'Category': 'Search Library',\n",
      "  'Company': '',\n",
      "  'Description': 'Fast, pure Python full text indexing, search, and spell '\n",
      "                 'checking library',\n",
      "  'Is a direct URL company /tool website?': 'No',\n",
      "  'Name': 'Whoosh',\n",
      "  'Parent': '',\n",
      "  'Tool Type': 'Library',\n",
      "  'URL': 'https://pypi.org/project/Whoosh/'},\n",
      " {'': '',\n",
      "  'Category': 'Machine Learning',\n",
      "  'Company': '',\n",
      "  'Description': 'An optimized distributed gradient boosting library designed '\n",
      "                 'to be highly efficient, flexible and portable',\n",
      "  'Is a direct URL company /tool website?': 'Yes',\n",
      "  'Name': 'XGBoost',\n",
      "  'Parent': '',\n",
      "  'Tool Type': 'Library',\n",
      "  'URL': 'https://xgboost.readthedocs.io/en/stable/'},\n",
      " {'': '',\n",
      "  'Category': 'Message Queue',\n",
      "  'Company': '',\n",
      "  'Description': 'High-performance asynchronous messaging library',\n",
      "  'Is a direct URL company /tool website?': 'Yes',\n",
      "  'Name': 'ZeroMQ',\n",
      "  'Parent': '',\n",
      "  'Tool Type': 'Library',\n",
      "  'URL': 'https://zeromq.org/'},\n",
      " {'': '',\n",
      "  'Category': 'Web Automation',\n",
      "  'Company': '',\n",
      "  'Description': 'Library which provides a high-level API to control Chrome or '\n",
      "                 'Chromium',\n",
      "  'Is a direct URL company /tool website?': 'Yes',\n",
      "  'Name': 'Puppeteer',\n",
      "  'Parent': 'JavaScript, Node.JS',\n",
      "  'Tool Type': 'Library, Scraping',\n",
      "  'URL': 'https://pptr.dev/'},\n",
      " {'': '',\n",
      "  'Category': 'Language Model',\n",
      "  'Company': '',\n",
      "  'Description': 'Bidirectional Encoder Representations from Transformers',\n",
      "  'Is a direct URL company /tool website?': 'No',\n",
      "  'Name': 'BERT',\n",
      "  'Parent': '',\n",
      "  'Tool Type': 'Model',\n",
      "  'URL': 'https://www.nvidia.com/en-us/glossary/bert/#:~:text=BERT%20is%20a%20model%20for,RoBERTa%2C%20ALBERT%2C%20and%20DistilBERT.'},\n",
      " {'': '',\n",
      "  'Category': 'Synthetic Data Generation',\n",
      "  'Company': '',\n",
      "  'Description': 'Conditional Tabular GAN',\n",
      "  'Is a direct URL company /tool website?': 'No',\n",
      "  'Name': 'CTGAN',\n",
      "  'Parent': '',\n",
      "  'Tool Type': 'Model',\n",
      "  'URL': 'https://sdv.dev/SDV/user_guides/single_table/ctgan.html'},\n",
      " {'': '',\n",
      "  'Category': 'Deep Learning & Neural Networks',\n",
      "  'Company': '',\n",
      "  'Description': 'Generates multiple image outputs based on your textual '\n",
      "                 'description',\n",
      "  'Is a direct URL company /tool website?': 'Yes',\n",
      "  'Name': 'DALLE3',\n",
      "  'Parent': '',\n",
      "  'Tool Type': 'Model',\n",
      "  'URL': 'https://dalle3.ai/'},\n",
      " {'': '',\n",
      "  'Category': 'Computer Vision',\n",
      "  'Company': '',\n",
      "  'Description': 'Semantic image segmentation model',\n",
      "  'Is a direct URL company /tool website?': 'No',\n",
      "  'Name': 'DeepLabV3+',\n",
      "  'Parent': '',\n",
      "  'Tool Type': 'Model',\n",
      "  'URL': 'https://wiki.cloudfactory.com/docs/mp-wiki/model-architectures/deeplabv3'},\n",
      " {'': '',\n",
      "  'Category': 'Computer Vision',\n",
      "  'Company': '',\n",
      "  'Description': 'Scalable and efficient object detection',\n",
      "  'Is a direct URL company /tool website?': 'No',\n",
      "  'Name': 'EfficientDet',\n",
      "  'Parent': '',\n",
      "  'Tool Type': 'Model',\n",
      "  'URL': 'https://paperswithcode.com/method/efficientdet'},\n",
      " {'': '',\n",
      "  'Category': 'NLP',\n",
      "  'Company': '',\n",
      "  'Description': 'Neural network machine learning model trained using internet '\n",
      "                 'data to generate any type of text',\n",
      "  'Is a direct URL company /tool website?': 'Yes',\n",
      "  'Name': 'GPT-3',\n",
      "  'Parent': '',\n",
      "  'Tool Type': 'Model',\n",
      "  'URL': 'https://openai.com/index/gpt-3-apps/'}]\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "print(\"CSV data\")\n",
    "pprint.pprint(website_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S1SO6txzTBT_"
   },
   "source": [
    "## Initialize the Firecrawl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o8GphTUy7IS_"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from firecrawl import FirecrawlApp\n",
    "app = FirecrawlApp(api_key=FIRECRAWL_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8-G8fB6KyGP7",
    "outputId": "3f65ec79-3f23-4099-fc77-a38ada788b33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling: https://pypi.org/project/Whoosh/\n",
      "Pausing for 1 minute to comply with crawl limit...\n",
      "Crawling: https://xgboost.readthedocs.io/en/stable/\n",
      "Pausing for 1 minute to comply with crawl limit...\n",
      "Crawling: https://zeromq.org/\n",
      "Pausing for 1 minute to comply with crawl limit...\n",
      "Crawling: https://pptr.dev/\n",
      "Pausing for 1 minute to comply with crawl limit...\n",
      "Crawling: https://www.nvidia.com/en-us/glossary/bert/#:~:text=BERT%20is%20a%20model%20for,RoBERTa%2C%20ALBERT%2C%20and%20DistilBERT.\n",
      "Pausing for 1 minute to comply with crawl limit...\n",
      "Crawling: https://sdv.dev/SDV/user_guides/single_table/ctgan.html\n",
      "Pausing for 1 minute to comply with crawl limit...\n",
      "Crawling: https://dalle3.ai/\n",
      "Pausing for 1 minute to comply with crawl limit...\n",
      "Crawling: https://wiki.cloudfactory.com/docs/mp-wiki/model-architectures/deeplabv3\n",
      "Pausing for 1 minute to comply with crawl limit...\n",
      "Crawling: https://paperswithcode.com/method/efficientdet\n",
      "Pausing for 1 minute to comply with crawl limit...\n",
      "Crawling: https://openai.com/index/gpt-3-apps/\n",
      "Pausing for 1 minute to comply with crawl limit...\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Crawl websites and handle responses\n",
    "url_response = {}\n",
    "crawl_per_min = 1  # Max crawl per minute\n",
    "\n",
    "# Track crawls\n",
    "crawled_websites = 0\n",
    "scraped_pages = 0\n",
    "\n",
    "for i, website_dict in enumerate(website_list):\n",
    "    url = website_dict.get('URL')\n",
    "    print(f\"Crawling: {url}\")\n",
    "\n",
    "    try:\n",
    "        response = app.crawl_url(\n",
    "            url,\n",
    "            params={\n",
    "                'limit': 5,  # Limit pages to scrape per site.\n",
    "                'scrapeOptions': {'formats': ['markdown', 'html']}\n",
    "            }\n",
    "        )\n",
    "        crawled_websites += 1\n",
    "\n",
    "    except Exception as exc:\n",
    "        print(f\"Failed to fetch {url} -> {exc}\")\n",
    "        continue\n",
    "\n",
    "    # Store the scraped data and associated info in the response dict\n",
    "    url_response[url] = {\n",
    "        \"scraped_data\": response.get(\"data\"),\n",
    "        \"csv_data\": website_dict\n",
    "    }\n",
    "\n",
    "    # Pause to comply with crawl per minute limit for free version its 1 crawl per minute\n",
    "    if i!=len(website_list) and (i + 1) % crawl_per_min == 0:\n",
    "        print(\"Pausing for 1 minute to comply with crawl limit...\")\n",
    "        time.sleep(60)  # Pause for 1 minute after every crawl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gwu6VIMvTKp1"
   },
   "source": [
    "## Create  llamaindex documents from the scraped content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fHSEWg7FBdSS"
   },
   "outputs": [],
   "source": [
    "from llama_index.core import Document\n",
    "documents = []\n",
    "\n",
    "for _, scraped_content in url_response.items():\n",
    "    csv_data = scraped_content.get(\"csv_data\")\n",
    "    scraped_results = scraped_content.get(\"scraped_data\")\n",
    "\n",
    "    for scraped_site_dict in scraped_results:\n",
    "        for result in scraped_results:\n",
    "            markdown_content = result.get(\"markdown\")\n",
    "            title = result.get(\"metadata\").get(\"title\")\n",
    "            url = result.get(\"metadata\").get(\"sourceURL\")\n",
    "            documents.append(\n",
    "                Document(\n",
    "                    text=markdown_content,\n",
    "                    metadata={\n",
    "                        \"title\": title,\n",
    "                        \"url\": url,\n",
    "                        \"description\": csv_data.get(\"Description\"),\n",
    "                        \"category\": csv_data.get(\"Category\")\n",
    "                    }\n",
    "                )\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uwtRhcpQT294"
   },
   "source": [
    "# Create The RAG Pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iDOgwesPI-Kq"
   },
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "Settings.llm = OpenAI(model=\"gpt-4o-mini\")\n",
    "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")\n",
    "Settings.text_splitter = SentenceSplitter(chunk_size=512, chunk_overlap=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "61ZLk2GoJ4VH",
    "outputId": "313c29fc-ed6e-415d-cf83-58c6222d54aa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:numexpr.utils:NumExpr defaulting to 2 threads.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R1ImxmsoUNVo"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "def display_response(response):\n",
    "    display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "pcFcFgCvJ7Y8",
    "outputId": "22bd7ce5-b603-4e75-8b54-f101a6919e53"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<b>Graph retrieval-augmented generation (GraphRAG) is a framework designed to enhance large language models by organizing fragmented knowledge into structured graphs. This approach addresses the challenges of complex reasoning by improving the way knowledge is constructed and retrieved. The framework integrates various components to optimize performance, particularly in scenarios where domain shifts occur. It utilizes a seed graph schema to guide the extraction of relevant entities and relationships, while also employing community detection techniques to enhance knowledge organization. This results in a more cohesive and efficient reasoning process, allowing for better adaptability and performance across different tasks and domains.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------\n",
      "Node ID\t 1da23efc-071a-43f6-a408-9923e74cdf48\n",
      "Title\t Trending Papers - Hugging Face\n",
      "URL\t https://paperswithcode.com/method/efficientdet\n",
      "Score\t 0.5759035989233018\n",
      "Description\t Scalable and efficient object detection\n",
      "Category\t Computer Vision\n",
      "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
      "Node ID\t 3f9313aa-0be7-4953-b725-506e7431c544\n",
      "Title\t Trending Papers - Hugging Face\n",
      "URL\t https://paperswithcode.com/method/efficientdet\n",
      "Score\t 0.4738705323473313\n",
      "Description\t Scalable and efficient object detection\n",
      "Category\t Computer Vision\n",
      "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n"
     ]
    }
   ],
   "source": [
    "query = \"Explain GraphRAG\" # Enter your query here, it should be relevant to the crawled websites\n",
    "res = query_engine.query(query)\n",
    "display_response(res)\n",
    "\n",
    "print(\"-----------------\")\n",
    "# Show the retrieved nodes\n",
    "for src in res.source_nodes:\n",
    "  print(\"Node ID\\t\", src.node_id)\n",
    "  print(\"Title\\t\", src.metadata['title'])\n",
    "  print(\"URL\\t\", src.metadata['url'])\n",
    "  print(\"Score\\t\", src.score)\n",
    "  print(\"Description\\t\", src.metadata.get(\"description\"))\n",
    "  print(\"Category\\t\", src.metadata.get(\"category\"))\n",
    "  print(\"-_\"*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 231
    },
    "id": "8Uw13m2AVFWf",
    "outputId": "53954541-ebaa-47e0-8ded-87ee73f4b0dd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<b>The provided information does not include any details about Qdrant. Therefore, I cannot provide an answer regarding what Qdrant is.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------\n",
      "Node ID\t 89cde9fe-5235-4266-bfcb-0ac9203a0a57\n",
      "Title\t Compare Features | Synthetic Data Vault\n",
      "URL\t https://docs.sdv.dev/sdv/explore/sdv-enterprise/compare-features\n",
      "Score\t 0.29939159415659344\n",
      "Description\t Conditional Tabular GAN\n",
      "Category\t Synthetic Data Generation\n",
      "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n"
     ]
    }
   ],
   "source": [
    "# Querying not relevant to the crawled websites.\n",
    "\n",
    "query = \"What is qdrant?\"\n",
    "res = query_engine.query(query)\n",
    "display_response(res)\n",
    "\n",
    "print(\"-----------------\")\n",
    "# Show the retrieved nodes\n",
    "for src in res.source_nodes:\n",
    "  print(\"Node ID\\t\", src.node_id)\n",
    "  print(\"Title\\t\", src.metadata['title'])\n",
    "  print(\"URL\\t\", src.metadata['url'])\n",
    "  print(\"Score\\t\", src.score)\n",
    "  print(\"Description\\t\", src.metadata.get(\"description\"))\n",
    "  print(\"Category\\t\", src.metadata.get(\"category\"))\n",
    "  print(\"-_\"*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8zQtPrap1miR"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
