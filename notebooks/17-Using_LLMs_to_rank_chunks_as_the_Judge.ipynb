{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0FbELaf7TrW7"
   },
   "source": [
    "# Install Packages and Setup Variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "Yubz8AanRRSW",
    "outputId": "1a14f62c-60c4-485a-a78a-2bb657272149"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.7/150.7 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m455.6/455.6 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m584.3/584.3 kB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m66.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m679.1/679.1 kB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.4/15.4 MB\u001b[0m \u001b[31m47.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.6/278.6 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m65.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m50.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m88.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.5/52.5 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.7/149.7 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.0/64.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m81.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.8/69.8 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.8/130.8 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.8/295.8 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m47.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m71.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m452.6/452.6 kB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.6/49.6 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "!pip install -q huggingface-hub==0.27.1 llama-index==0.10.49 llama-index-vector-stores-chroma==0.1.9 google-generativeai==0.5.4 llama-index-llms-gemini==0.1.11 openai==1.59.8 chromadb==0.5.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "xLXFuRW-TpUu"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set the following API Keys in the Python environment. Will be used later.\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"[OPENAI_API_KEY]\"\n",
    "# os.environ[\"GOOGLE_API_KEY\"] = \"<YOUR_API_KEY>\"\n",
    "\n",
    "from google.colab import userdata\n",
    "os.environ[\"OPENAI_API_KEY\"] = userdata.get('openai_api_key')\n",
    "os.environ[\"GOOGLE_API_KEY\"] = userdata.get('Google_api_key')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r6GCYYqqTuMc"
   },
   "source": [
    "# Load a Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "pupJpdZaTu5m"
   },
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core import Settings\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "Settings.llm = OpenAI(temperature=1, model=\"gpt-4o-mini\")\n",
    "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YSGHsZMMZj4E"
   },
   "source": [
    "# Load Indexes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z1lRMQtdOQ95"
   },
   "source": [
    "Downloading vector store from Huggingface hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "82abee68690d4fc9a7915c2bb5d98689",
      "745bf9a2282e4fa595c37a5c62a51a28",
      "d98fa568f16542149c2a730a03c23deb",
      "ba045eb616f14cec9f29bdc277333af4",
      "112807c2d725402fbf3b0a8baf6d8d6a",
      "afdabbfdf55845b39882f9f69d6fe919",
      "c76724a230bd498396b4e7bddd2952db",
      "f832c771c19944f29719181d79f764ca",
      "af6c922addca43718e3d2d4fb1d2767d",
      "e74e462d43a24eec9eca6d298e278c57",
      "6acf69ef1c664521b63ef0ec6d5a2caf"
     ]
    },
    "id": "o8Riolm_2iYd",
    "outputId": "9faa4703-1f6b-47d9-9e97-b0255372491e"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82abee68690d4fc9a7915c2bb5d98689",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vectorstore.zip:   0%|          | 0.00/97.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "vectorstore = hf_hub_download(repo_id=\"jaiganesan/ai_tutor_knowledge\", filename=\"vectorstore.zip\",repo_type=\"dataset\",local_dir=\"/content\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M8iaOOGyZeNp",
    "outputId": "fce4fcc1-e43d-46f5-ed20-450d6f3c5370"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  /content/vectorstore.zip\n",
      "   creating: ai_tutor_knowledge/\n",
      "   creating: ai_tutor_knowledge/684af133-f877-4230-bde4-575cf53b6688/\n",
      "  inflating: ai_tutor_knowledge/684af133-f877-4230-bde4-575cf53b6688/length.bin  \n",
      "  inflating: ai_tutor_knowledge/684af133-f877-4230-bde4-575cf53b6688/index_metadata.pickle  \n",
      "  inflating: ai_tutor_knowledge/684af133-f877-4230-bde4-575cf53b6688/link_lists.bin  \n",
      "  inflating: ai_tutor_knowledge/684af133-f877-4230-bde4-575cf53b6688/header.bin  \n",
      "  inflating: ai_tutor_knowledge/684af133-f877-4230-bde4-575cf53b6688/data_level0.bin  \n",
      "  inflating: ai_tutor_knowledge/chroma.sqlite3  \n"
     ]
    }
   ],
   "source": [
    "!unzip /content/vectorstore.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "6tzS_EKPZeLS"
   },
   "outputs": [],
   "source": [
    "# Load the vector store from the local storage.\n",
    "import chromadb\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "\n",
    "db2 = chromadb.PersistentClient(path=\"/content/ai_tutor_knowledge\")\n",
    "chroma_collection = db2.get_or_create_collection(\"ai_tutor_knowledge\")\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "0T6FL7J3ZrNK"
   },
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "# Create the index based on the vector store.\n",
    "index = VectorStoreIndex.from_vector_store(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "b8OilXIR8IO9"
   },
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine(similarity_top_k=10)\n",
    "\n",
    "res = query_engine.query(\"Explain how Advance RAG works?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "id": "8RMKJJv98R2o",
    "outputId": "809a0150-525a-4583-a480-6bfa16969aed"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Advance Retrieval-Augmented Generation (RAG) systems incorporate various methodologies to enhance the quality and efficiency of contextual information retrieval for language generation tasks. These systems utilize specialized techniques such as Self-Reflective RAG (Self-RAG) and Corrective RAG (CRAG), which focus on improving the relevance and accuracy of retrieved documents. \\n\\nSpecifically, Advance RAG methods, like SPECULATIVE RAG, aim to optimize processing speed while enhancing reasoning capabilities. They achieve this through a divide-and-conquer strategy, where a smaller specialized language model (the RAG drafter) generates multiple answer drafts from subsets of retrieved documents. These drafts are then verified by a larger generalist language model (the RAG verifier), which evaluates their quality and selects the best candidate for inclusion in the final output.\\n\\nThis collaborative approach allows for the generation of diverse and high-quality responses while minimizing computational load and latency issues. Overall, Advance RAG represents a significant evolution in integrating retrieval mechanisms with language generation, addressing challenges such as outdated information and hallucinations in knowledge-intensive tasks.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w2XBkzNwLle5"
   },
   "source": [
    "# RankGPT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "_it2CxTtLmHT"
   },
   "outputs": [],
   "source": [
    "from llama_index.core.postprocessor.rankGPT_rerank import RankGPTRerank\n",
    "\n",
    "rankGPT = RankGPTRerank(top_n=3,llm=Settings.llm, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YA3M9m9CL6AJ",
    "outputId": "f97e5beb-545b-43d7-b7bb-335b9a620ac9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Reranking, new rank list for nodes: [1, 0, 3, 9, 8, 7, 5, 6, 2, 4]"
     ]
    }
   ],
   "source": [
    "# Define a query engine that is responsible for retrieving related pieces of text,\n",
    "# and using a LLM to formulate the final answer.\n",
    "# The `node_postprocessors` function will be applied to the retrieved nodes.\n",
    "query_engine = index.as_query_engine(similarity_top_k=10, node_postprocessors=[rankGPT])\n",
    "\n",
    "res = query_engine.query(\"Explain how Retrieval Augmented Generation (RAG) works?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "id": "wgyjv9e6MCVm",
    "outputId": "35b001e0-95ff-4286-8bea-6d2ac17e126b"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"Retrieval-Augmented Generation (RAG) combines two main components: Retrieval and Generation. \\n\\n1. **Retrieval Component**: \\n   - This component extracts relevant information from external knowledge sources. It comprises two key phases:\\n     - **Indexing**: Organizes documents efficiently for retrieval, utilizing inverted indexes or dense vector encoding.\\n     - **Searching**: Identifies and fetches relevant documents based on user queries. This process may be supplemented by rerankers that refine the ranking of retrieved documents for better relevance.\\n\\n2. **Generation Component**: \\n   - The generation phase utilizes the content retrieved in response to user queries to produce coherent and contextually relevant replies. It employs techniques like prompting to enhance the quality of these generated responses. During this phase, the model interprets the provided input and integrates the extracted information without needing additional fine-tuning, thereby generating accurate responses that align with the user's intent.\\n\\nOverall, RAG models blend the strengths of retrieval-based and sequence-to-sequence generation, enabling them to access and manipulate knowledge more effectively, which is particularly beneficial for knowledge-intensive tasks in natural language processing.\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wUhOlwWcMEUT",
    "outputId": "11b58279-07bb-4850-b260-05661d88e059"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node ID\t 1c324686-fad7-4a41-bfd3-d44f9612ca91\n",
      "Title\t Evaluation of Retrieval-Augmented Generation: A Survey:Research Paper Information\n",
      "Text\t Authors: Hao Yu, Aoran Gan, Kai Zhang, Shiwei Tong, Qi Liu, and Zhaofeng Liu.Numerous studies of Retrieval-Augmented Generation (RAG) systems have emerged from various perspectives since the advent of Large Language Models (LLMs). The RAG system comprises two primary components: Retrieval and Generation. The retrieval component aims to extract relevant information from various external knowledge sources. It involves two main phases, indexing and searching. Indexing organizes documents to facilitate efficient retrieval, using either inverted indexes for sparse retrieval or dense vector encoding for dense retrieval. The searching component utilizes these indexes to fetch relevant documents based on the user's query, often incorporating the optional rerankers to refine the ranking of the retrieved documents. The generation component utilizes the retrieved content and question query to formulate coherent and contextually relevant responses with the prompting and inferencing phases. As the \"Emerging\" ability of LLMs and the breakthrough in aligning human commands, LLMs are the best performance choices model for the generation stage. Prompting methods like Chain of Thought, Tree of Thought, Rephrase and Respond guide better generation results. In the inferencing step, LLMs interpret the prompted input to generate accurate and in-depth responses that align with the query's intent and integrate the extracted information without further finetuning, such as fully finetuning or LoRA. Appendix A details the complete RAG structure. Figure 1 illustrates the structure of the RAG systems as mentioned.\n",
      "Score\t 0.5764609663328313\n",
      "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
      "Node ID\t 2aa05360-f43a-4819-bce7-0acf7b897eab\n",
      "Title\t Searching for Best Practices in Retrieval-Augmented Generation:1 Introduction\n",
      "Text\t Generative large language models are prone to producing outdated information or fabricating facts, although they were aligned with human preferences by reinforcement learning [1] or lightweight alternatives [2–5]. Retrieval-augmented generation (RAG) techniques address these issues by combining the strengths of pretraining and retrieval-based models, thereby providing a robust framework for enhancing model performance [6]. Furthermore, RAG enables rapid deployment of applications for specific organizations and domains without necessitating updates to the model parameters, as long as query-related documents are provided. Many RAG approaches have been proposed to enhance large language models (LLMs) through query-dependent retrievals [6–8]. A typical RAG workflow usually contains multiple intervening processing steps: query classification (determining whether retrieval is necessary for a given input query), retrieval (efficiently obtaining relevant documents for the query), reranking (refining the order of retrieved documents based on their relevance to the query), repacking (organizing the retrieved documents into a structured one for better generation), summarization (extracting key information for response generation from the repacked document and eliminating redundancies) and models. Implementing RAG also requires decisions on the ways to properly split documents into chunks, the types of embeddings to use for semantically representing these chunks, the choice of\n",
      "Score\t 0.5920924119308713\n",
      "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
      "Node ID\t e791eaf2-3cdf-477a-8602-1cbb30a3d48c\n",
      "Title\t RAG\n",
      "Text\t # RAG<div class=\"flex flex-wrap space-x-1\"><a href=\"https://huggingface.co/models?filter=rag\"><img alt=\"Models\" src=\"https://img.shields.io/badge/All_model_pages-rag-blueviolet\"></a></div>## OverviewRetrieval-augmented generation (\"RAG\") models combine the powers of pretrained dense retrieval (DPR) andsequence-to-sequence models. RAG models retrieve documents, pass them to a seq2seq model, then marginalize to generateoutputs. The retriever and seq2seq modules are initialized from pretrained models, and fine-tuned jointly, allowingboth retrieval and generation to adapt to downstream tasks.It is based on the paper [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401) by Patrick Lewis, Ethan Perez, Aleksandara Piktus, Fabio Petroni, VladimirKarpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, Douwe Kiela.The abstract from the paper is the following:*Large pre-trained language models have been shown to store factual knowledge in their parameters, and achievestate-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and preciselymanipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behindtask-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledgeremain open research problems. Pre-trained models with a differentiable access mechanism to explicit nonparametricmemory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore ageneral-purpose fine-tuning recipe for retrieval-augmented generation (RAG) — models which combine pre-trainedparametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is apre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with apre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passagesacross the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate ourmodels on a wide range of knowledge-intensive\n",
      "Score\t 0.5609488598979723\n",
      "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n"
     ]
    }
   ],
   "source": [
    "# Show the retrieved nodes\n",
    "for src in res.source_nodes:\n",
    "    print(\"Node ID\\t\", src.node_id)\n",
    "    print(\"Title\\t\", src.metadata[\"title\"])\n",
    "    print(\"Text\\t\", src.text.strip())\n",
    "    print(\"Score\\t\", src.score)\n",
    "    print(\"-_\" * 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5mcAcZqhQluE"
   },
   "source": [
    "# Custom Postprocessor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7v7vmJblQrN6"
   },
   "source": [
    "## The `Judger` Function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6k8IKlN9QvU7"
   },
   "source": [
    "The following function will query GPT-4o to retrieve the top three nodes that has highest similarity to the asked question.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "WhtJ1OeF9L3G"
   },
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "def judger(nodes, query):\n",
    "\n",
    "    # The model's output template\n",
    "    class OrderedNodes(BaseModel):\n",
    "        \"\"\"A node with the id and assigned score.\"\"\"\n",
    "\n",
    "        node_id: list\n",
    "        score: list\n",
    "\n",
    "    # Prepare the nodes and wrap them in <NODE></NODE> identifier, as well as the query\n",
    "    the_nodes = \"\"\n",
    "    for idx, item in enumerate(nodes):\n",
    "        the_nodes += f\"<NODE{idx+1}>\\nNode ID: {item.node_id}\\nText: {item.text}\\n</NODE{idx+1}>\\n\"\n",
    "\n",
    "    query = \"<QUERY>\\n{}\\n</QUERY>\".format(query)\n",
    "\n",
    "    # Define the prompt template\n",
    "    prompt_tmpl = PromptTemplate(\n",
    "        \"\"\"\n",
    "    You receive a qurey along with a list of nodes' text and their ids. Your task is to assign score\n",
    "    to each node based on its contextually closeness to the given query. The final output is each\n",
    "    node id along with its proximity score.\n",
    "    Here is the list of nodes:\n",
    "    {nodes_list}\n",
    "\n",
    "    And the following is the query:\n",
    "    {user_query}\n",
    "\n",
    "    Score each of the nodes based on their text and their relevancy to the provided query.\n",
    "    The score must be a decimal number between 0 an 1 so we can rank them.\"\"\"\n",
    "    )\n",
    "\n",
    "    # Define the an instance of GPT-4 and send the request\n",
    "    llm = OpenAI(model=\"gpt-4o-mini\")\n",
    "    ordered_nodes = llm.structured_predict(\n",
    "        OrderedNodes, prompt_tmpl, nodes_list=the_nodes, user_query=query\n",
    "    )\n",
    "\n",
    "    return ordered_nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q5f1GrBKZprO"
   },
   "source": [
    "## Define Postprocessor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yZujUJTvQ6Yu"
   },
   "source": [
    "The following class will use the `judger` function to rank the nodes, and filter them based on the ranks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "-QtyuC8fZun0"
   },
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "from llama_index.core import QueryBundle\n",
    "from llama_index.core.postprocessor.types import BaseNodePostprocessor\n",
    "from llama_index.core.schema import NodeWithScore\n",
    "\n",
    "\n",
    "class OpenaiAsJudgePostprocessor(BaseNodePostprocessor):\n",
    "    def _postprocess_nodes(\n",
    "        self, nodes: List[NodeWithScore], query_bundle: Optional[QueryBundle]\n",
    "    ) -> List[NodeWithScore]:\n",
    "\n",
    "        r = judger(nodes, query_bundle)\n",
    "\n",
    "        node_ids = r.node_id\n",
    "        scores = r.score\n",
    "\n",
    "        sorted_scores = sorted(enumerate(scores), key=lambda x: x[1], reverse=True)\n",
    "        top_three_nodes = [sorted_scores[i][0] for i in range(3)]\n",
    "\n",
    "        selected_nodes_id = [node_ids[item] for item in top_three_nodes]\n",
    "\n",
    "        final_nodes = []\n",
    "        for item in nodes:\n",
    "            if item.node_id in selected_nodes_id:\n",
    "                final_nodes.append(item)\n",
    "\n",
    "        return final_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "jk-lqYlYLipi"
   },
   "outputs": [],
   "source": [
    "judge = OpenaiAsJudgePostprocessor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cgtsvxR7SflP"
   },
   "source": [
    "## Query Engine with Postprocessor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "1Hh3RLCeLfXZ"
   },
   "outputs": [],
   "source": [
    "# Define a query engine that is responsible for retrieving related pieces of text,\n",
    "# and using a LLM to formulate the final answer.\n",
    "# The `node_postprocessors` function will be applied to the retrieved nodes.\n",
    "query_engine = index.as_query_engine(similarity_top_k=5, node_postprocessors=[judge])\n",
    "\n",
    "res = query_engine.query(\"Explain how Parameter Efficient Fine tuning (PEFT) works?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "id": "zmZv0EIyF0wG",
    "outputId": "241d768f-933c-469e-b1e8-544d4753f129"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Parameter Efficient Fine-Tuning (PEFT) is a method designed to adapt large pretrained models efficiently for various downstream applications without the need to fine-tune all model parameters. Instead of updating all parameters, PEFT focuses on fine-tuning a small subset of additional parameters. This significantly reduces both computational and storage costs while still achieving performance that is comparable to traditional full fine-tuning methods. \\n\\nPEFT integrates with popular libraries, allowing for an optimized process for loading, training, and using large models for inference. This integration facilitates faster performance and makes it more feasible for users to work with large language models on consumer hardware. Additionally, PEFT methods provide user support resources, such as quick start guides and how-to guides, to assist users in applying these techniques across various tasks, enhancing accessibility and efficiency in model training and application.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bBMaG6yaZzjA",
    "outputId": "63665f8f-b873-49fb-de30-6b138ed46dd6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node ID\t 6be88fa3-2f8b-43e7-aba0-d874b39809fc\n",
      "Title\t FourierFT: Discrete Fourier Transformation Fine-Tuning\n",
      "Text\t # FourierFT: Discrete Fourier Transformation Fine-Tuning[FourierFT](https://huggingface.co/papers/2405.03003) is a parameter-efficient fine-tuning technique that leverages Discrete Fourier Transform to compress the model's tunable weights. This method outperforms LoRA in the GLUE benchmark and common ViT classification tasks using much less parameters.FourierFT currently has the following constraints:- Only `nn.Linear` layers are supported.- Quantized layers are not supported.If these constraints don't work for your use case, consider other methods instead.The abstract from the paper is:> Low-rank adaptation (LoRA) has recently gained much interest in fine-tuning foundation models. It effectively reduces the number of trainable parameters by incorporating low-rank matrices A and B to represent the weight change, i.e., Delta W=BA. Despite LoRA's progress, it faces storage challenges when handling extensive customization adaptations or larger base models. In this work, we aim to further compress trainable parameters by enjoying the powerful expressiveness of the Fourier transform. Specifically, we introduce FourierFT, which treats Delta W as a matrix in the spatial domain and learns only a small fraction of its spectral coefficients. With the trained spectral coefficients, we implement the inverse discrete Fourier transform to recover Delta W. Empirically, our FourierFT method shows comparable or better performance with fewer parameters than LoRA on various tasks, including natural language understanding, natural language generation, instruction tuning, and image classification. For example, when performing instruction tuning on the LLaMA2-7B model, FourierFT surpasses LoRA with only 0.064M trainable parameters, compared to LoRA's 33.5M.## FourierFTConfig[[autodoc]] tuners.fourierft.config.FourierFTConfig## FourierFTModel[[autodoc]] tuners.fourierft.model.FourierFTModel\n",
      "Score\t 0.4624181214875688\n",
      "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
      "Node ID\t ffd813ca-d987-47fa-988d-2cddaa749553\n",
      "Title\t PEFT\n",
      "Text\t # PEFT🤗 PEFT (Parameter-Efficient Fine-Tuning) is a library for efficiently adapting large pretrained models to various downstream applications without fine-tuning all of a model's parameters because it is prohibitively costly. PEFT methods only fine-tune a small number of (extra) model parameters - significantly decreasing computational and storage costs - while yielding performance comparable to a fully fine-tuned model. This makes it more accessible to train and store large language models (LLMs) on consumer hardware.PEFT is integrated with the Transformers, Diffusers, and Accelerate libraries to provide a faster and easier way to load, train, and use large models for inference.<div class=\"mt-10\">  <div class=\"w-full flex flex-col space-y-4 md:space-y-0 md:grid md:grid-cols-2 md:gap-y-4 md:gap-x-5\">    <a class=\"!no-underline border dark:border-gray-700 p-5 rounded-lg shadow hover:shadow-lg\" href=\"quicktour\"      ><div class=\"w-full text-center bg-gradient-to-br from-blue-400 to-blue-500 rounded-lg py-1.5 font-semibold mb-5 text-white text-lg leading-relaxed\">Get started</div>      <p class=\"text-gray-700\">Start here if you're new to 🤗 PEFT to get an overview of the library's main features, and how to train a model with a PEFT method.</p>    </a>    <a class=\"!no-underline border dark:border-gray-700 p-5 rounded-lg shadow hover:shadow-lg\" href=\"./task_guides/image_classification_lora\"      ><div class=\"w-full text-center bg-gradient-to-br from-indigo-400 to-indigo-500 rounded-lg py-1.5 font-semibold mb-5 text-white text-lg leading-relaxed\">How-to guides</div>      <p class=\"text-gray-700\">Practical guides demonstrating how to apply various PEFT methods across different types of tasks like image classification, causal language modeling, automatic speech recognition, and more. Learn how to use 🤗 PEFT with the DeepSpeed and Fully Sharded Data Parallel scripts.</p>    </a>    <a\n",
      "Score\t 0.44532753288814986\n",
      "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
      "Node ID\t 22fe2e92-ea31-4b34-9b8c-9d6f5fa66e87\n",
      "Title\t PEFT configurations and models\n",
      "Text\t p-tuning to  \"encoder_dropout\": 0.0,  \"encoder_hidden_size\": 128,  \"encoder_num_layers\": 2,  \"encoder_reparameterization_type\": \"MLP\",  \"inference_mode\": true,  \"num_attention_heads\": 16,  \"num_layers\": 24,  \"num_transformer_submodules\": 1,  \"num_virtual_tokens\": 20,  \"peft_type\": \"P_TUNING\", #PEFT method type  \"task_type\": \"SEQ_CLS\", #type of task to train model on  \"token_dim\": 1024}```You can create your own configuration for training by initializing a [`PromptEncoderConfig`].```pyfrom peft import PromptEncoderConfig, TaskTypep_tuning_config = PromptEncoderConfig(    encoder_reparameterization_type=\"MLP\",    encoder_hidden_size=128,    num_attention_heads=16,    num_layers=24,    num_transformer_submodules=1,    num_virtual_tokens=20,    token_dim=1024,    task_type=TaskType.SEQ_CLS)```</hfoption></hfoptions>## PEFT modelsWith a PEFT configuration in hand, you can now apply it to any pretrained model to create a [`PeftModel`]. Choose from any of the state-of-the-art models from the [Transformers](https://hf.co/docs/transformers) library, a custom model, and even new and unsupported transformer architectures.For this tutorial, load a base [facebook/opt-350m](https://huggingface.co/facebook/opt-350m) model to finetune.```pyfrom transformers import AutoModelForCausalLMmodel = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\")```Use the [`get_peft_model`] function to create a [`PeftModel`] from the base facebook/opt-350m model and the `lora_config` you created earlier.```pyfrom peft import get_peft_modellora_model = get_peft_model(model, lora_config)lora_model.print_trainable_parameters()\"trainable params: 1,572,864 || all params: 332,769,280 || trainable%:\n",
      "Score\t 0.43584612073274953\n",
      "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n"
     ]
    }
   ],
   "source": [
    "# Show the retrieved nodes\n",
    "for src in res.source_nodes:\n",
    "    print(\"Node ID\\t\", src.node_id)\n",
    "    print(\"Title\\t\", src.metadata[\"title\"])\n",
    "    print(\"Text\\t\", src.text)\n",
    "    print(\"Score\\t\", src.score)\n",
    "    print(\"-_\" * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "J7sIPpFFTep3"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
