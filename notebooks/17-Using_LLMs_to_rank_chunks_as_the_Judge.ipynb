{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cypMRycdIDrV"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/towardsai/ai-tutor-rag-system/blob/main/notebooks/17-Using_LLMs_to_rank_chunks_as_the_Judge.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0FbELaf7TrW7"
   },
   "source": [
    "## Install Packages and Setup Variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "Yubz8AanRRSW",
    "outputId": "548605a3-edce-4b45-d22a-13c691eded21"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.8/19.8 MB\u001b[0m \u001b[31m76.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m62.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m70.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m84.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m303.3/303.3 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.3/103.3 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m91.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.9/131.9 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.7/65.7 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m208.0/208.0 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.8/510.8 kB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.5/42.5 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.9/63.9 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.5/310.5 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m62.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m452.2/452.2 kB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.4/144.4 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "!pip install -q huggingface-hub==0.34.4 llama-index==0.13.3 llama-index-llms-openai==0.5.4 llama-index-vector-stores-chroma==0.5.2 \\\n",
    "                llama-index-llms-google-genai==0.3.0 chromadb==1.0.20 jedi==0.19.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xLXFuRW-TpUu"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set the following API Keys in the Python environment. Will be used later.\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"[OPENAI_API_KEY]\"\n",
    "# os.environ[\"GOOGLE_API_KEY\"] = \"<YOUR_API_KEY>\"\n",
    "\n",
    "from google.colab import userdata\n",
    "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
    "os.environ[\"GOOGLE_API_KEY\"] = userdata.get('Google_api_key')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r6GCYYqqTuMc"
   },
   "source": [
    "# Load a Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pupJpdZaTu5m"
   },
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core import Settings\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "Settings.llm = OpenAI(model=\"gpt-5-mini\", additional_kwargs={'reasoning_effort':'minimal'})\n",
    "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YSGHsZMMZj4E"
   },
   "source": [
    "# Load Indexes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z1lRMQtdOQ95"
   },
   "source": [
    "Downloading vector store from Huggingface hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "03fcb8afebbe4d679ddfa031a5584889",
      "a8482af87a12425ea1af8535a52b54f2",
      "d3b23b00ee25422a973097d29cf89b91",
      "a2b5c652832442d9bbfa07cf2c831279",
      "1273a264a9c048268e113492e3d507ed",
      "037b1c74841c4876b01f3374620419f9",
      "450a0c3bf1d946aeaa6d2b6725823907",
      "56f5851ef5564214b56fd1fed1c813c1",
      "46b8365a3f3a4da6bce37f4eb9dd983f",
      "f5da4875413349ca8b1a345c7d436992",
      "bdee156fe61f48eaa730cf26ed0a1aab"
     ]
    },
    "id": "o8Riolm_2iYd",
    "outputId": "0f17d2cc-5cd9-4599-80d9-dfeeccb37eb1"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03fcb8afebbe4d679ddfa031a5584889",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vectorstore.zip:   0%|          | 0.00/97.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "vectorstore = hf_hub_download(repo_id=\"jaiganesan/ai_tutor_knowledge\", filename=\"vectorstore.zip\",repo_type=\"dataset\",local_dir=\"/content\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M8iaOOGyZeNp",
    "outputId": "cf641396-fd5d-419b-88e9-850383f216c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  /content/vectorstore.zip\n",
      "   creating: ai_tutor_knowledge/\n",
      "   creating: ai_tutor_knowledge/684af133-f877-4230-bde4-575cf53b6688/\n",
      "  inflating: ai_tutor_knowledge/684af133-f877-4230-bde4-575cf53b6688/length.bin  \n",
      "  inflating: ai_tutor_knowledge/684af133-f877-4230-bde4-575cf53b6688/index_metadata.pickle  \n",
      "  inflating: ai_tutor_knowledge/684af133-f877-4230-bde4-575cf53b6688/link_lists.bin  \n",
      "  inflating: ai_tutor_knowledge/684af133-f877-4230-bde4-575cf53b6688/header.bin  \n",
      "  inflating: ai_tutor_knowledge/684af133-f877-4230-bde4-575cf53b6688/data_level0.bin  \n",
      "  inflating: ai_tutor_knowledge/chroma.sqlite3  \n"
     ]
    }
   ],
   "source": [
    "!unzip /content/vectorstore.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6tzS_EKPZeLS"
   },
   "outputs": [],
   "source": [
    "# Load the vector store from the local storage.\n",
    "import chromadb\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "\n",
    "db2 = chromadb.PersistentClient(path=\"/content/ai_tutor_knowledge\")\n",
    "chroma_collection = db2.get_or_create_collection(\"ai_tutor_knowledge\")\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0T6FL7J3ZrNK"
   },
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "# Create the index based on the vector store.\n",
    "index = VectorStoreIndex.from_vector_store(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b8OilXIR8IO9"
   },
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine(similarity_top_k=10)\n",
    "\n",
    "res = query_engine.query(\"Explain how Advance RAG works?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 184
    },
    "id": "8RMKJJv98R2o",
    "outputId": "dc601809-0b30-4d6f-d482-e4851045d3dd"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'The material doesn’t mention anything called “Advance RAG.” Based on the provided information, there are several closely related RAG variants described (Adaptive RAG, Corrective RAG, Self‑RAG / Self‑Reflective RAG, Speculative RAG). If you meant one of those, here are concise explanations so you can pick the intended one:\\n\\n- Adaptive RAG: Routes a question to different retrieval strategies depending on the question type or need (i.e., select the most appropriate retriever or indexing approach), improving the chance of retrieving relevant documents.\\n\\n- Corrective RAG (CRAG): Acts as a fallback / retrieval‑quality filter. An external evaluator checks retrieved documents and, when they are low‑relevance, triggers fallback retrieval (for example, a web search) to get better evidence before generation.\\n\\n- Self‑Reflective RAG (Self‑RAG): Instruction‑tunes a model to generate self‑reflection tags that guide dynamic retrieval and let the model critique document relevance during generation; it performs iterative self‑checks to detect hallucinations or irrelevant content and refines answers.\\n\\n- Speculative RAG: Splits the task into drafting and verification. A smaller specialized RAG drafter generates multiple diverse drafts (each from a subset of retrieved docs) in parallel; a larger generalist LM then verifies and integrates those drafts. This reduces input length per draft, mitigates position bias, and can improve both accuracy and latency.\\n\\nIf you meant “Adaptive RAG” (sometimes called “Advance RAG” informally), the first description above is the match. Tell me which one you meant or paste the exact phrase/line and I’ll explain that variant in more detail.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w2XBkzNwLle5"
   },
   "source": [
    "# RankGPT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_it2CxTtLmHT"
   },
   "outputs": [],
   "source": [
    "from llama_index.core.postprocessor.rankGPT_rerank import RankGPTRerank\n",
    "\n",
    "rankGPT = RankGPTRerank(top_n=3,llm=Settings.llm, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YA3M9m9CL6AJ",
    "outputId": "9904d925-8750-4df6-c593-5e5c81adc3c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Reranking, new rank list for nodes: [1, 3, 0, 4, 6, 8, 9, 7, 2, 5]"
     ]
    }
   ],
   "source": [
    "# Define a query engine that is responsible for retrieving related pieces of text,\n",
    "# and using a LLM to formulate the final answer.\n",
    "# The `node_postprocessors` function will be applied to the retrieved nodes.\n",
    "query_engine = index.as_query_engine(similarity_top_k=10, node_postprocessors=[rankGPT])\n",
    "\n",
    "res = query_engine.query(\"Explain how Retrieval Augmented Generation (RAG) works?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 184
    },
    "id": "wgyjv9e6MCVm",
    "outputId": "239dfb42-2bc5-4bf7-d4fc-0d97366d423d"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Retrieval-Augmented Generation (RAG) is a two-part approach that combines external information retrieval with language generation to produce more accurate, up-to-date, and grounded responses. Here’s how it works, step by step:\\n\\n1. Two main components\\n- Retrieval: fetches relevant documents or passages from external knowledge sources (non‑parametric memory).\\n- Generation: uses a pretrained large language model (parametric memory) to produce the final response conditioned on the query and retrieved content.\\n\\n2. Retrieval component details\\n- Indexing: source documents are organized for efficient lookup. This can use sparse inverted indexes or dense vector encodings (embeddings) stored in a vector index.\\n- Searching: the user query is used to search the index to obtain candidate documents or chunks. Neural retrievers (dense) or traditional retrieval (sparse) can be used.\\n- Optional reranking: retrieved candidates can be reranked by a specialized model to improve the ordering of most relevant passages.\\n\\n3. Typical RAG workflow (additional processing steps often applied)\\n- Query classification: determine whether a query needs retrieval at all.\\n- Retrieval: fetch candidate passages.\\n- Reranking: refine which passages are most relevant.\\n- Repacking: organize or combine retrieved content into a structured context for the generator.\\n- Summarization: condense or extract salient information from the repacked documents to reduce redundancy and fit context limits.\\n- (Then) Generation: feed the query plus the prepared retrieved content into the generation model.\\n\\n4. Generation component details\\n- The generation model is usually a large pretrained seq2seq or LLM. It conditions on the retrieved content to produce coherent, contextually grounded responses.\\n- Prompting techniques (e.g., Chain of Thought, Tree of Thought, rephrase-and-respond) can be applied to guide the model’s reasoning and produce higher-quality, more accurate outputs.\\n- Inference: the model interprets the prompted input and integrates the retrieved facts to generate the final answer without necessarily requiring further full-model fine-tuning.\\n\\n5. Benefits and design choices\\n- Combines parametric knowledge (what the model stores) with non‑parametric, updatable external knowledge, improving accuracy on knowledge-intensive tasks and providing provenance.\\n- Choices such as chunking strategy, embedding type, retriever architecture, whether to fine-tune retriever and generator jointly, and how to post-process retrieved content affect performance, latency, and deployment complexity.\\n\\nIn short, RAG augments a generative model by first retrieving relevant external information and then conditioning generation on that information, often with intermediate steps (reranking, repacking, summarization) and reasoning prompts to improve fidelity and relevance.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wUhOlwWcMEUT",
    "outputId": "df1f0fda-54f8-41d0-f295-adf44f2a0893"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node ID\t 1c324686-fad7-4a41-bfd3-d44f9612ca91\n",
      "Title\t Evaluation of Retrieval-Augmented Generation: A Survey:Research Paper Information\n",
      "Text\t Authors: Hao Yu, Aoran Gan, Kai Zhang, Shiwei Tong, Qi Liu, and Zhaofeng Liu.Numerous studies of Retrieval-Augmented Generation (RAG) systems have emerged from various perspectives since the advent of Large Language Models (LLMs). The RAG system comprises two primary components: Retrieval and Generation. The retrieval component aims to extract relevant information from various external knowledge sources. It involves two main phases, indexing and searching. Indexing organizes documents to facilitate efficient retrieval, using either inverted indexes for sparse retrieval or dense vector encoding for dense retrieval. The searching component utilizes these indexes to fetch relevant documents based on the user's query, often incorporating the optional rerankers to refine the ranking of the retrieved documents. The generation component utilizes the retrieved content and question query to formulate coherent and contextually relevant responses with the prompting and inferencing phases. As the \"Emerging\" ability of LLMs and the breakthrough in aligning human commands, LLMs are the best performance choices model for the generation stage. Prompting methods like Chain of Thought, Tree of Thought, Rephrase and Respond guide better generation results. In the inferencing step, LLMs interpret the prompted input to generate accurate and in-depth responses that align with the query's intent and integrate the extracted information without further finetuning, such as fully finetuning or LoRA. Appendix A details the complete RAG structure. Figure 1 illustrates the structure of the RAG systems as mentioned.\n",
      "Score\t 0.5764409349459677\n",
      "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
      "Node ID\t e791eaf2-3cdf-477a-8602-1cbb30a3d48c\n",
      "Title\t RAG\n",
      "Text\t # RAG<div class=\"flex flex-wrap space-x-1\"><a href=\"https://huggingface.co/models?filter=rag\"><img alt=\"Models\" src=\"https://img.shields.io/badge/All_model_pages-rag-blueviolet\"></a></div>## OverviewRetrieval-augmented generation (\"RAG\") models combine the powers of pretrained dense retrieval (DPR) andsequence-to-sequence models. RAG models retrieve documents, pass them to a seq2seq model, then marginalize to generateoutputs. The retriever and seq2seq modules are initialized from pretrained models, and fine-tuned jointly, allowingboth retrieval and generation to adapt to downstream tasks.It is based on the paper [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401) by Patrick Lewis, Ethan Perez, Aleksandara Piktus, Fabio Petroni, VladimirKarpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, Douwe Kiela.The abstract from the paper is the following:*Large pre-trained language models have been shown to store factual knowledge in their parameters, and achievestate-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and preciselymanipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behindtask-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledgeremain open research problems. Pre-trained models with a differentiable access mechanism to explicit nonparametricmemory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore ageneral-purpose fine-tuning recipe for retrieval-augmented generation (RAG) — models which combine pre-trainedparametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is apre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with apre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passagesacross the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate ourmodels on a wide range of knowledge-intensive\n",
      "Score\t 0.5609655108535478\n",
      "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
      "Node ID\t 2aa05360-f43a-4819-bce7-0acf7b897eab\n",
      "Title\t Searching for Best Practices in Retrieval-Augmented Generation:1 Introduction\n",
      "Text\t Generative large language models are prone to producing outdated information or fabricating facts, although they were aligned with human preferences by reinforcement learning [1] or lightweight alternatives [2–5]. Retrieval-augmented generation (RAG) techniques address these issues by combining the strengths of pretraining and retrieval-based models, thereby providing a robust framework for enhancing model performance [6]. Furthermore, RAG enables rapid deployment of applications for specific organizations and domains without necessitating updates to the model parameters, as long as query-related documents are provided. Many RAG approaches have been proposed to enhance large language models (LLMs) through query-dependent retrievals [6–8]. A typical RAG workflow usually contains multiple intervening processing steps: query classification (determining whether retrieval is necessary for a given input query), retrieval (efficiently obtaining relevant documents for the query), reranking (refining the order of retrieved documents based on their relevance to the query), repacking (organizing the retrieved documents into a structured one for better generation), summarization (extracting key information for response generation from the repacked document and eliminating redundancies) and models. Implementing RAG also requires decisions on the ways to properly split documents into chunks, the types of embeddings to use for semantically representing these chunks, the choice of\n",
      "Score\t 0.592082248078237\n",
      "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n"
     ]
    }
   ],
   "source": [
    "# Show the retrieved nodes\n",
    "for src in res.source_nodes:\n",
    "    print(\"Node ID\\t\", src.node_id)\n",
    "    print(\"Title\\t\", src.metadata[\"title\"])\n",
    "    print(\"Text\\t\", src.text.strip())\n",
    "    print(\"Score\\t\", src.score)\n",
    "    print(\"-_\" * 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5mcAcZqhQluE"
   },
   "source": [
    "# Custom Postprocessor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7v7vmJblQrN6"
   },
   "source": [
    "## The `Judger` Function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6k8IKlN9QvU7"
   },
   "source": [
    "The following function will query GPT-5 (mini) to retrieve the top three nodes that has highest similarity to the asked question.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WhtJ1OeF9L3G"
   },
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "def judger(nodes, query):\n",
    "\n",
    "    # The model's output template\n",
    "    class OrderedNodes(BaseModel):\n",
    "        \"\"\"A node with the id and assigned score.\"\"\"\n",
    "\n",
    "        node_id: list\n",
    "        score: list\n",
    "\n",
    "    # Prepare the nodes and wrap them in <NODE></NODE> identifier, as well as the query\n",
    "    the_nodes = \"\"\n",
    "    for idx, item in enumerate(nodes):\n",
    "        the_nodes += f\"<NODE{idx+1}>\\nNode ID: {item.node_id}\\nText: {item.text}\\n</NODE{idx+1}>\\n\"\n",
    "\n",
    "    query = \"<QUERY>\\n{}\\n</QUERY>\".format(query)\n",
    "\n",
    "    # Define the prompt template\n",
    "    prompt_tmpl = PromptTemplate(\n",
    "        \"\"\"\n",
    "    You receive a qurey along with a list of nodes' text and their ids. Your task is to assign score\n",
    "    to each node based on its contextually closeness to the given query. The final output is each\n",
    "    node id along with its proximity score.\n",
    "    Here is the list of nodes:\n",
    "    {nodes_list}\n",
    "\n",
    "    And the following is the query:\n",
    "    {user_query}\n",
    "\n",
    "    Score each of the nodes based on their text and their relevancy to the provided query.\n",
    "    The score must be a decimal number between 0 an 1 so we can rank them.\"\"\"\n",
    "    )\n",
    "\n",
    "    # Define the an instance of GPT-5 mini and send the request\n",
    "    llm = OpenAI(model=\"gpt-5-mini\", additional_kwargs={'reasoning_effort':'minimal'})\n",
    "    ordered_nodes = llm.structured_predict(\n",
    "        OrderedNodes, prompt_tmpl, nodes_list=the_nodes, user_query=query\n",
    "    )\n",
    "\n",
    "    return ordered_nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q5f1GrBKZprO"
   },
   "source": [
    "## Define Postprocessor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yZujUJTvQ6Yu"
   },
   "source": [
    "The following class will use the `judger` function to rank the nodes, and filter them based on the ranks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-QtyuC8fZun0"
   },
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "from llama_index.core import QueryBundle\n",
    "from llama_index.core.postprocessor.types import BaseNodePostprocessor\n",
    "from llama_index.core.schema import NodeWithScore\n",
    "\n",
    "\n",
    "class OpenaiAsJudgePostprocessor(BaseNodePostprocessor):\n",
    "    def _postprocess_nodes(\n",
    "        self, nodes: List[NodeWithScore], query_bundle: Optional[QueryBundle]\n",
    "    ) -> List[NodeWithScore]:\n",
    "\n",
    "        r = judger(nodes, query_bundle)\n",
    "\n",
    "        node_ids = r.node_id\n",
    "        scores = r.score\n",
    "\n",
    "        sorted_scores = sorted(enumerate(scores), key=lambda x: x[1], reverse=True)\n",
    "        num_nodes_to_select = min(3, len(sorted_scores))\n",
    "        top_nodes = [sorted_scores[i][0] for i in range(num_nodes_to_select)]\n",
    "        # top_nodes = [sorted_scores[i][0] for i in range(3)]\n",
    "\n",
    "        selected_nodes_id = [node_ids[item] for item in top_nodes]\n",
    "\n",
    "        final_nodes = []\n",
    "        for item in nodes:\n",
    "            if item.node_id in selected_nodes_id:\n",
    "                final_nodes.append(item)\n",
    "\n",
    "        return final_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jk-lqYlYLipi"
   },
   "outputs": [],
   "source": [
    "judge = OpenaiAsJudgePostprocessor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cgtsvxR7SflP"
   },
   "source": [
    "## Query Engine with Postprocessor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Hh3RLCeLfXZ"
   },
   "outputs": [],
   "source": [
    "# Define a query engine that is responsible for retrieving related pieces of text,\n",
    "# and using a LLM to formulate the final answer.\n",
    "# The `node_postprocessors` function will be applied to the retrieved nodes.\n",
    "query_engine = index.as_query_engine(similarity_top_k=10, node_postprocessors=[judge])\n",
    "\n",
    "res = query_engine.query(\"Compare Retrieval Augmented Generation (RAG) and Parameter efficient Finetuning (PEFT)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 184
    },
    "id": "zmZv0EIyF0wG",
    "outputId": "cea51b1e-e021-42ee-c9e1-d12d731c4e63"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Retrieval-Augmented Generation (RAG) and Parameter-Efficient Fine-Tuning (PEFT) are different techniques addressing complementary problems in working with large language models. Comparing them along key dimensions:\\n\\nPurpose\\n- RAG: Augments generation by giving the model access to external, non-parametric knowledge (e.g., a dense vector index of Wikipedia). It improves factuality, specificity, and diversity for knowledge‑intensive tasks by retrieving relevant documents and conditioning generation on them.\\n- PEFT: Focuses on updating a pre-trained model for a downstream task while changing only a small subset of model parameters (or adding small adapters), to make fine-tuning cheaper and more parameter-efficient.\\n\\nHow they work\\n- RAG: Combines a neural retriever (dense vector index) with a seq2seq generator. Retrieved passages are passed to the seq2seq model and the output is marginalized over retrieved documents. Two formulations exist: one conditions on the same retrieved passages for the entire generated sequence; the other can use different passages per token.\\n- PEFT: Keeps the base model’s parameters largely frozen and fine-tunes a small number of additional parameters (e.g., adapters, LoRA, prompt-tuning) so the model adapts to a task with far fewer trainable parameters.\\n\\nWhere they act (memory & knowledge)\\n- RAG: Adds non‑parametric memory (external retrieval index) to complement the model’s parametric memory, enabling up-to-date or large-scale knowledge access and providing provenance for retrieved evidence.\\n- PEFT: Operates only within the model’s parametric memory, making training/updating more efficient but not changing how the model accesses external knowledge.\\n\\nTraining & adaptation\\n- RAG: Typically initializes retriever and seq2seq from pretrained models and fine-tunes them (often jointly) so retrieval and generation cooperate and adapt to downstream tasks.\\n- PEFT: Fine-tunes only small parameter subsets or added modules, enabling fast/low-cost adaptation without updating the full model.\\n\\nEffect on generation and tasks\\n- RAG: Especially beneficial for knowledge‑intensive NLP and open‑domain QA—produces more factual, specific, and diverse outputs than parametric-only seq2seq baselines, and can set state-of-the-art on open-domain QA by leveraging retrieved documents.\\n- PEFT: Enables efficient specialization of large models for a task (cost, storage, and compute benefits) but does not by itself provide external knowledge access or the retrieval-based factual grounding that RAG provides.\\n\\nComplementarity\\n- These methods are complementary: one can use PEFT techniques to efficiently fine-tune the generator (or retriever) within a RAG pipeline, combining efficient adaptation with external knowledge access.\\n\\nLimitations each addresses\\n- RAG addresses limitations of pre-trained models in accessing and precisely manipulating world knowledge and in providing provenance.\\n- PEFT addresses the cost and resource requirements of full-model fine-tuning.\\n\\nIn short: RAG augments model inputs with retrieved external documents to improve knowledge-grounded generation; PEFT reduces the number of trainable parameters to make fine-tuning large models efficient. They solve different problems and can be used together.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bBMaG6yaZzjA",
    "outputId": "3edce72d-ce85-46d8-96d2-70c027dee924"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node ID\t 40f472bb-265c-438a-bb45-25be7c023dc8\n",
      "Title\t RAG\n",
      "Text\t extractive downstream tasks. We explore ageneral-purpose fine-tuning recipe for retrieval-augmented generation (RAG) — models which combine pre-trainedparametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is apre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with apre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passagesacross the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate ourmodels on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks,outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generationtasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-artparametric-only seq2seq baseline.*This model was contributed by [ola13](https://huggingface.co/ola13).## Usage tipsRetrieval-augmented generation (\"RAG\") models combine the powers of pretrained dense retrieval (DPR) and Seq2Seq models. RAG models retrieve docs, pass them to a seq2seq model, then marginalize to generate outputs. The retriever and seq2seq modules are initialized from pretrained models, and fine-tuned jointly, allowing both retrieval and generation to adapt to downstream tasks.## RagConfig[[autodoc]] RagConfig## RagTokenizer[[autodoc]] RagTokenizer## Rag specific outputs[[autodoc]] models.rag.modeling_rag.RetrievAugLMMarginOutput[[autodoc]] models.rag.modeling_rag.RetrievAugLMOutput## RagRetriever[[autodoc]] RagRetriever<frameworkcontent><pt>## RagModel[[autodoc]] RagModel    - forward## RagSequenceForGeneration[[autodoc]] RagSequenceForGeneration    - forward    - generate## RagTokenForGeneration[[autodoc]] RagTokenForGeneration    - forward    - generate</pt><tf>## TFRagModel[[autodoc]] TFRagModel    - call## TFRagSequenceForGeneration[[autodoc]] TFRagSequenceForGeneration    - call\n",
      "Score\t 0.45722685660780626\n",
      "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
      "Node ID\t e791eaf2-3cdf-477a-8602-1cbb30a3d48c\n",
      "Title\t RAG\n",
      "Text\t # RAG<div class=\"flex flex-wrap space-x-1\"><a href=\"https://huggingface.co/models?filter=rag\"><img alt=\"Models\" src=\"https://img.shields.io/badge/All_model_pages-rag-blueviolet\"></a></div>## OverviewRetrieval-augmented generation (\"RAG\") models combine the powers of pretrained dense retrieval (DPR) andsequence-to-sequence models. RAG models retrieve documents, pass them to a seq2seq model, then marginalize to generateoutputs. The retriever and seq2seq modules are initialized from pretrained models, and fine-tuned jointly, allowingboth retrieval and generation to adapt to downstream tasks.It is based on the paper [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401) by Patrick Lewis, Ethan Perez, Aleksandara Piktus, Fabio Petroni, VladimirKarpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, Douwe Kiela.The abstract from the paper is the following:*Large pre-trained language models have been shown to store factual knowledge in their parameters, and achievestate-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and preciselymanipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behindtask-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledgeremain open research problems. Pre-trained models with a differentiable access mechanism to explicit nonparametricmemory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore ageneral-purpose fine-tuning recipe for retrieval-augmented generation (RAG) — models which combine pre-trainedparametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is apre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with apre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passagesacross the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate ourmodels on a wide range of knowledge-intensive\n",
      "Score\t 0.44530831574406854\n",
      "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
      "Node ID\t ff8ede2b-20d5-4650-8099-0f3ea4fc61d4\n",
      "Title\t Speculative RAG: Enhancing Retrieval Augmented Generation through Drafting:Baselines\n",
      "Text\t Standard RAG: For standard RAG, we incorporate all the retrieved documents into the prompt as contextual information. Refer to Appendix C for detailed prompts. We run standard RAG experiments off-the-shelf LLMs including Mistral78, Mistral-InstructB (Jiang et al., 2023a), MixtralB78, Mixtral-InstructsaB (Jiang et al., 2024), and AlpacaB78 (Dubois et al., 2024). We also include the performance of Toolformer (Schick et al., 2024) and SAIL (Luo et al., 2023a) which are originally reported from Asai et al. (2023). Toolformer78 is an LM instruction-tuned to use tools including a search engine, and SAIL78 is an LM instruction-tuned on the Alpaca instruction tuning set augmented with search results from information sources such as DuckDuckGo and Wikipedia. Self-Reflective RAG and Corrective RAG: Self-Reflective RAG (Self-RAG) (Asai et al., 2023) and Corrective RAG (CRAG) (Yan et al., 2024) are more advanced RAG systems that enhance the quality of contextual information in the retrieval results. CRAG introduces an external evaluator to assess the quality of retrieved documents and to refine them before the generation process. Self-RAG instruction-tunes an LM to generate special self-reflection tags. These tags guides the LM to dynamically retrieve documents when necessary, critique the retrieved documents relevance before generating responses. Self-CRAG is to apply the Self-RAG approach on the refined documents of CRAG. We adopt the same backbone LLMs across all methods as our proposed Speculative RAG.\n",
      "Score\t 0.4097040473345475\n",
      "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n"
     ]
    }
   ],
   "source": [
    "# Show the retrieved nodes\n",
    "for src in res.source_nodes:\n",
    "    print(\"Node ID\\t\", src.node_id)\n",
    "    print(\"Title\\t\", src.metadata[\"title\"])\n",
    "    print(\"Text\\t\", src.text)\n",
    "    print(\"Score\\t\", src.score)\n",
    "    print(\"-_\" * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J7sIPpFFTep3"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
