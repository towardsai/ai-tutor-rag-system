{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-zE1h0uQV7uT"
   },
   "source": [
    "# Install Packages and Setup Variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "tAWL0cErZ9by",
    "outputId": "b26391c8-9da9-4af8-9409-f6b15fe77b4f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/56.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.6/80.6 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m51.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.4/177.4 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m47.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m584.3/584.3 kB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m171.5/171.5 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m65.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.5/15.5 MB\u001b[0m \u001b[31m48.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m187.6/187.6 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.6/278.6 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.2/139.2 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m52.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m52.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.1/211.1 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m47.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.8/55.8 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.2/44.2 MB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.8/69.8 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m58.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m198.9/198.9 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m44.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.7/410.7 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.0/241.0 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m262.5/262.5 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.2/113.2 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.8/295.8 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.9/104.9 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m486.3/486.3 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m70.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m452.6/452.6 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.6/137.6 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.6/49.6 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for html2text (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for tinysegmenter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for spider-client (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for feedfinder2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for jieba3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.17.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.29.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q llama-index==0.10.57 openai==1.59.6 llama-index-finetuning==0.1.12 llama-index-embeddings-huggingface==0.2.3 llama-index-embeddings-cohere==0.1.9 llama-index-readers-web==0.1.23 cohere==5.6.2 tiktoken==0.7.0 chromadb==0.5.5 html2text==2024.2.26 sentence-transformers==2.7.0 pydantic==2.10.5 llama-index-vector-stores-chroma==0.1.10 llama-index-llms-azure-openai==0.1.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "riuXwpSPcvWC"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set the following API Keys in the Python environment. Will be used later.\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"<YOUR_OPENAI_KEY>\"\n",
    "\n",
    "# from google.colab import userdata\n",
    "# os.environ[\"OPENAI_API_KEY\"] = userdata.get('openai_api_key')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "jIEeZzqLbz0J"
   },
   "outputs": [],
   "source": [
    "# Allows running asyncio in environments with an existing event loop, like Jupyter notebooks.\n",
    "\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bkgi2OrYzF7q"
   },
   "source": [
    "# Load the Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "9oGT6crooSSj"
   },
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core import Settings\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "Settings.llm = OpenAI(temperature=0, model=\"gpt-4o-mini\")\n",
    "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OWaT6rL7ksp8"
   },
   "source": [
    "# Download knowledge base\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67,
     "referenced_widgets": [
      "1861ae27ad2d42a5872e06f67e2621b2",
      "8fd0495df849495c8c0b0531f976a054",
      "1afccc839a0347a9950102e5e2592630",
      "70c597a4d4bc487f82d1cee190b08646",
      "60732c48d9e24975a7db2645e9e9a072",
      "9c020ccaf9bd4850b3a5352a75f70894",
      "94b22af00f19478e87d80f18da6afdea",
      "dc2ff8e79f6f43e0b26e2fb326229ad7",
      "5d8b32aae2b7434cbbde9e2a9a228aee",
      "2793a8e924a348268d5866f5a2f79b9e",
      "f6dbc6276c634766857fe9fc36f612a5"
     ]
    },
    "id": "8TCIoLB89S3q",
    "outputId": "cf90bc53-5172-4aeb-8331-fc076467f04e"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1861ae27ad2d42a5872e06f67e2621b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vectorstore.zip:   0%|          | 0.00/97.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'vectorstore.zip'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "hf_hub_download(repo_id=\"jaiganesan/ai_tutor_knowledge\", filename=\"vectorstore.zip\", repo_type=\"dataset\", local_dir=\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SodY2Xpf_kxg",
    "outputId": "2d5ef637-20d0-47bf-e4b5-ff3faef6fdf0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  vectorstore.zip\n",
      "   creating: ai_tutor_knowledge/\n",
      "   creating: ai_tutor_knowledge/684af133-f877-4230-bde4-575cf53b6688/\n",
      "  inflating: ai_tutor_knowledge/684af133-f877-4230-bde4-575cf53b6688/length.bin  \n",
      "  inflating: ai_tutor_knowledge/684af133-f877-4230-bde4-575cf53b6688/index_metadata.pickle  \n",
      "  inflating: ai_tutor_knowledge/684af133-f877-4230-bde4-575cf53b6688/link_lists.bin  \n",
      "  inflating: ai_tutor_knowledge/684af133-f877-4230-bde4-575cf53b6688/header.bin  \n",
      "  inflating: ai_tutor_knowledge/684af133-f877-4230-bde4-575cf53b6688/data_level0.bin  \n",
      "  inflating: ai_tutor_knowledge/chroma.sqlite3  \n"
     ]
    }
   ],
   "source": [
    "!unzip -o vectorstore.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vWnDHyblUKRJ"
   },
   "source": [
    "# Create vector index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "mXi56KTXk2sp"
   },
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "# Load the vector store from the local storage.\n",
    "db = chromadb.PersistentClient(path=\"./ai_tutor_knowledge\")\n",
    "chroma_collection = db.get_collection(\"ai_tutor_knowledge\")\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "\n",
    "# Create the index based on the vector store.\n",
    "vector_index = VectorStoreIndex.from_vector_store(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XjIQGo11j5N-"
   },
   "source": [
    "# Create keyword index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Za6m06wpcJpN",
    "outputId": "31b76876-e680-4bd0-bbe4-30824cc7c571"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Number of requested results 100000000 is greater than number of elements in index 5834, updating n_results = 5834\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5834\n"
     ]
    }
   ],
   "source": [
    "def retrieve_all_nodes_from_vector_index(vector_index, query=\"Whatever\", similarity_top_k=100000000):\n",
    "    # Set similarity_top_k to a large number to retrieve all the nodes\n",
    "    vector_retriever = vector_index.as_retriever(similarity_top_k=similarity_top_k)\n",
    "\n",
    "    # Retrieve all nodes\n",
    "    all_nodes = vector_retriever.retrieve(query)\n",
    "    nodes = [item.node for item in all_nodes]\n",
    "\n",
    "    return nodes\n",
    "\n",
    "nodes = retrieve_all_nodes_from_vector_index(vector_index)\n",
    "print(len(nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "hcmwBAsCZIwR"
   },
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleKeywordTableIndex\n",
    "\n",
    "# Define the KeywordTable mIndex using all the nodes.\n",
    "keyword_index = SimpleKeywordTableIndex(nodes=nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K3wtAa7Lo2Vh"
   },
   "source": [
    "# Hybrid Retriever\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "txPFNOkUo2Kj"
   },
   "outputs": [],
   "source": [
    "from llama_index.core import QueryBundle\n",
    "from llama_index.core.schema import NodeWithScore\n",
    "from llama_index.core.retrievers import (\n",
    "    BaseRetriever,\n",
    "    VectorIndexRetriever,\n",
    "    KeywordTableSimpleRetriever,\n",
    ")\n",
    "from typing import List\n",
    "\n",
    "class HybridRetriever(BaseRetriever):\n",
    "    \"\"\"Hybrid retriever that performs both semantic search and keyword search.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vector_retriever: VectorIndexRetriever,\n",
    "        keyword_retriever: KeywordTableSimpleRetriever,\n",
    "        max_retrieve: int = 10,\n",
    "    ) -> None:\n",
    "        \"\"\"Init params.\"\"\"\n",
    "\n",
    "        self._vector_retriever = vector_retriever\n",
    "        self._keyword_retriever = keyword_retriever\n",
    "        self._max_retrieve = max_retrieve\n",
    "        super().__init__()\n",
    "\n",
    "    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n",
    "        \"\"\"Retrieve nodes given query.\"\"\"\n",
    "\n",
    "        vector_nodes = self._vector_retriever.retrieve(query_bundle)\n",
    "        keyword_nodes = self._keyword_retriever.retrieve(query_bundle)\n",
    "\n",
    "        resulting_nodes = []\n",
    "        node_ids_added = set()\n",
    "        for i in range(min(len(vector_nodes), len(keyword_nodes))):\n",
    "            vector_node = vector_nodes[i]\n",
    "            if vector_node.node.node_id not in node_ids_added:\n",
    "                resulting_nodes += [vector_node]\n",
    "                node_ids_added.add(vector_node.node.node_id)\n",
    "\n",
    "            keyword_node = keyword_nodes[i]\n",
    "            if keyword_node.node.node_id not in node_ids_added:\n",
    "                resulting_nodes += [keyword_node]\n",
    "                node_ids_added.add(keyword_node.node.node_id)\n",
    "\n",
    "        return resulting_nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8JPD8yAinVSq"
   },
   "source": [
    "# Test hybrid retriever vs vector retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b0gue7cyctt1",
    "outputId": "a7d3bb14-ea71-4337-e667-be3addf6c222"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KOSMOS-2 is a Transformer-based causal language model designed for multimodal tasks, trained on a large dataset of grounded image-text pairs known as GRIT. It enhances its capabilities by converting spatial coordinates from bounding boxes in the dataset into location tokens, which are linked to their corresponding text spans. This format allows the model to connect visual elements with textual descriptions effectively.\n",
      "\n",
      "The model can handle various tasks, including multimodal grounding (such as referring expression comprehension and phrase grounding), multimodal referring (referring expression generation), perception-language tasks, and general language understanding and generation. KOSMOS-2 aims to advance the development of Embodiment AI, contributing to the convergence of language, multimodal perception, action, and world modeling, which are essential for achieving artificial general intelligence.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import get_response_synthesizer\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "\n",
    "# Create hybrid query engine\n",
    "vector_retriever = VectorIndexRetriever(index=vector_index, similarity_top_k=6)\n",
    "keyword_retriever = KeywordTableSimpleRetriever(index=keyword_index, num_chunks_per_query=6)\n",
    "hybrid_retriever = HybridRetriever(vector_retriever, keyword_retriever, max_retrieve=6)\n",
    "response_synthesizer = get_response_synthesizer(llm=Settings.llm)\n",
    "hybrid_query_engine = RetrieverQueryEngine(\n",
    "    retriever=hybrid_retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    ")\n",
    "\n",
    "# Test the query engine\n",
    "answer = hybrid_query_engine.query(\"How does KOSMOS-2 work?\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MgMRS-bO_vbr",
    "outputId": "ce88b17c-dfb9-4cc4-a7a3-986814ebca9d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The provided context does not contain information about KOSMOS-2 or its workings. Therefore, I cannot provide an answer to that query.\n"
     ]
    }
   ],
   "source": [
    "# Create vector query engine\n",
    "vector_retriever = VectorIndexRetriever(index=vector_index, similarity_top_k=6)\n",
    "vector_query_engine = RetrieverQueryEngine(\n",
    "    retriever=vector_retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    ")\n",
    "\n",
    "# Test the query engine\n",
    "answer = vector_query_engine.query(\"How does KOSMOS-2 work?\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iMkpzH7vvb09"
   },
   "source": [
    "# Evaluate\n",
    "\n",
    "Run the following code if you want to generate an evaluation dataset from scratch. You can choose to download an evaluation dataset running the cell after this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "H8a3eKgKvckU"
   },
   "outputs": [],
   "source": [
    "# from llama_index.core.evaluation import generate_question_context_pairs\n",
    "\n",
    "# # Create questions for each segment. These questions will be used to\n",
    "# # assess whether the retriever can accurately identify and return the\n",
    "# # corresponding segment when queried.\n",
    "# rag_eval_dataset = generate_question_context_pairs(\n",
    "#     nodes, llm=Settings.llm, num_questions_per_chunk=1\n",
    "# )\n",
    "\n",
    "# # We can save the evaluation dataset as a json file for later use.\n",
    "# rag_eval_dataset.save_json(\"./rag_eval_dataset_question_context.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0O7cLF_TlnZV"
   },
   "source": [
    "You can download a version of the evaluation dataset with the following code cell, so that you don't have to create the eval dataset from scratch with the code above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "ae7a4e31d6e74701a5927a7ca534087e",
      "f1842bc73a474521b6fa12bdc770d1a1",
      "7d24d1dd7aff4c89bed8fad5510417ea",
      "fc89cd7329f94665bf1b59b545cff903",
      "27d57f9f1d23440cbaaad8f7c086f169",
      "eaf621132675421e85b2e8c1fbe92117",
      "44d48b3efeae4be08987a06f95aa8803",
      "683c251e56bd454585f3fab945fabf3d",
      "908fe92b49c6491896ab69ca23d8aa0e",
      "2b501055675540059cb7b1c7de237771",
      "382bdbdbb14740289057805ac2029b84"
     ]
    },
    "id": "3sA1K84U254o",
    "outputId": "b2626030-c294-4800-bfe7-8323c5d75ef4"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae7a4e31d6e74701a5927a7ca534087e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)_dataset_question_context_subset_50.json:   0%|          | 0.00/14.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "from llama_index.finetuning.embeddings.common import EmbeddingQAFinetuneDataset\n",
    "\n",
    "# Download the evaluation dataset\n",
    "hf_hub_download(repo_id=\"jaiganesan/ai_tutor_knowledge\", filename=\"rag_eval_dataset_question_context_subset_50.json\", repo_type=\"dataset\", local_dir=\".\")\n",
    "rag_eval_dataset = EmbeddingQAFinetuneDataset.from_json(\"./rag_eval_dataset_question_context_subset_50.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "H7ubvcbk27vr"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#  A simple function to show the evaluation result.\n",
    "def from_eval_results_to_dataframe(name, eval_results):\n",
    "    \"\"\"Convert evaluation results to a pandas dataframe.\"\"\"\n",
    "    metric_dicts = []\n",
    "    for eval_result in eval_results:\n",
    "        metric_dict = eval_result.metric_vals_dict\n",
    "        metric_dicts.append(metric_dict)\n",
    "\n",
    "    full_df = pd.DataFrame(metric_dicts)\n",
    "\n",
    "    hit_rate = full_df[\"hit_rate\"].mean()\n",
    "    mrr = full_df[\"mrr\"].mean()\n",
    "\n",
    "    metric_df = pd.DataFrame(\n",
    "        {\"Retriever Name\": [name], \"Hit Rate\": [hit_rate], \"MRR\": [mrr]}\n",
    "    )\n",
    "\n",
    "    return metric_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uNLxDxoc2-Ac",
    "outputId": "cd6f633f-8bbd-46d1-bdd1-404c13c99083"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Retriever Name  Hit Rate   MRR\n",
      "0  Hybrid retriever top_2      0.66  0.59\n",
      "           Retriever Name  Hit Rate   MRR\n",
      "0  Vector retriever top_2       0.6  0.57\n",
      "           Retriever Name  Hit Rate       MRR\n",
      "0  Hybrid retriever top_4      0.72  0.599833\n",
      "           Retriever Name  Hit Rate       MRR\n",
      "0  Vector retriever top_4      0.68  0.591667\n",
      "           Retriever Name  Hit Rate       MRR\n",
      "0  Hybrid retriever top_6      0.76  0.604333\n",
      "           Retriever Name  Hit Rate       MRR\n",
      "0  Vector retriever top_6      0.68  0.591667\n",
      "           Retriever Name  Hit Rate       MRR\n",
      "0  Hybrid retriever top_8       0.8  0.607205\n",
      "           Retriever Name  Hit Rate       MRR\n",
      "0  Vector retriever top_8      0.74  0.599881\n",
      "            Retriever Name  Hit Rate       MRR\n",
      "0  Hybrid retriever top_10      0.82  0.608634\n",
      "            Retriever Name  Hit Rate       MRR\n",
      "0  Vector retriever top_10      0.76  0.602103\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.evaluation import RetrieverEvaluator\n",
    "\n",
    "# We can evaluate the retievers with different top_k values.\n",
    "for i in [2, 4, 6, 8, 10]:\n",
    "    # Evaluate hybrid retriever\n",
    "    vector_retriever = VectorIndexRetriever(index=vector_index, similarity_top_k=i)\n",
    "    keyword_retriever = KeywordTableSimpleRetriever(index=keyword_index, num_chunks_per_query=i)\n",
    "    hybrid_retriever = HybridRetriever(vector_retriever, keyword_retriever, max_retrieve=i)\n",
    "    retriever_evaluator = RetrieverEvaluator.from_metric_names(\n",
    "        [\"mrr\", \"hit_rate\"], retriever=hybrid_retriever\n",
    "    )\n",
    "    eval_results = await retriever_evaluator.aevaluate_dataset(rag_eval_dataset)\n",
    "    print(from_eval_results_to_dataframe(f\"Hybrid retriever top_{i}\", eval_results))\n",
    "\n",
    "    # Evaluate vector retriever\n",
    "    vector_retriever = VectorIndexRetriever(index=vector_index, similarity_top_k=i)\n",
    "    retriever_evaluator = RetrieverEvaluator.from_metric_names(\n",
    "        [\"mrr\", \"hit_rate\"], retriever=vector_retriever\n",
    "    )\n",
    "    eval_results = await retriever_evaluator.aevaluate_dataset(rag_eval_dataset)\n",
    "    print(from_eval_results_to_dataframe(f\"Vector retriever top_{i}\", eval_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "1MB1YD1E3EKM"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "llamaindexkernel",
   "language": "python",
   "name": "llamaindexkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
