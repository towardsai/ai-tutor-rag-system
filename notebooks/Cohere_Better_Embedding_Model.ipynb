{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_x0uP9idZ_DJ"
      },
      "source": [
        "**Install Packages and Setup Variables**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q llama-index==0.14.0 openai==1.107.0 llama-index-finetuning==0.4.0 llama-index-embeddings-huggingface==0.6.0 jedi==0.19.2\\\n",
        "                llama-index-embeddings-cohere==0.6.0 cohere==5.17.0 llama-index-readers-web==0.5.1 tiktoken==0.11.0 chromadb==1.0.20 \\\n",
        "                llama-index-vector-stores-chroma==0.5.2 llama-index-llms-google-genai==0.3.0 llama-index-llms-openai==0.5.4 \\\n",
        "                llama-index-embeddings-instructor==0.4.0"
      ],
      "metadata": {
        "id": "p_KP7Ge_uII9",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CZxVvBx2CFak"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Set the following API Keys in the Python environment. Will be used later.\n",
        "# os.environ[\"OPENAI_API_KEY\"] = \"<YOUR_OPENAI_KEY>\"\n",
        "# os.environ[\"COHERE_API_KEY\"] = \"<YOUR_OPENAI_KEY>\"\n",
        "# os.environ[\"GOOGLE_API_KEY\"] = \"<YOUR_OPENAI_KEY>\"\n",
        "\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
        "os.environ[\"COHERE_API_KEY\"] = userdata.get('COHERE_API_KEY')\n",
        "os.environ[\"GOOGLE_API_KEY\"] = userdata.get('Google_api_key')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b5wNUYxuCHBE"
      },
      "outputs": [],
      "source": [
        "# Allows running asyncio in environments with an existing event loop, like Jupyter notebooks.\n",
        "\n",
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3o6KHBoaNr0"
      },
      "source": [
        "**Load a Model**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.embeddings.cohere import CohereEmbedding\n",
        "from llama_index.llms.openai import OpenAI\n",
        "from llama_index.core import Settings\n",
        "\n",
        "Settings.llm = OpenAI(model=\"gpt-5\", additional_kwargs={\"reasoning_effort\": \"minimal\"})\n",
        "\n",
        "Settings.embed_model = CohereEmbedding(model_name=\"embed-english-v3.0\",api_key=os.environ[\"COHERE_API_KEY\"])"
      ],
      "metadata": {
        "id": "wznQ7k8mfaym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xHQU21OECJVb"
      },
      "outputs": [],
      "source": [
        "# # If you have Paid Gemini API Key\n",
        "\n",
        "# from llama_index.llms.google_genai import GoogleGenAI\n",
        "# llm = GoogleGenAI(model=\"gemini-2.5-flash\", temperature=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VaifBRDcaU7X"
      },
      "source": [
        "**Create a VectoreStore**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IeOaYoQGZPNc"
      },
      "outputs": [],
      "source": [
        "import chromadb\n",
        "\n",
        "# create a client and a new collection\n",
        "# chromadb.EphemeralClient saves data in-memory.\n",
        "\n",
        "chroma_client = chromadb.PersistentClient(path=\"./mini-llama-articles\")\n",
        "chroma_collection = chroma_client.create_collection(\"mini-llama-articles\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tNr31jBoCLX3"
      },
      "outputs": [],
      "source": [
        "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
        "\n",
        "# Define a storage context object using the created vector database.\n",
        "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bU6I4qXmaZiM"
      },
      "source": [
        "## **Load the Dataset (CSV)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzRmU_DZaqX1"
      },
      "source": [
        "The dataset includes several articles from the TowardsAI blog, which provide an in-depth explanation of the LLaMA2 model. Read the dataset as a long string."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O0n2I1OZKrQ8"
      },
      "outputs": [],
      "source": [
        "!curl -o ./mini-llama-articles.csv https://raw.githubusercontent.com/AlaFalaki/tutorial_notebooks/main/data/mini-llama-articles.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpGJU4tIbCw6"
      },
      "source": [
        "**Read File**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v0SNZediCNBi"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "\n",
        "rows = []\n",
        "\n",
        "# Load the file as a JSON\n",
        "with open(\"./mini-llama-articles.csv\", mode=\"r\", encoding=\"utf-8\") as file:\n",
        "    csv_reader = csv.reader(file)\n",
        "\n",
        "    for idx, row in enumerate(csv_reader):\n",
        "        if idx == 0:\n",
        "            continue\n",
        "            # Skip header row\n",
        "        rows.append(row)\n",
        "\n",
        "# The number of characters in the dataset.\n",
        "len(rows)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRobYkGPbLjO"
      },
      "source": [
        "**Convert to Document obj**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U4c1Ym5YCPCs"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import Document\n",
        "\n",
        "# Convert the chunks to Document objects so the LlamaIndex framework can process them.\n",
        "documents = [\n",
        "    Document(text=row[1], metadata={\"title\": row[0], \"url\": row[2], \"source_name\": row[3]})\n",
        "    for row in rows[:12] # To Avoid Rate limit error in Cohere Trial API Key\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgn3S1JGbX3w"
      },
      "source": [
        "**Transforming**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "005zALbTCQrH"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.text_splitter import TokenTextSplitter\n",
        "\n",
        "# Define the splitter object that split the text into segments with 512 tokens,\n",
        "# with a 128 overlap between the segments.\n",
        "text_splitter = TokenTextSplitter(separator=\" \", chunk_size=512, chunk_overlap=128)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aB2biw88bmxE"
      },
      "source": [
        "There are two options to use the Cohere embeddings:\n",
        "\n",
        "- input_type=\"search_document\": Employ this option for texts (documents) intended for storage in your vector database.\n",
        "\n",
        "- input_type=\"search_query\": Use this when issuing search queries to locate the most related documents within your vector database."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9b4r3raRCWr2"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.extractors import (\n",
        "    SummaryExtractor,\n",
        "    QuestionsAnsweredExtractor,\n",
        "    KeywordExtractor,\n",
        ")\n",
        "from llama_index.embeddings.cohere import CohereEmbedding\n",
        "from llama_index.core.ingestion import IngestionPipeline\n",
        "\n",
        "\n",
        "# Create the pipeline to apply the transformation on each chunk,\n",
        "# and store the transformed text in the chroma vector store.\n",
        "pipeline = IngestionPipeline(\n",
        "    transformations=[\n",
        "        text_splitter,\n",
        "        QuestionsAnsweredExtractor(questions=3),\n",
        "        SummaryExtractor(summaries=[\"prev\", \"self\"]),\n",
        "        KeywordExtractor(keywords=10),\n",
        "        CohereEmbedding(model_name=\"embed-english-v3.0\", input_type=\"search_document\", api_key=os.environ[\"COHERE_API_KEY\"]),\n",
        "    ],\n",
        "    vector_store=vector_store,\n",
        ")\n",
        "\n",
        "# Run the transformation pipeline.\n",
        "nodes = pipeline.run(documents=documents, show_progress=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "va2wcjo2CXPI"
      },
      "outputs": [],
      "source": [
        "len(nodes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w5ucyprJCZIg"
      },
      "outputs": [],
      "source": [
        "len(nodes[0].embedding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xBHheNAUCnmE"
      },
      "outputs": [],
      "source": [
        "# Compress the vector store directory to a zip file to be able to download and use later.\n",
        "!zip -r vectorstore_cohere.zip mini-llama-articles"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0waG5hWdb1D7"
      },
      "source": [
        "**Load Indexes**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8BATim6b270"
      },
      "source": [
        "If you have already uploaded the zip file for the vector store checkpoint, please uncomment the code in the following cell block to extract its contents. After doing so, you will be able to load the dataset from local storage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KBdgyY_9CoGM"
      },
      "outputs": [],
      "source": [
        "# Load the vector store from the local storage.\n",
        "db = chromadb.PersistentClient(path=\"./mini-llama-articles\")\n",
        "chroma_collection = db.get_or_create_collection(\"mini-llama-articles\")\n",
        "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZrA6oDtGoylO"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import VectorStoreIndex\n",
        "\n",
        "# Create the index based on the vector store.\n",
        "index = VectorStoreIndex.from_vector_store(vector_store)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RF5Z_QCZcEwl"
      },
      "source": [
        "**Query Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "isHNAqSbCtTf"
      },
      "outputs": [],
      "source": [
        "# Define a query engine that is responsible for retrieving related pieces of text,\n",
        "# and using a LLM to formulate the final answer.\n",
        "query_engine = index.as_query_engine(similarity_top_k=5)\n",
        "\n",
        "res = query_engine.query(\"How many parameters LLaMA 2 model has?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zJjCN_VNT9s3"
      },
      "outputs": [],
      "source": [
        "res.response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-uFFe60NCw5S"
      },
      "outputs": [],
      "source": [
        "# Show the retrieved nodes\n",
        "for src in res.source_nodes:\n",
        "    print(\"Node ID\\t\", src.node_id)\n",
        "    print(\"Title\\t\", src.metadata[\"title\"])\n",
        "    print(\"Text\\t\", src.text)\n",
        "    print(\"Score\\t\", src.score)\n",
        "    print(\"-_\" * 20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDb7WHnRcne3"
      },
      "source": [
        "**Evaluate**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lQUQEYrHCxXQ"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.evaluation import generate_question_context_pairs\n",
        "from llama_index.llms.openai import OpenAI\n",
        "\n",
        "# Create questions for each segment. These questions will be used to\n",
        "# assess whether the retriever can accurately identify and return the\n",
        "# corresponding segment when queried.\n",
        "\n",
        "llm_gpt = OpenAI(model=\"gpt-5\", additional_kwargs={\"reasoning_effort\": \"minimal\"})\n",
        "\n",
        "rag_eval_dataset = generate_question_context_pairs(\n",
        "    nodes, llm=llm_gpt, num_questions_per_chunk=1\n",
        ")\n",
        "\n",
        "# We can save the evaluation dataset as a json file for later use.\n",
        "rag_eval_dataset.save_json(\"./rag_eval_dataset_cohere.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmlbVifNcv82"
      },
      "source": [
        "If you have uploaded the generated question JSON file, please uncomment the code in the next cell block. This will avoid the need to generate the questions manually, saving you time and effort."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "88bv7AerC2Al"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "#  A simple function to show the evaluation result.\n",
        "def display_results_retriever(name, eval_results):\n",
        "    \"\"\"Display results from evaluate.\"\"\"\n",
        "\n",
        "    metric_dicts = []\n",
        "    for eval_result in eval_results:\n",
        "        metric_dict = eval_result.metric_vals_dict\n",
        "        metric_dicts.append(metric_dict)\n",
        "\n",
        "    full_df = pd.DataFrame(metric_dicts)\n",
        "\n",
        "    hit_rate = full_df[\"hit_rate\"].mean()\n",
        "    mrr = full_df[\"mrr\"].mean()\n",
        "\n",
        "    metric_df = pd.DataFrame(\n",
        "        {\"Retriever Name\": [name], \"Hit Rate\": [hit_rate], \"MRR\": [mrr]}\n",
        "    )\n",
        "\n",
        "    return metric_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "jRgN3aSiC5mQ",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "from llama_index.core.evaluation import RetrieverEvaluator\n",
        "import time\n",
        "\n",
        "# We can evaluate the retievers with different top_k values.\n",
        "for i in [2, 4, 6, 8, 10]:\n",
        "    retriever = index.as_retriever(similarity_top_k=i)\n",
        "    retriever_evaluator = RetrieverEvaluator.from_metric_names(\n",
        "        [\"mrr\", \"hit_rate\"], retriever=retriever\n",
        "    )\n",
        "\n",
        "    eval_results = await retriever_evaluator.aevaluate_dataset(rag_eval_dataset)\n",
        "    print(display_results_retriever(f\"Retriever top_{i}\", eval_results))\n",
        "\n",
        "    time.sleep(70) # To Avaoid Rate limit error in Trial Cohere API Key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "S2DcdLmHU82p",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "from llama_index.core.evaluation import (\n",
        "    RelevancyEvaluator,\n",
        "    FaithfulnessEvaluator,\n",
        "    BatchEvalRunner,\n",
        ")\n",
        "from llama_index.llms.openai import OpenAI\n",
        "llm_gpt_mini = OpenAI(model=\"gpt-5-mini\", additional_kwargs={\"reasoning_effort\": \"minimal\"})\n",
        "\n",
        "for i in [2, 4, 6, 8, 10]:\n",
        "    # Set Faithfulness and Relevancy evaluators\n",
        "    query_engine = index.as_query_engine(similarity_top_k=i)\n",
        "\n",
        "    faithfulness_evaluator = FaithfulnessEvaluator(llm=llm_gpt_mini)\n",
        "    relevancy_evaluator = RelevancyEvaluator(llm=llm_gpt_mini)\n",
        "\n",
        "    # Run evaluation\n",
        "    queries = list(rag_eval_dataset.queries.values())\n",
        "    batch_eval_queries = queries[:20]\n",
        "\n",
        "    runner = BatchEvalRunner(\n",
        "        {\"faithfulness\": faithfulness_evaluator, \"relevancy\": relevancy_evaluator},\n",
        "        workers=8,\n",
        "    )\n",
        "    eval_results = await runner.aevaluate_queries(\n",
        "        query_engine, queries=batch_eval_queries\n",
        "    )\n",
        "    faithfulness_score = sum(\n",
        "        result.passing for result in eval_results[\"faithfulness\"]\n",
        "    ) / len(eval_results[\"faithfulness\"])\n",
        "    print(f\"top_{i} faithfulness_score: {faithfulness_score}\")\n",
        "\n",
        "    relevancy_score = sum(\n",
        "        result.passing for result in eval_results[\"faithfulness\"]\n",
        "    ) / len(eval_results[\"relevancy\"])\n",
        "    print(f\"top_{i} relevancy_score: {relevancy_score}\")\n",
        "    print(\"-_\" * 10)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GOxblDJN9-1J"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}