{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TCSQ-KFvNeMj"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/towardsai/ai-tutor-rag-system/blob/main/notebooks/Structured(JSON)_PDF_Data_Extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "rCndQqynkNiQ",
    "outputId": "dc2d6b50-4e3e-4ceb-c1d0-1c21b95aba1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m329.6/329.6 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -q openai==2.14.0 pypdf==6.5.0 google-genai==1.55.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "PdAwk6gTsDA3"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# Set the \"OPENAI_API_KEY\" in the Python environment. Will be used by OpenAI client later.\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"[OPENAI_API_KEY]\"\n",
    "\n",
    "from google.colab import userdata\n",
    "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
    "os.environ[\"GOOGLE_API_KEY\"] = userdata.get('GOOGLE_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kk_QGIJLT8qY"
   },
   "source": [
    "### OpenAI Structured Output without `response_format`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "S_F_PrQCT6lp"
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "gaCYqKDrT6Va",
    "outputId": "5d7e3c4c-f267-4464-eee2-57a79fe01bfe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"Top10BestSellingBooks\": [\n",
      "    {\n",
      "      \"title\": \"Don Quixote\",\n",
      "      \"author\": \"Miguel de Cervantes\",\n",
      "      \"yearPublished\": \"1605\",\n",
      "      \"summary\": \"A deluded nobleman becomes a self-styled knight-errant, embarking with his squire Sancho Panza on misadventures that satirize chivalry and explore reality versus illusion.\"\n",
      "    },\n",
      "    {\n",
      "      \"title\": \"A Tale of Two Cities\",\n",
      "      \"author\": \"Charles Dickens\",\n",
      "      \"yearPublished\": \"1859\",\n",
      "      \"summary\": \"Set in London and Paris during the French Revolution, intertwined lives face sacrifice and redemption amid political upheaval and personal loyalties.\"\n",
      "    },\n",
      "    {\n",
      "      \"title\": \"The Lord of the Rings\",\n",
      "      \"author\": \"J.R.R. Tolkien\",\n",
      "      \"yearPublished\": \"1954\",\n",
      "      \"summary\": \"A fellowship undertakes a perilous quest to destroy a powerful ring and defeat the Dark Lord Sauron, exploring friendship, courage, and fate in Middle-earth.\"\n",
      "    },\n",
      "    {\n",
      "      \"title\": \"The Little Prince\",\n",
      "      \"author\": \"Antoine de Saint-Exupéry\",\n",
      "      \"yearPublished\": \"1943\",\n",
      "      \"summary\": \"A pilot stranded in the desert meets a young prince from another planet, reflecting on love, loss, and what truly matters through whimsical encounters.\"\n",
      "    },\n",
      "    {\n",
      "      \"title\": \"Harry Potter and the Philosopher's Stone\",\n",
      "      \"author\": \"J.K. Rowling\",\n",
      "      \"yearPublished\": \"1997\",\n",
      "      \"summary\": \"An orphan discovers he is a wizard and attends Hogwarts, where he makes friends and confronts the dark legacy of Lord Voldemort.\"\n",
      "    },\n",
      "    {\n",
      "      \"title\": \"And Then There Were None\",\n",
      "      \"author\": \"Agatha Christie\",\n",
      "      \"yearPublished\": \"1939\",\n",
      "      \"summary\": \"Ten strangers are lured to an isolated island and accused of past crimes, then die one by one in a chilling, methodical mystery.\"\n",
      "    },\n",
      "    {\n",
      "      \"title\": \"Dream of the Red Chamber\",\n",
      "      \"author\": \"Cao Xueqin\",\n",
      "      \"yearPublished\": \"1791\",\n",
      "      \"summary\": \"A grand portrait of an aristocratic family's rise and fall, blending romance, social critique, and Buddhist-Daoist reflections on impermanence.\"\n",
      "    },\n",
      "    {\n",
      "      \"title\": \"The Hobbit\",\n",
      "      \"author\": \"J.R.R. Tolkien\",\n",
      "      \"yearPublished\": \"1937\",\n",
      "      \"summary\": \"Bilbo Baggins is swept into a quest with dwarves to reclaim a lost homeland from a dragon, discovering bravery and cunning along the way.\"\n",
      "    },\n",
      "    {\n",
      "      \"title\": \"The Lion, the Witch and the Wardrobe\",\n",
      "      \"author\": \"C.S. Lewis\",\n",
      "      \"yearPublished\": \"1950\",\n",
      "      \"summary\": \"Four siblings enter the magical land of Narnia through a wardrobe, joining Aslan to overthrow the White Witch's icy tyranny.\"\n",
      "    },\n",
      "    {\n",
      "      \"title\": \"She: A History of Adventure\",\n",
      "      \"author\": \"H. Rider Haggard\",\n",
      "      \"yearPublished\": \"1887\",\n",
      "      \"summary\": \"Explorers encounter an immortal queen in Africa, blending romance, adventure, and themes of power and immortality.\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are a helpful assistant designed to output information exclusively in JSON format. Your response should contain only raw JSON data with no additional text, explanations, or comments. Do not include backticks (`) or any code block delimiters in your response.\n",
    "\n",
    "Always use the key `\"Top10BestSellingBooks\"` when listing top 10 best-selling books. Follow the specified JSON structure below:\n",
    "\n",
    "### JSON Format Example\n",
    "{\n",
    "  \"Top10BestSellingBooks\": [\n",
    "    {\n",
    "      \"title\": \"Book Title\",\n",
    "      \"author\": \"Author Name\",\n",
    "      \"yearPublished\": \"Year\",\n",
    "      \"summary\": \"Brief summary of the book.\"\n",
    "    },\n",
    "    {\n",
    "      \"title\": \"Book Title 2\",\n",
    "      \"author\": \"Author Name 2\",\n",
    "      \"yearPublished\": \"Year\",\n",
    "      \"summary\": \"Brief summary of the book.\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "Always respond with clean JSON output that can be directly used with JSON parsers like `json.loads()`.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "prompt = \"Give me the names of the 10 best-selling books, their authors, the year they were published, and a concise summary in JSON format\"\n",
    "\n",
    "# Making the API call\n",
    "response = client.responses.create(\n",
    "  model=\"gpt-5\",\n",
    "  instructions=system_prompt,\n",
    "  input=prompt,\n",
    "  reasoning={\"effort\": \"minimal\"}\n",
    ")\n",
    "print(response.output[1].content[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hY1nUQ8QT6Rv",
    "outputId": "a52fbd02-a533-4439-b5e1-8270791692e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "{'title': 'Don Quixote', 'author': 'Miguel de Cervantes', 'yearPublished': '1605', 'summary': 'A deluded nobleman becomes a self-styled knight-errant, embarking with his squire Sancho Panza on misadventures that satirize chivalry and explore reality versus illusion.'}\n",
      "-------------------------------------\n",
      "Don Quixote\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "result_book = json.loads(response.output[1].content[0].text)\n",
    "\n",
    "print(type(result_book))\n",
    "\n",
    "print(result_book['Top10BestSellingBooks'][0])\n",
    "print(\"-------------------------------------\")\n",
    "print(result_book['Top10BestSellingBooks'][0]['title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ZpYV-q0--QB"
   },
   "source": [
    "## OpenAI Strucutred output (JSON) with `response_format`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "iH1CDNr30dtU"
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "8702bCYi9xpy"
   },
   "outputs": [],
   "source": [
    "prompt = \"Give me the names of the 10 best-selling books, their authors, the year they were published, and a concise summary in JSON format\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "IlCRdRTQ9FNh"
   },
   "outputs": [],
   "source": [
    "# The response format- JSON schema\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional\n",
    "from typing import List\n",
    "\n",
    "class Books(BaseModel):\n",
    "    \"\"\"Summary\"\"\"\n",
    "    title: str = Field(..., description=\"Title of the book\")\n",
    "    author: str = Field(..., description=\"Author of the book\")\n",
    "    yearPublished: int = Field(..., description=\"Year the book was published\", alias=\"yearPublished\")\n",
    "    summary: str = Field(..., description=\"Brief summary of the book\")\n",
    "\n",
    "class BookResponseFormatJSON(BaseModel):\n",
    "  Top10BestSellingBooks: List[Books] = Field(..., description=\"List of top 10 best-selling books\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "BwcVCEOExNuL"
   },
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are a helpful assistant designed to output information exclusively in JSON format.\n",
    "### JSON Format Example\n",
    "{\n",
    "  \"Top10BestSellingBooks\": [\n",
    "    {\n",
    "      \"title\": \"Book Title\",\n",
    "      \"author\": \"Author Name\",\n",
    "      \"yearPublished\": \"Year\",\n",
    "      \"summary\": \"Brief summary of the book.\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "kY_AfQvR91Ei",
    "outputId": "63009563-853c-4343-c57b-4b5d6be398c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top10BestSellingBooks=[Books(title='Don Quixote', author='Miguel de Cervantes', yearPublished=1605, summary='A deluded nobleman, inspired by chivalric romances, becomes a self-styled knight-errant with his squire Sancho Panza, leading to comedic and poignant adventures that question reality and idealism.'), Books(title='A Tale of Two Cities', author='Charles Dickens', yearPublished=1859, summary='Set in London and Paris during the French Revolution, the novel follows Charles Darnay and Sydney Carton in a story of sacrifice, resurrection, and the ravages of social injustice.'), Books(title='The Lord of the Rings', author='J. R. R. Tolkien', yearPublished=1954, summary='An epic quest across Middle-earth to destroy a powerful ring that could enslave the world, testing friendship, courage, and the corrupting lure of power.'), Books(title='The Little Prince', author='Antoine de Saint-Exupéry', yearPublished=1943, summary='A pilot stranded in the desert meets a young prince from another planet, whose fable-like tales explore love, loss, and seeing with the heart rather than the eyes.'), Books(title=\"Harry Potter and the Sorcerer's Stone\", author='J. K. Rowling', yearPublished=1997, summary='An orphan discovers he is a wizard, attends Hogwarts School, and confronts the dark wizard who killed his parents, beginning a journey of friendship and self-discovery.'), Books(title='And Then There Were None', author='Agatha Christie', yearPublished=1939, summary='Ten strangers are lured to a remote island and accused of past crimes; they are killed one by one in a chilling, puzzle-like mystery.'), Books(title='Dream of the Red Chamber', author='Cao Xueqin', yearPublished=1791, summary='A semi-autobiographical Qing dynasty saga depicting the rise and fall of an aristocratic family, rich with romance, poetry, and social observation.'), Books(title='The Hobbit', author='J. R. R. Tolkien', yearPublished=1937, summary='Bilbo Baggins, a reluctant hobbit, joins dwarves on a quest to reclaim their mountain home and treasure, encountering trolls, elves, and a pivotal ring.'), Books(title='The Lion, the Witch and the Wardrobe', author='C. S. Lewis', yearPublished=1950, summary='Four siblings enter the magical land of Narnia through a wardrobe, joining Aslan to defeat a tyrannical witch and restore peace.'), Books(title='The Da Vinci Code', author='Dan Brown', yearPublished=2003, summary='A symbologist and cryptologist unravel clues across Europe after a Louvre curator’s murder, exposing a conspiracy tied to religious history and secret societies.')]\n"
     ]
    }
   ],
   "source": [
    "# Making the API call\n",
    "response = client.responses.parse(\n",
    "  model=\"gpt-5\",\n",
    "  instructions=system_prompt,\n",
    "  input=prompt,\n",
    "  text_format=BookResponseFormatJSON,\n",
    "  reasoning={\"effort\": \"minimal\"}\n",
    ")\n",
    "print(response.output_parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lY4mHR4wQEzq",
    "outputId": "0a4a3137-174e-47f2-f473-1491b0ec3205"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '__main__.BookResponseFormatJSON'>\n"
     ]
    }
   ],
   "source": [
    "print(type(response.output_parsed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B96g0Rxwqjc9",
    "outputId": "02e0d249-71fd-4039-a6a7-7ecd9c81e464"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title='Don Quixote' author='Miguel de Cervantes' yearPublished=1605 summary='A deluded nobleman, inspired by chivalric romances, becomes a self-styled knight-errant with his squire Sancho Panza, leading to comedic and poignant adventures that question reality and idealism.'\n",
      "----------------------------------------------\n",
      "Don Quixote\n"
     ]
    }
   ],
   "source": [
    "print(response.output_parsed.Top10BestSellingBooks[0])\n",
    "print(\"----------------------------------------------\")\n",
    "print(response.output_parsed.Top10BestSellingBooks[0].title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nISPPdFA1WvX"
   },
   "source": [
    "## Strucutred output from PDF + OpenAI + pdf2images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "e2e3e4da63164d379a4abd6b8e7f3b08",
      "6df80c0c087e47faafd4c8f30a01c3b2",
      "875d5986d99146ef968963803a483aa2",
      "816b6ab9f90b4e52bb4c2f1a2a899d16",
      "f1113db23d0548c3b5c49238424cf986",
      "b77b35ff0ba84e14a3118053957ac9e5",
      "5c95405cefa5484d9523774f6235c3a4",
      "59e8fb21cd37480aaabd7cc8627557e9",
      "bf1e03cc7a264b92935e94b99b11d879",
      "45bb2e38fe0741ff9f086566179535d1",
      "74b26a99052a4b69bf34e674c77db260"
     ]
    },
    "id": "n3QpOuoOiMvB",
    "outputId": "2a49d85e-a8d9-43ed-db56-4adfb400d386"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2e3e4da63164d379a4abd6b8e7f3b08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "rag_research_paper.zip:   0%|          | 0.00/7.51M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "file_path = hf_hub_download(repo_id=\"jaiganesan/ai_tutor_knowledge\", filename=\"rag_research_paper.zip\",repo_type=\"dataset\",local_dir=\"/content\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZgQt32r8V50j",
    "outputId": "bdbbc5d8-7e8d-4982-b849-b9c5b3e4943f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  /content/rag_research_paper.zip\n",
      "   creating: /content/rag_research_paper/\n",
      "  inflating: /content/rag_research_paper/2405.07437v2.pdf  \n",
      "  inflating: /content/rag_research_paper/2407.01219v1.pdf  \n",
      "  inflating: /content/rag_research_paper/2407.07858v1.pdf  \n",
      "  inflating: /content/rag_research_paper/2407.08223v1.pdf  \n",
      "  inflating: /content/rag_research_paper/2407.16833v1.pdf  \n",
      "  inflating: /content/rag_research_paper/2407.21712v1.pdf  \n",
      "  inflating: /content/rag_research_paper/2408.08067v2.pdf  \n",
      "  inflating: /content/rag_research_paper/2408.08921v1.pdf  \n"
     ]
    }
   ],
   "source": [
    "!unzip /content/rag_research_paper.zip -d /content/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "f1Ogn8R7E5WZ"
   },
   "outputs": [],
   "source": [
    "system_instruction_prompt =\"\"\"\n",
    "You are an expert in extracting structured data from research paper PDFs.\n",
    "\n",
    "Task Description:\n",
    "Your task is to process an entire research paper provided as a PDF document and extract comprehensive, structured information from it. This includes all text, headlines, and detailed descriptions of visual elements. The final output must be a single, well-structured JSON object.\n",
    "\n",
    "Must Follow Guidelines:\n",
    "1.  Process the Entire PDF: Treat the input as a complete document, not as individual pages.\n",
    "2.  Accurate Data Extraction: Extract all text and information with high precision. Do not summarize, paraphrase, or omit any details.\n",
    "3.  Logical Structure: Organize the extracted content into logical sections based on the paper's structure (e.g., Abstract, Introduction, Methods, Results, Conclusion, Appendices).\n",
    "4.  Complete Information: Ensure no information is fragmented. Merge text that spans columns or pages into coherent paragraphs and sentences.\n",
    "\n",
    "Content Requirements:\n",
    "\n",
    "1.  Source Identification:\n",
    "    *   Accurately extract the arXiv ID (e.g., `arXiv:2405.07437v2`). Verify its correctness. If no arXiv ID is present, use `null`.\n",
    "\n",
    "2.  Headlines and Sections:\n",
    "    *   Extract all headlines and subheadlines to define the structure (e.g., \"1. Introduction,\" \"2.1. System Architecture\").\n",
    "    *   If a section of content has no visible headline, generate a descriptive title for it based on its content.\n",
    "    *   Each distinct section of the paper should become a separate object in the final JSON output.\n",
    "\n",
    "3.  Text Content:\n",
    "    *   For each section, extract the complete and verbatim text. Preserve all technical details, equations, and specific terminology.\n",
    "\n",
    "4.  Visual Elements (Figures, Tables, Graphs, Architectures):\n",
    "    *   Within the content of each section, when you encounter a visual element, provide a detailed analysis.\n",
    "    *   **Title/Caption:** Extract the exact title and caption.\n",
    "    *   **Detailed Description:** Describe the visual element's purpose and what it depicts.\n",
    "    *   **Key Information:** Detail the main trends, data points, comparisons, and conclusions shown. For architectures, describe the components, layers, and data flow.\n",
    "    *   **Contextual Insights:** Include any related insights or explanations from the surrounding text that refer to the visual element.\n",
    "\n",
    "Required Output Format (JSON):\n",
    "\n",
    "Your output must be a single JSON object that strictly adheres to the following structure:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"source_name\": \"Extract complete arXiv ID including prefix (e.g., arXiv:2405.07437v2). If none, use null.\",\n",
    "  \"source_id\": \"Extract complete arXiv ID including prefix (e.g., arXiv:2405.07437v2). If none, use null.\",\n",
    "  \"research_paper_data\": [\n",
    "    {\n",
    "      \"content_title\": \"The title of the first section (e.g., 'Abstract').\",\n",
    "      \"content\": \"The complete, verbatim text of the Abstract. Include descriptions of any visual elements if present.\"\n",
    "    },\n",
    "    {\n",
    "      \"content_title\": \"The title of the second section (e.g., '1. Introduction').\",\n",
    "      \"content\": \"The complete, verbatim text of the Introduction. This section should include detailed descriptions of any figures or tables found within it, as per the 'Visual Elements' guidelines.\"\n",
    "    },\n",
    "    .\n",
    "    .\n",
    "    .\n",
    "  ]\n",
    "}\n",
    "```\n",
    "Key Guidelines:\n",
    "- Extract exact content without summarization\n",
    "- Ensure accuracy in complex technical details\n",
    "- Maintain logical content organization\n",
    "- Include complete visual element analysis\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "HPKc_1JkwQfd"
   },
   "outputs": [],
   "source": [
    "# The response format- JSON schema\n",
    "class ResearchPaperData(BaseModel):\n",
    "  content_title: Optional[str] = Field(..., description=\"Extract or generate headlines and subheadlines (e.g., Abstract, Introduction, Methods, etc). Include section titles and subsection headings.\")\n",
    "  content: Optional[str] = Field(..., description=\"For each section: - Complete text content - Visual element descriptions - Figure/graph details: * Title/caption * Description * Key trends/comparisons * Architecture details * Related insights, Don't Summarize, Extract all the Content in the section\")\n",
    "\n",
    "class ResearchPaperResponseFormatJSON(BaseModel):\n",
    "  source_name: str = Field(..., description=\"Extract Research paper Title.\")\n",
    "  source_id: str = Field(..., description=\"Extract complete arXiv ID including prefix (e.g., arXiv:2405.07437v2). Verify ID accuracy multiple times. if there is no Arxiv ID return None\")\n",
    "  research_paper_data: List[ResearchPaperData] = Field(..., description=\"List of Extracted research paper data complete data without summarizing,\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "gQdjwxjT8Rb2"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import base64\n",
    "import json\n",
    "from pypdf import PdfReader, PdfWriter\n",
    "from openai import OpenAI\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI()\n",
    "\n",
    "# Parameters\n",
    "PDF_FOLDER = \"/content/rag_research_paper\"      # Folder containing multiple PDFs\n",
    "PAGES_PER_CHUNK = 5                             # Number of pages per chunk\n",
    "SYSTEM_INSTRUCTION = system_instruction_prompt  # Your system prompt\n",
    "RESPONSE_FORMAT = ResearchPaperResponseFormatJSON\n",
    "\n",
    "def split_pdf_by_pages(pdf_path, pages_per_chunk=PAGES_PER_CHUNK):\n",
    "    \"\"\"Split a single PDF into smaller page-range chunks.\"\"\"\n",
    "    reader = PdfReader(pdf_path)\n",
    "    total_pages = len(reader.pages)\n",
    "    chunks = []\n",
    "\n",
    "    for start in range(0, total_pages, pages_per_chunk):\n",
    "        end = min(start + pages_per_chunk, total_pages)\n",
    "        writer = PdfWriter()\n",
    "        for page_idx in range(start, end):\n",
    "            writer.add_page(reader.pages[page_idx])\n",
    "        chunk_filename = f\"{os.path.splitext(os.path.basename(pdf_path))[0]}_\" \\\n",
    "                         f\"pages_{start+1}_{end}.pdf\"\n",
    "        chunk_path = os.path.join(\"/content\", chunk_filename)\n",
    "        with open(chunk_path, \"wb\") as f_out:\n",
    "            writer.write(f_out)\n",
    "        chunks.append({\"path\": chunk_path, \"pages\": f\"{start+1}-{end}\"})\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "mYtvX44N8XxV"
   },
   "outputs": [],
   "source": [
    "def process_pdf_chunk(chunk, system_instruction, response_format):\n",
    "    \"\"\"Call OpenAI API to extract structured data from a PDF chunk.\"\"\"\n",
    "    with open(chunk[\"path\"], \"rb\") as f:\n",
    "        b64 = base64.b64encode(f.read()).decode(\"utf-8\")\n",
    "    response = client.responses.parse(\n",
    "        model=\"gpt-5\",\n",
    "        instructions=system_instruction,\n",
    "        text_format=response_format,\n",
    "        reasoning={\"effort\": \"minimal\"},\n",
    "        input=[{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"input_text\", \"text\": f\"Extract structured content from pages {chunk['pages']}.\"},\n",
    "                {\"type\": \"input_file\", \"filename\": os.path.basename(chunk[\"path\"]),\n",
    "                 \"file_data\": f\"data:application/pdf;base64,{b64}\"}\n",
    "            ]\n",
    "        }],\n",
    "    )\n",
    "    return response.output_parsed.research_paper_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "mjFUo7tYaCCb"
   },
   "outputs": [],
   "source": [
    "# from google import genai\n",
    "# from google.genai import types\n",
    "\n",
    "# # Initialize Gemini client\n",
    "# client = genai.Client(api_key=os.environ.get(\"GOOGLE_API_KEY\"))\n",
    "\n",
    "\n",
    "# def process_pdf_chunk(chunk, system_instruction,response_format):\n",
    "#     with open(chunk[\"path\"], \"rb\") as f:\n",
    "#         pdf_bytes = f.read()\n",
    "\n",
    "#     pdf_part = types.Part.from_bytes(\n",
    "#         data=pdf_bytes,\n",
    "#         mime_type=\"application/pdf\"\n",
    "#     )\n",
    "\n",
    "#     user_prompt = f\"Extract structured content from pages {chunk['pages']}.\"\n",
    "\n",
    "#     # Call Gemini with structured output\n",
    "#     response = client.models.generate_content(\n",
    "#         model=\"gemini-2.5-flash-preview-09-2025\",\n",
    "#         contents=[pdf_part,user_prompt],\n",
    "#         config=types.GenerateContentConfig(\n",
    "#             system_instruction=system_instruction,\n",
    "#             response_mime_type=\"application/json\",\n",
    "#             response_schema=response_format,\n",
    "#         )\n",
    "#     )\n",
    "\n",
    "#     # Parse the response\n",
    "#     parsed_response = json.loads(response.text)\n",
    "#     return parsed_response.get(\"research_paper_data\", [])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "jeMovQC68d0v"
   },
   "outputs": [],
   "source": [
    "def convert_to_dict(obj):\n",
    "    \"\"\"Convert Pydantic model or custom object to dictionary.\"\"\"\n",
    "    if hasattr(obj, 'model_dump'):\n",
    "        # For Pydantic v2\n",
    "        return obj.model_dump()\n",
    "    elif hasattr(obj, 'dict'):\n",
    "        # For Pydantic v1\n",
    "        return obj.dict()\n",
    "    elif hasattr(obj, '__dict__'):\n",
    "        # For regular objects\n",
    "        return obj.__dict__\n",
    "    else:\n",
    "        # If it's already a basic type\n",
    "        return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V_eTVa5M8kW6",
    "outputId": "e0e52cf2-164c-435e-bc98-f318434c88c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 2407.21712v1.pdf...\n"
     ]
    }
   ],
   "source": [
    "# Main workflow\n",
    "all_results = {}\n",
    "pdf_paths = glob.glob(os.path.join(PDF_FOLDER, \"*.pdf\"))\n",
    "\n",
    "for pdf_path in pdf_paths:\n",
    "    pdf_name = os.path.basename(pdf_path)\n",
    "    print(f\"Processing {pdf_name}...\")\n",
    "    chunks = split_pdf_by_pages(pdf_path)\n",
    "    results = []\n",
    "    for chunk in chunks:\n",
    "        data = process_pdf_chunk(chunk, SYSTEM_INSTRUCTION, RESPONSE_FORMAT)\n",
    "        # Convert each ResearchPaperData object to a dictionary\n",
    "        for item in data:\n",
    "            results.append(convert_to_dict(item))\n",
    "    all_results[pdf_name] = results\n",
    "\n",
    "    # Remove the break if you want to process all PDFs**\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wBfYOkx-tVvj",
    "outputId": "90cc2637-4889-46fd-e65e-2bd299d2a0b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved structured data to 2407.21712v1_structured.json\n",
      "\n",
      "Cleaning up temporary chunk files...\n",
      "Removed: /content/2407.21712v1_pages_11_12.pdf\n",
      "Removed: /content/2407.21712v1_pages_6_10.pdf\n",
      "Removed: /content/2407.21712v1_pages_1_5.pdf\n"
     ]
    }
   ],
   "source": [
    "# Save combined results for each PDF\n",
    "for pdf_name, data in all_results.items():\n",
    "    out_file = f\"{pdf_name.rsplit('.',1)[0]}_structured.json\"\n",
    "    with open(out_file, \"w\") as f_json:\n",
    "        json.dump(data, f_json, indent=2)\n",
    "    print(f\"Saved structured data to {out_file}\")\n",
    "\n",
    "print(\"\\nCleaning up temporary chunk files...\")\n",
    "for pdf_path in pdf_paths:\n",
    "    pdf_base = os.path.splitext(os.path.basename(pdf_path))[0]\n",
    "    chunk_pattern = os.path.join(\"/content\", f\"{pdf_base}_pages_*.pdf\")\n",
    "    for chunk_file in glob.glob(chunk_pattern):\n",
    "        os.remove(chunk_file)\n",
    "        print(f\"Removed: {chunk_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "ZDx-hRVctgHQ",
    "outputId": "a4dc337f-57c4-4eef-ce10-4d2712a57668"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'content_title': 'Title and Authors',\n",
       "  'content': 'Adaptive Retrieval-Augmented Generation for Conversational Systems\\nXi Wang1, Procheta Sen2, Ruizhe Li3, Emine Yilmaz4\\n1University of Sheffield, 2University of Liverpool, 3University of Aberdeen, 4University College London\\nxi.wang@sheffield.ac.uk1, procheta.sen@liverpool.ac.uk2, ruizhe.li@abdn.ac.uk3, emine.yilmaz@ucl.ac.uk4'},\n",
       " {'content_title': 'Abstract',\n",
       "  'content': 'Despite the success of integrating large language models into the development of conversational systems, many studies have shown the effectiveness of retrieving and augmenting external knowledge for informative responses. Hence, many existing studies commonly assume the always need for Retrieval Augmented Generation (RAG) in a conversational system without explicit control. This raises a research question about such a necessity. In this study, we propose to investigate the need for each turn of system response to be augmented with external knowledge. In particular, by leveraging human judgements on the binary choice of adaptive augmentation, we develop RAGate, a gating model, which models conversation context and relevant inputs to predict if a conversational system requires RAG for improved responses. We conduct extensive experiments on devising and applying RAGate to conversational models and well-rounded analyses of different conversational scenarios. Our experimental results and analysis indicate the effective application of RAGate in RAG-based conversational systems in identifying system responses for appropriate RAG with high-quality responses and a high generation confidence. This study also identifies the correlation between the generation’s confidence level and the relevance of the augmented knowledge.'},\n",
       " {'content_title': '1 Introduction',\n",
       "  'content': 'Recently, the advancement of Large Language Models (LLMs) has significantly improved conversational systems, enabling the generation of natural and high-quality responses (Ni et al., 2023). Despite these advancements, recent studies have identified several limitations on the simple use of LLMs to address conversational tasks (Onoe et al., 2022; Huang et al., 2021; Ren et al., 2018). These limitations include the lack of up-to-date knowledge (Onoe et al., 2022), the generation of non-factual or hallucinated content (Huang et al., 2021), and restricted domain adaptability (Ren et al., 2018). These issues can hinder the development of conversational agents with satisfactory user experience. To address these identified challenges, a common approach is to retrieve and augment LLMs with external knowledge to enhance the conversational response, making them more accurate, reliable, and adaptable to different domains (Zhao et al., 2020; Lian et al., 2019; Ye et al., 2024). For example, Shuster et al. (2021) demonstrated that using a dense retrieval model (DPR) (Karpukhin et al., 2020) to retrieve relevant knowledge for augmentation can significantly reduce the hallucination rate, according to a corresponding human evaluation. Similarly, Yang et al. (2020) showed that leveraging a graph-structured knowledge base can boost the reasoning ability and domain generalisability of task-oriented conversational agents. These achievements of knowledge-augmented techniques highlight a promising direction for enhancing conversational agents and address the current limitations.\\nHowever, while implementing retrieval augmentation to a conversational system for improved response, we question the necessity of knowledge augmentation for every turn of system responses. To develop effective human-computer conversations, it is essential to provide factual and relevant responses, offer appropriate amount of information, and not unnaturally drive and shift the conversation to non-relevant topics (Kasirzadeh and Gabriel, 2023; Miehling et al., 2024). We argue that overusing external knowledge could result in system responses against these core criteria. Figure 1 presents a conversation example that shows how the system response to a generic user utterance about suggesting activities can vary with and without augmented knowledge. The knowledge-augmented system response is being information conditioned with limited diversity and assuming specific user preferences. In contrast, without the addition of external knowledge, the system response is more diverse and natural in this early stage of a conversation. This indicates that misusing external knowledge can lead to problematic system responses and a negative user experience.\\nTo address this, we investigate an adaptive retrieval-augmented generation solution for effective conversational systems. In particular, motivated by the gate function in long-short term memory models (Graves and Graves, 2012), which explicitly controls the use of input and memory, we propose a binary knowledge gate mechanism, called RAGate, to manipulate the use of external knowledge for a conversational system. To model the conversation context and accurately estimate the need for augmentation, we leverage the human labels as ground truth and develop RAGate by exploring the use of recent advanced language models or constructing attention neural gate models. To validate the effectiveness of RAGate, we conduct extensive experiments on an annotated Task-Oriented Dialogue (TOD) system dataset, KETOD, that builds upon the SGD dataset with TOD-spanning 16 domains, such as Restaurant and Weather. The experimental results show that RAGate enables conversational systems to efficiently use external knowledge at appropriate conversation turns, producing high-quality system responses. In particular, by modelling the uncertainty and confidence level of the system – which correlates with the likelihood of hallucinated output (Varshney et al., 2023) – we show that the \"always\" augmentation of external knowledge can significantly increase generation uncertainty and the risk of hallucination. After applying RAGate, we can effectively control the conversation system to make confident and informative responses. In addition, by varying the use of knowledge snippets in different relevance levels, we also observe the positive correlation between the calculated confidence score and the relevance of augmented knowledge, which can be valuable for many future studies.\\nVisual Element: Figure 1\\n- Title/Caption: Figure 1: Example conversation when generating a response with or without a knowledge snippet using a language model (GPT-4 in this example).\\n- Detailed Description: The figure shows a dialogue snippet illustrating two alternative system responses to the same user request: “Can you find me some interesting things to do?” A “Knowledge” box contains a snippet about clouds: “Cloud on earth, clouds are formed by the saturation of air in the homosphere. Cloud the Droplets or particles are suspended in the atmosphere above the surface of a planetary body.” Two paths are indicated: “Use Knowledge” and “Not Use Knowledge.” Under “Not Use Knowledge,” the system replies with general, diverse suggestions such as creative, physical, and entertainment activities. Under “Use Knowledge,” the system provides suggestions that include “Explore the science of clouds,” alongside virtual museum tours, online courses, and reading.\\n- Key Information: The knowledge-conditioned response becomes narrowly focused on the provided cloud-related snippet, potentially reducing diversity and imposing implicit user preferences. The non-knowledge response remains broad and conversationally natural for an early-stage, open-ended user query.\\n- Contextual Insights: The surrounding text uses this example to argue that indiscriminate augmentation with external knowledge at every turn may harm user experience by constraining responses and introducing irrelevant specificity.'},\n",
       " {'content_title': 'arXiv Identifier and Metadata',\n",
       "  'content': 'arXiv:2407.21712v1 [cs.CL] 31 Jul 2024'},\n",
       " {'content_title': '2 Related Work',\n",
       "  'content': 'In the pipeline of knowledge-augmented generation for a conversation system, two main components are identified: the knowledge retriever and the response generator. Existing studies have improved conversational responses to different extents by improving one or both components (Li et al., 2022; Komeili et al., 2022; Wang et al., 2024).\\nKnowledge Retrieval: Several studies have explored the use of dense passage retrieval techniques (Lewis et al., 2020; Karpukhin et al., 2020) and public search service for effective retrievers (Li et al., 2022). For example, Li et al. (2022) retrieved Wikipedia passages through a database interface and then ranked them according to statistical relevance, calculated by TF-IDF, or semantic relevance as per cosine similarity. Similarly, Komeili et al. used a search engine API to retrieve relevant knowledge but first transformed the dialogue context into a natural search query using an encoder-decoder model before searching.\\nJoint Optimisation of Retriever and Generator: On the other hand, another thread of research studies has explored joint optimisation approaches. For instance, Shi et al. (2023) introduced a retriever-generator architecture that aims to improve the performance of Task-Oriented Dialogue (TOD) systems by using a dual-feedback mechanism. The retriever identifies relevant knowledge from a database, while the generator uses this information to create appropriate system responses. The feedback from the generator is further used as pseudo-labels to train the retriever to select pertinent information. Shen et al. (2023) introduced a training method based on maximal marginal likelihood. This method jointly optimise a perceptive retriever and the response generation in a feedback loop. The proposed approach incorporates meta-knowledge, which guides the generator to improve the utilisation of knowledge and, consequently, the quality of the generated responses. Kang et al. (2023) further advance the retriever by proposing SUbgraph Retrieval-augmented GEneration (SURGE), which employed a graph neural network (GNN)-based context-relevant subgraph retriever. SURGE incorporates contrastive learning to optimise the latent representation space, ensuring that generated texts closely resemble the retrieved subgraphs.\\nDespite the richness of existing retrieval-augmented generation techniques for conversational systems, they commonly hypothesise that every conversation turn needs external knowledge. However, the necessity of augmenting every turn of the conversation with external knowledge remains questionable. A relevant thread of work that aims to answer this question is the introduction of the knowledge-seeking turn detection task using the DSTC-9 dataset (Kim et al., 2020), and the follow-up studies, such as (Hong et al., 2023; Jin et al., 2021). However, this task is to identify the turns in conversations injected by human workers about knowledge enquiry instead of identifying the system responses that require knowledge augmentation for improvements. This research gap highlights the value and novelty of this study, which investigates the adaptive use of retrieval-augmented generation for advanced conversational systems.\\nVisual Element: Figure 2\\n- Title/Caption: Figure 2: RAGate variants for implementing the gating function. The three variants are the prediction with pre-trained language models after prompting (1), after parameter-efficient fine-tuning (2), and with a multi-head attention encoder (3).\\n- Detailed Description: The figure shows three schematic variants. Variant (1) depicts an LLM receiving “Prompt ⊕ LLM Context or (Context + knowledge)” and producing a “Prediction.” Variant (2) shows “Prompt ⊕ LLM Context or (Context + knowledge)” feeding into “Add & Norm,” “Multi-Head Attention,” “Add & Norm,” “Feed Forward,” stacked N times, followed by “Linear,” “Softmax,” and “Prediction,” representing a parameter-efficient fine-tuned architecture. Variant (3) illustrates an encoder stack that processes inputs (context or context plus knowledge) with multi-head attention and feed-forward layers, culminating in a softmax prediction. A separate block shows a “Response Generator” that consumes “Context or (Context & knowledge)”, conditioned on the RAGate decision and “External Knowledge.”\\n- Key Information: The figure clarifies that RAGate can be implemented via prompting an LLM, parameter-efficient fine-tuning of an LLM, or a standalone multi-head attention encoder classifier. It also indicates how the gate’s binary decision controls whether the response generator receives contextual input alone or augmented with retrieved knowledge.\\n- Contextual Insights: The text around the figure positions these as three complementary instantiations to study adaptive gating over external knowledge in conversational RAG systems.'},\n",
       " {'content_title': '3 Methodology',\n",
       "  'content': '3.1 Problem Formulation\\nThis study addresses the challenge of effectively identifying conversation turns that require augmentation of external knowledge. In particular, we aim to develop a gate mechanism that dynamically determines when to search for external knowledge to ensure natural, relevant and contextually appropriate responses. First, we define the task of user-system conversation. Let D = {d1, d2, ..., d|D|} be a set of user-system dialogues, and each dialogue d comprises a sequence of interactions between users and systems (i.e., d = {u0, s0, u1, s1, ..., uT , sT }) with varying lengths. Here, ut and st denote the user utterance and system response at the t-th turn, respectively. The conversational context up to turn t can be formulated by aggregating the previous user-system interactions, i.e., ct = u0, s0, .., ut. With this context information ct, the conversation system can augment it with a list of retrieved external knowledge, et,k, where k represents the ranking cutoff for the retrieved knowledge. Hence, the binary gate mechanism proposed in this study, deciding the knowledge augmentation, can be formulated as f(ct) = {0, 1} or f(ct, et,k) = {0, 1} if the external knowledge et,k is considered. Then, the follow-up response generation function g(·) can be formulated as follows:\\ng(·) = ( g(ct, et,k) if f(ct) or f(ct, et,k) ; g(ct) otherwise. (1)\\nHence, by evaluating and estimating the necessity of augmenting with external knowledge, we dynamically update the conversational response generation accordingly.\\n3.2 RAGate Gate Mechanism\\nTo effectively estimate the need to use external knowledge and implement adaptive retrieval augmented generation for a conversation system, we introduce our proposed gate mechanism, RAGate, that uses the conversational context and, optionally, the retrieved external knowledge to predict the binary choice of using external knowledge. In particular, we explore three RAGate variants that are implemented by the use of Large Language Models (LLMs) with devised prompts, with parameter efficient fine-tuning (e.g., QLoRA (Dettmers et al., 2024)) and the construction of an end-to-end multi-head attention encoder. This exploration is motivated by the recent advancement of transformer-structured neural models in natural language processing. In Figure 2, we illustrate the application of RAGate and its three variants. We describe each of these three variants to clarify the use of RAGate:\\nRAGate-Prompt: As denoted by Arora et al. (2022), a language model can effectively adapt to new tasks by using a natural language prompt that explains the process to address the tasks without extra training. Hence, we can formulate a gate function f(·) as f(y|ct) = f(y|Θ, ct, p), where Θ denotes the used language model with its pre-trained weights and p is the devised natural language prompt. Alternatively, if the retrieved knowledge is also involved in prediction, we have f(y|ct) = f(y|Θ, ct, et,k, p). Specifically, we explore two types of prompts: zero-shot and in-context learning. Zero-shot prompts describe the task that uses the conversational context and, optionally, the retrieved knowledge to generate a response with binary feedback. As for the in-context learning prompts, we augment the zero-shot prompts with illustrative examples. We show the set of prompts in Appendix A.\\nRAGate-PEFT: Despite the high adaptability of the language model with devised prompts, we further explored the use of instruction tuning on language models with a parameter-efficient fine-tuning method (i.e., QLoRA (Dettmers et al., 2024)) to meet the goal of an effective gate function. QLoRA is built upon the known Low-rank Adapter (LoRA) (Hu et al., 2021), which keeps the pre-trained weight matrix W0 frozen and addresses the gradient updates of the weight matrix ∆W through low-rank approximation (i.e., ∆W = BA, where B and A are the result of lower-rank decomposition on ∆W). Hence, the forward pass during the model training can be updated from h = W0x + ∆W x to h = W0x + BAx. QLoRA (Dettmers et al., 2024), which is used in this study, further quantises the language model into a 4-bit NormalFloat data type and leverages the page-to-page transfer between the CPU and GPU to further avoid memory spikes. To implement RAGate-PEFT, we format the train data with devised instructions, joined with paired inputs and outputs for developing parameter-efficient fine-tuned large language models. In particular, we provide a set of instruction-input-output triples for model training. The input can vary with the provision of a set of available features. Apart from the use of the conversational context (contx), we also include the system response (resp), synthetic responses generated by the language model (syn-resp) due to the missing responses as input in the practical scenario, the name entities within the incoming responses (ner), retrieved knowledge (know) and the description of the knowledge source, e.g., the WikiHow website (source). By using various combinations of inputs and customising the corresponding instructions, we explore the effectiveness of the resulting learned language models that implement the RAGate-PEFT.\\nRAGate-MHA: Apart from the use of pre-trained language models and further fine-tuned language models, we also explore the introduction of a multi-head attention neural encoder to model the context as input and estimate the augmentation necessity (i.e., RAGate-MHA). Here, we describe the model structure of RAGate-MHA. At first, as denoted by (Vaswani et al., 2017), the attention mechanism is formulated as the interaction between three objects, queries Q, keys K, and values V : Attention(Q, K, V ) = softmax( QKT / √dk )V. To estimate the necessity of augmentation, we fit the context and the retrieved knowledge into the roles of these three objects. Specifically, we include the setups of (1) using context only (contx) or (2) using the concatenated context and retrieved knowledge (contx ⊕ know) as queries, keys, and values, and (3) using the context as queries and interact with the retrieved knowledge as keys and values (contx × know). Next, following (Vaswani et al., 2017) in the encoder construction of a transformer model, we encode the inputs via an input embedding layer into latent vectors and a position encoding layer to encode the order of tokens in the sequence. After that, we leverage the multi-head attention to learn attention weights on the inputs and then followed by a feed-forward network:\\nFFN(x) = max(0, xW1 + b1)W2 + b2 (2)\\nwhere W1 and W2 are two learned parameter matrics with two bias terms (b1 and b2). Both multi-head attention and feed-forward neural modules are followed by residual connection (He et al., 2016) and layer normalisation (Ba et al., 2016). Unlike the introduction of another decoder module that addresses the sequence-to-sequence generation in (Vaswani et al., 2017), we followed the encoder output with a linear projection module and a softmax function for our binary classification task.'},\n",
       " {'content_title': '4 Model Training and Evaluation Setups',\n",
       "  'content': 'We evaluate the performance of introducing RAGate according to its binary classification performance and the effectiveness of the resulting response generation. Specifically, we use the KETOD dataset (Chen et al., 2022), which has fully annotated 5,324 dialogues and 52,063 turns of conversations. In particular, it is associated with 33,761 knowledge snippets to be retrieved and augmented. In addition, KETOD was developed with human labels on turns of conversations (around 12.1% of turns) about the need for augmenting with retrieved knowledge snippets for a natural and informative system response. Hence, we use these human labels as natural ground truths when evaluating RAGate. It is worth indicating that many current knowledge-augmented conversational datasets often ground their conversations on the knowledge snippet, such as Wizard of Wikipedia (Dinan et al., 2018) and CMU_DoG (Zhou et al., 2018), which makes them not a natural fit to be investigated in this study.\\nDue to the limited computational resource availability, we explore the use of Llama-v2-7B and Llama-v2-13B to implement RAGate-prompt and fine-tune Llama-v2-7B for RAGate-PEFT. We implement QLoRA using the PEFT library (Mangrulkar et al., 2022) and set the lower rank to 16. As discussed in Section 3, we have various input features to be combined for performance optimisation. We begin with the use of context only, then concatenate the context with the real response (contx-resp), with the synthetic response and recognised entities (contx-syn-resp-ner) and further extend with the use of retrieved knowledge (contx-syn-resp-ner-know) or the source of knowledge (contx-syn-resp-ner-source). Specifically, we retrieve the relevant knowledge by exploring the use of TF-IDF and a learned BERT ranker. We evaluate their performance with the classic Recall@1 and Recall@3 on the test collection. We use a shallow cutoff because we only use top-relevant knowledge snippets for augmentation. Table 1 shows their retrieval performance. According to the leading performance of BERT-Ranker, we augment knowledge with its retrieved top 3 relevant knowledge snippets (i.e., k = 3). Regarding the development of RAGate-MHA, we explore the combinations of 2 to 8 layers, 2 or 4 heads and the embedding size in [64, 128, 256] for the best classification accuracy. We report the precision, recall, F1, Area Under Curve (AUC) and the False Discovery Rate (FDR) as the main measures to show the classification effectiveness. Next, we further deploy the best-performing RAGate gate function to update the KETOD dialogue system (Chen et al., 2022), which uses GPT-2 (Radford et al., 2019) as the backbone model. To highlight the effect of various augmentation setups, we use the context with the gold action without extra prediction as input to KETOD. Then, we compare the resulting performance to the KETOD model without knowledge augmentation and augmenting every system response as baselines. To report the response generation effectiveness, we report how close the response is to the ground truth via BLEU, ROUGE-1/2/L and BERTScores and the confidence score calculated by the minimum probabilities of individual tokens that compose the response. As argued by Varshney et al. (2023), this calculated confidence score can highly correlate with a language model’s likelihood of generating hallucinated responses. We trained our models and conducted the evaluations on one machine with one NVIDIA 4090 GPU.\\nVisual Element: Table 1\\n- Title/Caption: Table 1: Retrieval Performance Evaluation when using context as the query.\\n- Detailed Description: A table listing retrieval models and their recall metrics: TF-IDF (Recall@1 = 0.0227, Recall@3 = 0.0871) and BERT-Ranker (Recall@1 = 0.2475, Recall@3 = 0.4714).\\n- Key Information: BERT-Ranker substantially outperforms TF-IDF at shallow cutoffs, justifying its use for selecting top-3 knowledge snippets (k = 3) for augmentation.\\n- Contextual Insights: The choice of retriever directly impacts the quality of the knowledge fed to the RAG pipeline and subsequently the gating and generation components.'},\n",
       " {'content_title': '5 Results and Analysis — 5.1 Augmentation Need Classification',\n",
       "  'content': 'First, we evaluate the classification accuracy of our developed RAGate gate methods for addressing the adaptive RAG to system responses. Table 2 presents the classification performance of RAGate baselines while evaluated on the test collection of the KETOD dataset, which includes rich human labels on the use of RAG for response generation. As discussed in Section 3, we explore the development of RAGate with three variants: the use of LLM prompting (RAGate-Prompt), parameter-efficient fine-tuned LLMs (RAGate-PEFT), and a neural classifier with Multi-Head Attention structure (RAGate-MHA).'},\n",
       " {'content_title': 'Table 2: Classification accuracy on adaptive augmentation for system response',\n",
       "  'content': 'Model Variants Precision Recall F1\\nRAGate-Prompt: LLMs – Zero Shot\\nLlama-2-7B 0.1323 0.0278 0.0460\\nLlama-2-13B 0.1422 0.1083 0.1230\\nRAGate-Prompt: LLMs – In-Context Learning\\nLlama-2-7B 0.1417 0.0294 0.0487\\nLlama-2-13B 0.0989 0.0851 0.0915\\nRAGate-PEFT: Parameter Efficient Fine-tuned LLMs (Llama2-7B)\\n[contx⊕resp] 0.4926 0.3095 0.3802\\ncontx-only 0.5203 0.3359 0.4082\\ncontx-(syn-resp)-ner 0.6818 0.2321 0.3464\\ncontx-(syn-resp)-ner-know 0.4698 0.0603 0.1069\\ncontx-(syn-resp)-ner-source 0.4000 0.0185 0.0355\\nRAGate-MHA: Context with / without Knowledge Input\\nMHA(contx)-h(4)-l(5)-emb(64) 0.3210 0.5541 0.4065\\nMHA([contx⊕know])-h(4)-l(2)-emb(64) 0.2795 0.5201 0.3636\\nMHA(contx×know)-h(4)-l(2)-emb(64) 0.2272 0.5835 0.3271\\nRAGate-MHA: Context-Response Input\\nMHA([contx⊕resp])-h(4)-l(4)-emb(64) 0.3500 0.5510 0.4281\\nTable 2: Classification accuracy on adaptive augmentation for system response. \"contx\", \"resp\", and \"know\" refer to the use of context, initial system response, and retrieved knowledge snippets as input. \"syn-resp\" and \"ner\" are the additional synthetic response and name entity recognition steps in the model fine-tuning prompts. h, l and emb refer to the best-performed configuration on the number of heads, layers and embedding size.'},\n",
       " {'content_title': 'RAGate performance with LLM prompting versus fine-tuning',\n",
       "  'content': 'RAGate performance with LLM prompting versus fine-tuning. By comparing the corresponding performance reported in Table 2, we observe that, on average, fine-tuning a Llama-2-7B with QLoRA (i.e., RAGate-PEFT) can significantly outperform RAGate-Prompt. For example, by looking at the RAG-PEFT with context-only input, without using extra input features and instruction updates, it can outperform all RAG-Prompt approaches by a big margin (e.g., 0.4082 versus the highest 0.1230 F1 scores). This reflects the difficulty of this adaptive knowledge augmentation task, which can not be properly addressed by prompting a general pretrained language model. In particular, the use of larger language models and the in-context learning setup, which often result in improved performance (Arora et al., 2022), can not guarantee the enhancement of models’ classification accuracy regarding this classification task.\\nRegarding the performance of RAGate-PEFT approaches, by first examining the effect of using synthetic response and recognised name entities, we observe significantly improved precision (0.5203 to 0.6818) but with the cost of lower recall (0.3359 to 0.2321). In addition, when we add the retrieved knowledge to the input features for prediction, we observe a significant performance drop across all evaluated aspects. This can be caused by the additional complexity introduced by the included retrieved knowledge snippets. Furthermore, we also explored the performance impact of naming the source of the knowledge snippet. We use wikiHow1 in this study, which can provide rich task instructions for offering informative task-oriented system response (Sen et al., 2023). However, the fine-tuned model cannot reasonably connect the promised rich resource from the knowledge source and the prediction of augmentation necessity.'},\n",
       " {'content_title': 'RAGate Performance between fine-tuned LLM and MHA classifier',\n",
       "  'content': 'RAGate Performance between fine-tuned LLM and MHA classifier. Next, by comparing the experimental results of RAGate-MHA and RAGate-PEFT in Table 2, we observe a wide-margin recall improvement using RAGate-MHA, reaching a minimum recall of 0.52, but with significantly lower precision accuracy. In Table 2, we also include the use of both the context and the initial system responses (i.e., MHA([contx, resp])) for additional insights. We can observe that a higher precision can be achieved but the use of response does not improve the recall performance. These results are consistent with the observed performance of RAGate-PEFT, which further encourages the use of a synthetic response due to the unavailability of a system response in a practical scenario. In addition, we also observe a similar performance drop when including the retrieved knowledge snippets for classification. Even though the RAGate-MHA model, using the interaction between context and retrieved knowledge snippets, can achieve the highest recall of 0.5835, it can not outperform the RAGate-MHA using context-only on other metrics. Hence, considering the similar F1 and AUC performance levels of RAGate-PEFT and RAGate-MHA leads to a trade-off balance between precision and recall for the two groups of approaches. To further evaluate the classification effectiveness of RAGate, in Appendix B, we provide a detailed discussion of a conducted user study that explores whether RAGate can also assess the potential contribution of retrieved snippets when predicting the decision for retrieval augmentation.'},\n",
       " {'content_title': '5.2 Adaptive Augmentation Analysis',\n",
       "  'content': '5.2 Adaptive Augmentation Analysis\\nIn addition to the classification accuracy, we also compare the choice of human workers and RAGate approaches in augmenting specific turns.\\n1\\nhttps://www.wikihow.com\\nVisual Element: Figure 3\\n- Title/Caption: Figure 3: Frequency analysis of adaptive augmentations about the position of a conversation.\\n- Detailed Description: A bar or histogram-style figure that shows the number of selected augmentations across binned relative positions within conversations from 10% to 100% of the conversation length. Three series are compared: RAGate-PEFT, RAGate-MHA, and Human Labels.\\n- Key Information: The x-axis is labeled \"Position of Turns in a Conversation\" with tick labels 10% 20% 30% 40% 50% 60% 70% 80% 90% 100%. The y-axis is \"Number of Selected Augmentations\" with values indicated up to at least 300. Human labels show most selections at the beginning of conversations; RAGate-MHA closely follows this trend, and RAGate-PEFT aligns less closely but still captures early-turn emphasis.\\n- Contextual Insights: Specifically, we analyse the frequency of augmentation in different positions of conversations and different domains covered in the KETOD dataset. We use the RAGate-PEFT (contx-(syn-resp)-ner) with the highest precision and RAGate-MHA (MHA(contx)) with the best overall performance in the above analysis as representatives for comparison. Figure 3 presents the frequency in different positions. Due to the unequal number of conversational turns, we use the ratio to indicate the relative position. According to the reported results in Figure 3, most human augmentation selections happen at the beginning of a conversation. This trend is also effectively captured by both RAGate approaches, especially RAGate-MHA. This can be caused by the reason that a conversation is semantically coherent, and once sufficient additional information is provided at the early stage, the value of knowledge augmentation to the later turns is naturally lower.\\nOn the other hand, Figure 4 presents the augmentation frequency over different domains. We observe that system responses about certain domains are selected more often by humans than other domains, such as travel, hotels, trains, flights, service and rental cars, which require access to additional information to assist the suggestion-making, and the domains, like movies, music, media, events that often include entities require enriched description. By looking into the performance of RAGate-PEFT and RAGate-MHA, RAGate-MHA can make aligned selections for humans. However, the RAGate-PEFT does not guarantee the identification of appropriate augmentation use and often presents fewer augmentations, apart from the travel domain. Hence, by considering both position and domain augmentation frequency, we conclude that RAGate-MHA can outperform RAGate-MHA and effectively capture the trend of augmentation needs.\\nVisual Element: Figure 4\\n- Title/Caption: Figure 4: Frequency analysis of adaptive augmentations about dialogue domains.\\n- Detailed Description: A horizontal bar chart listing domains on the y-axis and the number of selected augmentations on the x-axis. Three series are shown: RAGate-PEFT, RAGate-MHA, and Human Labels. Domains include Messaging, RentalCars, RideSharing, Events, Buses, Media, Weather, Services, Payment, Music, Restaurants, Trains, Movies, Calendar, Flights, Hotels, Homes, Travel, Alarm.\\n- Key Information: Human labels show higher augmentation frequency for domains like Travel, Hotels, Trains, Flights, Services, and Rental Cars, and also for entity-rich domains such as Movies, Music, Media, Events. RAGate-MHA aligns more closely with human selections than RAGate-PEFT, which often presents fewer augmentations except in the travel domain.\\n- Contextual Insights: These frequency analyses support that augmentation need is higher early in conversations and in certain domains requiring additional knowledge or enriched entity description.'},\n",
       " {'content_title': 'Table 3: Performance of applying RAGate and compared to the KETOD baseline on the KETOD dataset',\n",
       "  'content': 'Variants # Augs BLEU ROUGE-L BERTScore Confidence\\nNo-Aug 0 9.38• 0.3780• 0.8105• 9.3425• –\\nAugment BERT Ranker Retrieved Knowledge\\nRAGate-PEFT 230 10.45 0.3825 0.8144 9.3374 -0.05%\\nRAGate-MHA 787 12.14 0.3882 0.8192 9.3083 -0.36%\\nRandom-Aug 230 9.53 0.3784 0.8110 9.2984 -0.47%\\nRandom-Aug 787 10.01 0.3795 0.8126 9.1877 -1.65%\\nHuman-label 631 11.66 0.3856 0.8176 9.2550 -0.93%\\nAug-All 4964 16.08 0.3927 0.8258 8.3677 -10.43%\\nAugment Rank-1 Relevant Knowledge\\nRAGate-Llama 230 10.54 0.3822 0.8142 9.3642 +0.23%\\nRAGate-MHA 787 11.99 0.3883 0.8191 9.3774 +0.37%\\nRandom-Aug 230 9.51 0.3784 0.8110 9.3328 -0.10%\\nRandom-Aug 787 10.01 0.3800 0.8127 9.2982 -0.47%\\nHuman-label 631 11.52 0.3846 0.8170 9.3218 -0.22%\\nAugment-All 4964 16.05 0.3944 0.8259 9.0655 -2.9%\\nTable 3: Performance of applying RAGate and compared to the KETOD baseline on the KETOD dataset. Confidence is calculated by the average value over the lowest logit of each generation.'},\n",
       " {'content_title': '5.3 RAGate for Response Generation',\n",
       "  'content': '5.3 RAGate for Response Generation\\nTo evaluate the effect of adaptive RAG for a conversational system, we use RAGate-PEFT (contx-(syn-resp)-ner) with the highest precision and RAGate-MHA (MHA(contx)) with the best overall performance in the above analysis, to support the adaptive retrieval augmented conversational response generation. Table 3 presents the results of applying RAGAte to the KETOD model for adaptive knowledge augmentation when evaluated on the KETOD dataset. We include four types of adaptive augmentation, namely the use of RAGate and comparison to the random selection with equal numbers of selections, human choice, and the commonly used \"all\" augmentation. In addition, to explore the effect of varied quality of knowledge snippets, we also extend the evaluation of using the top-3 knowledge snippets ranked by different retrievers (i.e., BERT-ranker and TF-IDF) and the use of knowledge snippets at the 1st and 5th rank according to the BERT-ranker. Due to the space limit, we first present the results of using BERT-ranker retrieved and top-1 relevant knowledge and top-1 relevant in Table 3 and show the full results in the Appendix C.\\nAt first, without adaptive knowledge augmentation, we compare the choice of response generation without augmentation and with \"always\" augmentation (i.e., No-Aug versus Aug-All). In Table 3, we observe that by augmenting a total of 4,964 system responses in the test collection, the conversational model can generate more informative and effective responses according to the reported scores of BLEU, ROUGE and BERTscore. This aligns with the reported effectiveness of RAG in many existing studies. However, we also identify a significant drop in the model’s generation confidence level. As denoted by Varshney et al. (2023), a lower confidence level can correlate with a higher chance of generating hallucinated responses, which could be caused by the unnecessary use of external knowledge. Hence, to investigate the effectiveness of adaptive knowledge augmentation, we examine the impact of using RAGate. According to the reported experimental results in Table 3, the adaptive augmented response generation with fewer knowledge snippets can indeed result in a higher confidence level than Aug-All.\\nMoreover, comparing the performance between RAGate and random selections shows that, considering equal numbers (230 or 787 according to the classification with RAGate) of system responses for augmentation, RAGate can further result in a higher quality of generated response. RAGate-MHA even enables results that are comparable to Aug-All’s response quality, with only 787 turn augmentations instead of all 4964 turns. Specifically, the use of RAGate-PEFT, which identifies 230 turns of system responses for knowledge augmentation, can even outperform the random baseline that augments 787 system response turns with improved response quality. Apart from the improved response quality, RAGate also enables the conversational model to maintain a high confidence level and ensure faithful responses. Indeed, using RAGate-MHA, which augments 787 system responses, only lowers the average confidence score by 0.36%, instead of the 1.65% when randomly selecting an equal number of turns to augment.\\nIn addition, considering the use of different quality and amount of knowledge snippets for augmentation, we also include the use of the most relevant knowledge snippet according to BERT-ranker in Table 3. We observe that the use of different amounts of knowledge snippets in different relevance levels has a marginal effect on this learned dialogue system. However, we observe a significant difference in the confidence level. We observe that using only the most relevant knowledge snippet enables the Aug-All to suffer less from a lower confidence level. In particular, the application of RAGate can even increase the confidence level of the conversation system in response generation. This indicates that the confidence score can also correlate with the quality of the augmented knowledge snippets. This observation is further validated using knowledge snippets with fifth-ranking positions by BERT-ranker and the use of TF-IDF ranker. We include the full experimental results in Table 4 and attached in the Appendix. These observations indicate the value of adaptive system response augmentation via RAGate in generating high-quality outputs, ensuring faithful responses, and potentially saving retrieval costs. We also show the value of using confidence scores to reflect the contribution of RAG.'},\n",
       " {'content_title': '6 Conclusions',\n",
       "  'content': '6 Conclusions\\nOur study investigates a core research question about whether retrieval-augmented generation is always useful to a conversational system. To answer this research question, we propose adaptive retrieval-augmented generation for conversational systems and introduce corresponding gate functions, RAGate, for explicit control. A comprehensive set of experiments and results show the RAGate approaches can effectively identify augmentation needs. In addition, RAGate can capture human preference by augmenting the beginning turns of conversations, and RAGate can further identify knowledge augmentation for assisting suggestion-making and enriching description. When applying RAGate to conversational systems, we observe that it can ensure comparable quality of generated responses and enable the system to increase generation confidence for faithful outputs, especially with the appropriate use of relevant knowledge snippets.'},\n",
       " {'content_title': 'Limitations',\n",
       "  'content': 'Limitations\\nThere are three limitations of this study. At first, due to the main focus of examining the adaptive retrieval-augmented generation for a conversation system. We only consider a few examples of retrieval techniques (TF-IDF and BERT-ranker), which can be further extended to recent retrieval techniques, such as dense passage retrieval for additional insights. The second limitation is the missing use of larger language models, such as GPT-4, due to the shortage of computational resources. Including larger language models for conversational systems could introduce additional experimental insights. The third limitation is the shortage of appropriate conversational data for extensive evaluations. This is mainly caused by the recent development of the retrieval augmented generation technique and its application to conversational systems. Future research is encouraged to address this limitation.'},\n",
       " {'content_title': 'Ethics Statement',\n",
       "  'content': 'Ethics Statement\\nAll experiments in this study were conducted using publicly available datasets and open-released language models, which do not contain any private information that could raise ethical concerns.'},\n",
       " {'content_title': 'References',\n",
       "  'content': 'References\\nSimran Arora, Avanika Narayan, Mayee F Chen, Laurel Orr, Neel Guha, Kush Bhatia, Ines Chami, and Christopher Re. 2022. Ask me anything: A simple strategy for prompting language models. In The Eleventh International Conference on Learning Representations.\\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. 2016. Layer normalization. arXiv preprint arXiv:1607.06450.\\nZhiyu Chen, Bing Liu, Seungwhan Moon, Chinnadhurai Sankar, Paul A Crook, and William Yang Wang. 2022. Ketod: Knowledge-enriched task-oriented dialogue. In Findings of the Association for Computational Linguistics: NAACL 2022, pages 2581–2593.\\nTim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. 2024. Qlora: Efficient finetuning of quantized llms. Advances in Neural Information Processing Systems, 36.\\nEmily Dinan, Stephen Roller, Kurt Shuster, Angela Fan, Michael Auli, and Jason Weston. 2018. Wizard of wikipedia: Knowledge-powered conversational agents. In International Conference on Learning Representations.\\nAlex Graves and Alex Graves. 2012. Long short-term memory. Supervised sequence labelling with recurrent neural networks, pages 37–45.\\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770–778.\\nTaesuk Hong, Junhee Cho, Haeun Yu, Youngjoong Ko, and Jungyun Seo. 2023. Knowledge-grounded dialogue modelling with dialogue-state tracking, domain tracking, and entity extraction. Computer Speech & Language, 78:101460.\\nEdward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. 2021. Lora: Low-rank adaptation of large language models. In International Conference on Learning Representations.\\nXinxian Huang, Huang He, Siqi Bao, Fan Wang, Hua Wu, and Haifeng Wang. 2021. Plato-kag: Unsupervised knowledge-grounded conversation via joint modeling. In Proc. of NLP4ConvAI.\\nDi Jin, Shuyang Gao, Seokhwan Kim, Yang Liu, and Dilek Hakkani-Tur. 2021. Towards zero and few-shot knowledge-seeking turn detection in task-orientated dialogue systems. In 3rd Workshop on Natural Language Processing for Conversational AI, NLP4ConvAI 2021, pages 281–288.\\nMinki Kang, Jin Myung Kwak, Jinheon Baek, and Sung Ju Hwang. 2023. Knowledge graph-augmented language models for knowledge-grounded dialogue generation. arXiv preprint arXiv:2305.18846.\\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense passage retrieval for open-domain question answering. In Proc. of EMNLP.\\nAtoosa Kasirzadeh and Iason Gabriel. 2023. In conversation with artificial intelligence: aligning language models with human values. Philosophy & Technology, 36(2):27.\\nSeokhwan Kim, Mihail Eric, Karthik Gopalakrishnan, Behnam Hedayatnia, Yang Liu, and Dilek Hakkani-Tur. 2020. Beyond domain apis: Task-oriented conversational modeling with unstructured knowledge access. In Proc. of SIGDIAL.\\nMojtaba Komeili, Kurt Shuster, and Jason Weston. 2022. Internet-augmented dialogue generation. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 8460–8478.\\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. 2020. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems, 33:9459–9474.\\nYu Li, Baolin Peng, Yelong Shen, Yi Mao, Lars Liden, Zhou Yu, and Jianfeng Gao. 2022. Knowledge-grounded dialogue generation with a unified knowledge representation. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 206–218.\\nRongzhong Lian, Min Xie, Fan Wang, Jinhua Peng, and Hua Wu. 2019. Learning to select knowledge for response generation in dialog systems. In Proc. of IJCAI.\\nSourab Mangrulkar, Sylvain Gugger, Lysandre Debut, Younes Belkada, Sayak Paul, and Benjamin Bossan. 2022. Peft: State-of-the-art parameter-efficient fine-tuning methods. https://github.com/huggingface/peft.\\nErik Miehling, Manish Nagireddy, Prasanna Sattigeri, Elizabeth M Daly, David Piorkowski, and John T Richards. 2024. Language models in dialogue: Conversational maxims for human-ai interactions. arXiv preprint arXiv:2403.15115.\\nJinjie Ni, Tom Young, Vlad Pandelea, Fuzhao Xue, and Erik Cambria. 2023. Recent advances in deep learning based dialogue systems: A systematic survey. Artificial intelligence review, 56(4):3055–3155.\\nYasumasa Onoe, Michael Zhang, Eunsol Choi, and Greg Durrett. 2022. Entity cloze by date: What lms know about unseen entities. In Proc. of NAACL.\\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9.\\nLiliang Ren, Kaige Xie, Lu Chen, and Kai Yu. 2018. Towards universal dialogue state tracking. In Proc. of EMNLP.\\nAlireza Salemi and Hamed Zamani. 2024. Evaluating retrieval quality in retrieval-augmented generation. Preprint, arXiv:2404.13781.\\nProcheta Sen, Xi Wang, Ruiqing Xu, and Emine Yilmaz. 2023. Task2kb: A public task-oriented knowledge base. In Proceedings of the AAAI Conference on Artificial Intelligence.\\nWeizhou Shen, Yingqi Gao, Canbin Huang, Fanqi Wan, Xiaojun Quan, and Wei Bi. 2023. Retrieval-generation alignment for end-to-end task-oriented dialogue system. arXiv preprint arXiv:2310.08877.\\nTianyuan Shi, Liangzhi Li, Zijian Lin, Tao Yang, Xiaojun Quan, and Qifan Wang. 2023. Dual-feedback knowledge retrieval for task-oriented dialogue systems. arXiv preprint arXiv:2310.14528.\\nKurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, and Jason Weston. 2021. Retrieval augmentation reduces hallucination in conversation. In Proc. of EMNLP.\\nNeeraj Varshney, Wenlin Yao, Hongming Zhang, Jianshu Chen, and Dong Yu. 2023. A stitch in time saves nine: Detecting and mitigating hallucinations of llms by validating low-confidence generation. arXiv preprint arXiv:2307.03987.\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. Advances in neural information processing systems, 30.\\nHongru Wang, Wenyu Huang, Yang Deng, Rui Wang, Zezhong Wang, Yufei Wang, Fei Mi, Jeff Z Pan, and Kam-Fai Wong. 2024. Unims-rag: A unified multi-source retrieval-augmented generation for personalized dialogue systems. arXiv preprint arXiv:2401.13256.\\nShiquan Yang, Rui Zhang, and Sarah Erfani. 2020. Graphdialog: Integrating graph knowledge into end-to-end task-oriented dialogue systems. In Proc. of EMNLP.\\nLinhao Ye, Zhikai Lei, Jianghao Yin, Qin Chen, Jie Zhou, and Liang He. 2024. Boosting conversational question answering with fine-grained retrieval-augmentation and self-check. arXiv preprint arXiv:2403.18243.\\nHao Yu, Aoran Gan, Kai Zhang, Shiwei Tong, Qi Liu, and Zhaofeng Liu. 2024. Evaluation of retrieval-augmented generation: A survey. Preprint, arXiv:2405.07437.\\nXueliang Zhao, Wei Wu, Can Xu, Chongyang Tao, Dongyan Zhao, and Rui Yan. 2020. Knowledge-grounded dialogue generation with pre-trained language models. In Proc. of EMNLP.\\nKangyan Zhou, Shrimai Prabhumoye, and Alan W Black. 2018. A dataset for document grounded conversations. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 708–713.'},\n",
       " {'content_title': 'Appendix A: Prompts for RAGate-Prompt',\n",
       "  'content': 'A Prompts for RAGate-Prompt\\nIn this section, we list the used prompts for the RAGate-Prompt gate mechanism.\\nZero-Shot Prompt:\\nBelow is an instruction that describes a task.\\nPlease respond with ‘True’ or ‘False’ only that appropriately completes the request.\\n### Instruction: Analyse the conversational context so far. Generate an appropriate response. Consider the invovled entites. Estimate if augmenting the response with external knowledge is helpful with an output of ‘True’ or ‘False’ only.\\n### Input: [Converstion Context Input]\\n### Response:\\nIn-Context Learning Prompt:'},\n",
       " {'content_title': 'Table 4: Performance of applying RAGate and compared to KETOD on the SGD dataset',\n",
       "  'content': 'Augmentation Variants # Augs BLEU ROUGE-1 ROUGE-2 ROUGE-L BERTScore Confidence\\nNo-Aug 0 9.38 0.4111 0.2246 0.3780 0.8105 9.3425\\nAugment BERT Ranker Retrieved Knowledge\\nRAGate-Llama 230 10.45 0.4165 0.2273 0.3825 0.8144 9.3374\\nRAGate-MHA 787 12.14 0.4227 0.2318 0.3882 0.8192 9.3083\\nRandom-Aug 230 9.53 0.4119 0.2250 0.3784 0.8110 9.2984\\nRandom-Aug 787 10.01 0.4138 0.2265 0.3795 0.8126 9.1877\\nHuman-label 631 11.66 0.4198 0.2297 0.3856 0.8176 9.2550\\nAugment-All 4964 16.08 0.4301 0.2364 0.3927 0.8258 8.3677\\nAugment TF-IDF Ranker Retrieved Knowledge\\nRAGate-Llama 230 10.52 0.4165 0.2273 0.3826 0.8144 9.3418\\nRAGate-MHA 787 12.11 0.4233 0.2319 0.3889 0.8193 9.3058\\nRandom-Aug 230 9.47 0.4118 0.2251 0.3783 0.8110 9.3006\\nRandom-Aug 787 9.93 0.4136 0.2259 0.3793 0.8125 9.1942\\nHuman-label 631 11.60 0.4198 0.2293 0.3854 0.8175 9.2639\\nAugment-All 4964 15.76 0.4289 0.2345 0.3914 0.8256 8.4188\\nAugment Rank-1 Relevant Knowledge\\nRAGate-Llama 230 10.54 0.4162 0.2271 0.3822 0.8142 9.3642\\nRAGate-MHA 787 11.99 0.4227 0.2316 0.3883 0.8191 9.3774\\nRandom-Aug 230 9.51 0.4117 0.2250 0.3784 0.8110 9.3328\\nRandom-Aug 787 10.01 0.4140 0.2267 0.3800 0.8127 9.2982\\nHuman-label 631 11.52 0.4189 0.2289 0.3846 0.8170 9.3218\\nAugment-All 4964 16.05 0.4308 0.2365 0.3944 0.8259 9.0655\\nAugment Rank-5 Relevant Knowledge\\nRAGate-Llama 230 10.47 0.4161 0.2272 0.3823 0.8142 9.3592\\nRAGate-MHA 787 12.18 0.4224 0.2314 0.3883 0.8192 9.3704\\nRandom-Aug 230 9.52 0.4118 0.2252 0.3785 0.8110 9.3315\\nRandom-Aug 787 10.01 0.4135 0.2263 0.3794 0.8127 9.2961\\nHuman-label 631 11.58 0.4186 0.2287 0.3845 0.8170 9.3210\\nAugment-All 4964 15.97 0.4290 0.2349 0.3927 0.8256 9.0604\\nTable 4: Performance of applying RAGate and compared to KETOD on the SGD dataset. Confidence is calculated by the average value over the lowest logit of each generation.\\nVisual element analysis:\\n- Title/Caption: Table 4: Performance of applying RAGate and compared to KETOD on the SGD dataset. Confidence is calculated by the average value over the lowest logit of each generation.\\n- Detailed Description: A table comparing multiple augmentation strategies and models on the SGD dataset across metrics BLEU, ROUGE-1, ROUGE-2, ROUGE-L, BERTScore, and Confidence. Variants include No-Aug, RAGate-Llama, RAGate-MHA, Random-Aug, Human-label, and Augment-All, evaluated under different retrieval sources: BERT Ranker Retrieved Knowledge, TF-IDF Ranker Retrieved Knowledge, and specific rank positions (Rank-1 and Rank-5 Relevant Knowledge). Each row lists the number of augmentations (# Augs) and performance metrics.\\n- Key Information: Augment-All achieves the highest BLEU and ROUGE variants but shows notably lower Confidence values compared to fewer, more targeted augmentations. RAGate-MHA consistently outperforms Random-Aug across settings. Confidence tends to decrease when using a very large number of augmentations (e.g., Augment-All), indicating a trade-off between coverage and model certainty. No-Aug has the highest Confidence among non-augmented baselines but lower generation quality metrics.\\n- Contextual Insights: The surrounding text later discusses that using a suitable amount of relevant knowledge can improve confidence, aligning with observations in the table that targeted augmentation (e.g., RAGate) yields balanced quality and confidence compared to exhaustive augmentation.'},\n",
       " {'content_title': 'Below is an instruction that describes a task. Please respond with ‘True’ or ‘False’ only that appropriately completes the request.',\n",
       "  'content': '### Instruction: Analyse the conversational con\\ufffetext so far. Generate an appropriate response. Con\\ufffesider the invovled entites. Estimate if augmenting the response with external knowledge is helpful with an output of ‘True’ or ‘False’ only.\\n### Example 1: USER: I’m planning a trip, can you help me look for a flight? SYSTEM: Which day are you planning to return and from which city? USER: I want to go from NYC the day after tomorrow and return on the 13th of this month. SYSTEM: Where would you like to go? USER: I want to go to Vancouver, BC. Can you look for a Premium Economy class ticket. SYSTEM: I found 1 flight for you. It is a Delta Airlines flight that takes off at 6 am and returns at 2:50 am. The price is $505. USER: What is the departure airport, and how many stops does the flight have?\\n### Response: True\\n### Example 2: USER: Get me bus tickets to a Cher event on March 6th. SYSTEM: How many to buy? USER: only one, please.\\n### Response: False\\n### Input: [Converstion Context Input]\\n### Response:'},\n",
       " {'content_title': 'B Impact of Retrieval Quality on Adaptive RAG',\n",
       "  'content': 'To have a successful conversation model with a retrieval-augmented system, two main criteria must be met. One is identifying insufficient context, and the other is the quality of retrieved information (Salemi and Zamani, 2024; Yu et al., 2024). A conversational model performs better when both criteria are satisfied. In our proposed approach, as shown in Table 2, we have already assessed whether our adaptive retrieval method can detect insufficient context. We further explored to deter\\ufffemine whether our model can inherently estimate the quality of the retrieved snippets to address such insufficiency and, based on that, decide on the re\\ufffetrieval. Although we do not explicitly provide re\\ufffetrieved snippets to our model, retrieval comes with a corpus that includes potentially relevant knowl\\ufffeedge snippets. Consequently, given a query and a retrieval collection, it can be estimated whether useful information for the query exists in the corpus to address the insufficient context. To investigate by following this direction, we randomly selected 50 samples from instances where our proposed ap\\ufffeproach (RAGate-MHA, the best-performing gate model) predicted using retrieval augmentation. We asked domain experts (co-authors) to score whether they thought the retrieved snippets in those scenar\\ufffeios could be useful to response generation. Users rated the snippets on a scale of 0 − 4, with scores of 3 or 4 indicating ‘useful’ or ‘highly useful’. We found that in 54% of cases where the prediction was for augmentation, users also found the snippets useful. This indicates that our proposed approach can implicitly capture the potential for obtaining high-quality retrieval snippets.'},\n",
       " {'content_title': 'C Additional experimental results about RAGate for Response Generation',\n",
       "  'content': 'In Table 4, we include the complete experimental results of applying RAGate for adaptive retrieval\\ufffeaugmented system response generation. Specif\\ufffeically, explore the use of retrieved knowledge snippets to different extents of relevance. We in\\ufffeclude top-3 knowledge snippets retrieved by BERT\\uffferanker and TF-IDF. In addition, we also explore the use of knowledge snippets in different ranking positions (rank 1 and 5) according to the BERT\\uffferanker retriever. The experimental result shows that precisely using a suitable amount of relevant knowledge can generate a response with higher confidence (i.e., less is more). In addition, this observation also indicates the potential use of con\\ufffefidence levels to evaluate the quality of the aug\\ufffemented knowledge.\\nVisual element references: This section references Table 4 described earlier and interprets its results, emphasizing that targeted, relevant augmentation improves confidence and that confidence could serve as a signal for augmented knowledge quality.'}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_results['2407.21712v1.pdf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1vxh1DRs64lB",
    "outputId": "7f5d1908-01c4-4160-d4c3-3651716b8515"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_results['2407.21712v1.pdf'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "RbsEyZXJ7q3Z"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
