{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "srKM2rZiILNI"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/towardsai/ai-tutor-rag-system/blob/main/notebooks/Advanced_Retriever.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UwtfgR2TAiLM"
   },
   "source": [
    "## Install Packages and Setup Variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "sbO5PUR3AL-i",
    "outputId": "2557cec9-f07e-4b9d-9aec-43648ce3bc09"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.8/19.8 MB\u001b[0m \u001b[31m76.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m69.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m72.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.9/11.9 MB\u001b[0m \u001b[31m73.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m303.3/303.3 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.3/103.3 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m62.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.9/131.9 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.7/65.7 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m208.0/208.0 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.8/510.8 kB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.1/56.1 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.9/63.9 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.5/310.5 kB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m81.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m452.2/452.2 kB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.4/144.4 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "!pip install -q llama-index==0.14.0 llama-index-vector-stores-chroma==0.5.3 llama-index-llms-google-genai==0.5.0 \\\n",
    "                chromadb==1.0.21 llama-index-llms-openai==0.5.6 jedi==0.19.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "39OAU5OlByI0"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set the following API Keys in the Python environment. Will be used later.\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"[OPENAI_API_KEY]\"\n",
    "# os.environ[\"GOOGLE_API_KEY\"] = \"<YOUR_API_KEY>\"\n",
    "\n",
    "from google.colab import userdata\n",
    "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
    "os.environ[\"GOOGLE_API_KEY\"] = userdata.get('Google_api_key')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "crwd5lxZorYp"
   },
   "outputs": [],
   "source": [
    "# Allows running asyncio in environments with an existing event loop, like Jupyter notebooks.\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1rJCtcjwNIWg"
   },
   "source": [
    "## Language Model and Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q_2p3SxfNHi7"
   },
   "outputs": [],
   "source": [
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core import Settings\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.llms.google_genai import GoogleGenAI\n",
    "\n",
    "# Settings.llm = OpenAI(model=\"gpt-5-mini\")\n",
    "\n",
    "Settings.llm = GoogleGenAI(model=\"gemini-2.5-flash\", temperature=1)\n",
    "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KGzUXs_zWdKp"
   },
   "source": [
    "**Note: You can create a vector store from scratch using the code below, or you can load it from Hugging Face using the code provided in this notebook.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B2UvE-i9Nzon"
   },
   "source": [
    "# Create a Vector Store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O2haexSAByDD"
   },
   "outputs": [],
   "source": [
    "import chromadb\n",
    "\n",
    "# create client and a new collection\n",
    "# chromadb.EphemeralClient saves data in-memory.\n",
    "chroma_client = chromadb.PersistentClient(path=\"./ai_tutor_knowledge\")\n",
    "chroma_collection = chroma_client.create_collection(\"ai_tutor_knowledge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OHO6a-zaBxeG"
   },
   "outputs": [],
   "source": [
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core.storage.storage_context import StorageContext\n",
    "\n",
    "# Define a storage context object using the created vector database.\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hZz9_ZYNN4Kv"
   },
   "source": [
    "# Load the Dataset (JSON)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uvOjzNNAN4wg"
   },
   "source": [
    "## Download\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "488bdd920d2d4fc3a0975b2c6d0591f4",
      "62016715634f4fc695ebfa8a0ce13694",
      "401791117a4444e7b55ae7f515d79338",
      "10129ab9cd654896b6ab12b1bcb6fd6a",
      "651db8f7655744f9b31778a9b672e318",
      "e72f487941cf435ca2b922e5b56e3a97",
      "6508e1f7c3e6410cbb79d6c635ac00d3",
      "a3494acd8a0f463c965387938c81c4e9",
      "d90a56b524084b85908033a344946c84",
      "e8a75c57015e4adca30029683575873b",
      "a3ee9ea8860d46039e75c786775caad0"
     ]
    },
    "id": "x4llz2lHN2ij",
    "outputId": "5b3599d2-48e5-4cdb-f48d-8f7b018c1040"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "488bdd920d2d4fc3a0975b2c6d0591f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ai_tutor_knowledge.jsonl: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "file_path = hf_hub_download(repo_id=\"jaiganesan/ai_tutor_knowledge\", filename=\"ai_tutor_knowledge.jsonl\",repo_type=\"dataset\",local_dir=\"/content\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V-ezlgFaN-5u"
   },
   "source": [
    "# Read File\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_M-0-D4fN2fc",
    "outputId": "5a06f2f2-b4b5-49f7-b2fe-764a6cb4541f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "762"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "with open(file_path, \"r\") as file:\n",
    "    ai_tutor_knowledge = [json.loads(line) for line in file]\n",
    "\n",
    "len(ai_tutor_knowledge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PBimOJVwOCjl"
   },
   "source": [
    "# Convert to Document obj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ie--Y_3wN2c8"
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from llama_index.core import Document\n",
    "\n",
    "def create_docs_from_list(data_list: List[dict]) -> List[Document]:\n",
    "    documents = []\n",
    "    for data in data_list:\n",
    "        documents.append(\n",
    "            Document(\n",
    "                doc_id=data[\"doc_id\"],\n",
    "                text=data[\"content\"],\n",
    "                metadata={  # type: ignore\n",
    "                    \"url\": data[\"url\"],\n",
    "                    \"title\": data[\"name\"],\n",
    "                    \"tokens\": data[\"tokens\"],\n",
    "                    \"source\": data[\"source\"],\n",
    "                },\n",
    "                excluded_llm_metadata_keys=[\n",
    "                    \"title\",\n",
    "                    \"tokens\",\n",
    "                    \"source\",\n",
    "                ],\n",
    "                excluded_embed_metadata_keys=[\n",
    "                    \"url\",\n",
    "                    \"tokens\",\n",
    "                    \"source\",\n",
    "                ],\n",
    "            )\n",
    "        )\n",
    "    return documents\n",
    "\n",
    "doc = create_docs_from_list(ai_tutor_knowledge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lqQpen6bOEza"
   },
   "source": [
    "# Transforming\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zVBkAg6eN2an"
   },
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceWindowNodeParser\n",
    "\n",
    "# create the sentence window node parser\n",
    "node_parser = SentenceWindowNodeParser.from_defaults(\n",
    "    window_size=3,\n",
    "    include_metadata=True,\n",
    "    window_metadata_key=\"window\",\n",
    "    original_text_metadata_key=\"original_text\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KiDwIXFxN2YK"
   },
   "outputs": [],
   "source": [
    "documents = [i for i in doc if i.metadata['tokens']<8000]\n",
    "nodes = node_parser.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vGfVgqs2REzA",
    "outputId": "e4b91460-6e9b-46db-8499-2521709ae183"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22083"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f1aZ4wYVN2V1",
    "outputId": "f83dd680-4dc8-43b4-dfff-98a86397453c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextNode(id_='c6a654d8-4f46-4ca7-b526-e4b19c9dbc73', embedding=None, metadata={'url': 'https://towardsai.net/p/machine-learning/bert-huggingface-model-deployment-using-kubernetes-github-repo-03-07-2024', 'title': 'BERT HuggingFace Model Deployment using Kubernetes [ Github Repo]  03/07/2024', 'tokens': 768, 'source': 'tai_blog', 'window': 'Github Repo : https://github.com/vaibhawkhemka/ML-Umbrella/tree/main/MLops/Model_Deployment/Bert_Kubernetes_deployment   Model development is useless if you dont deploy it to production  which comes with a lot of issues of scalability and portability.    I have deployed a basic BERT model from the huggingface transformer on Kubernetes with the help of docker  which will give a feel of how to deploy and manage pods on production.    Model Serving and Deployment:ML Pipeline:Workflow:   Model server (using FastAPI  uvicorn) for BERT uncased model    Containerize model and inference scripts to create a docker image    Kubernetes deployment for these model servers (for scalability)  Testing   Components:Model serverUsed BERT uncased model from hugging face for prediction of next word [MASK].  Inference is done using transformer-cli which uses fastapi and uvicorn to serve the model endpoints   Server streaming:   Testing: (fastapi docs)   http://localhost:8888/docs/   { output: [ { score: 0.21721847355365753  token: 2204  token_str: good  sequence: today is a good day }  { score: 0.16623663902282715  token: 2047  token_str: new  sequence: today is a new day }  { score: 0.07342924177646637  token: 2307  token_str: great  sequence: today is a great day }  { score: 0.0656224861741066  token: 2502  token_str: big  sequence: today is a big day }  { score: 0.03518620505928993  token: 3376  token_str: beautiful  sequence: today is a beautiful day } ]   ContainerizationCreated a docker image from huggingface GPU base image and pushed to dockerhub after testing.   ', 'original_text': 'Github Repo : https://github.com/vaibhawkhemka/ML-Umbrella/tree/main/MLops/Model_Deployment/Bert_Kubernetes_deployment   Model development is useless if you dont deploy it to production  which comes with a lot of issues of scalability and portability.   '}, excluded_embed_metadata_keys=['url', 'tokens', 'source', 'window', 'original_text'], excluded_llm_metadata_keys=['title', 'tokens', 'source', 'window', 'original_text'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='d72b4670-84f0-54a3-b259-7eb7f218674e', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'url': 'https://towardsai.net/p/machine-learning/bert-huggingface-model-deployment-using-kubernetes-github-repo-03-07-2024', 'title': 'BERT HuggingFace Model Deployment using Kubernetes [ Github Repo]  03/07/2024', 'tokens': 768, 'source': 'tai_blog'}, hash='1d5c6d1afc72f8d7992482801ef9f690275cf327c9859462851524cb4242ab49'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='ad2309cb-d170-4b27-acca-6df4dcd6b101', node_type=<ObjectType.TEXT: '1'>, metadata={'window': 'Github Repo : https://github.com/vaibhawkhemka/ML-Umbrella/tree/main/MLops/Model_Deployment/Bert_Kubernetes_deployment   Model development is useless if you dont deploy it to production  which comes with a lot of issues of scalability and portability.    I have deployed a basic BERT model from the huggingface transformer on Kubernetes with the help of docker  which will give a feel of how to deploy and manage pods on production.    Model Serving and Deployment:ML Pipeline:Workflow:   Model server (using FastAPI  uvicorn) for BERT uncased model    Containerize model and inference scripts to create a docker image    Kubernetes deployment for these model servers (for scalability)  Testing   Components:Model serverUsed BERT uncased model from hugging face for prediction of next word [MASK].  Inference is done using transformer-cli which uses fastapi and uvicorn to serve the model endpoints   Server streaming:   Testing: (fastapi docs)   http://localhost:8888/docs/   { output: [ { score: 0.21721847355365753  token: 2204  token_str: good  sequence: today is a good day }  { score: 0.16623663902282715  token: 2047  token_str: new  sequence: today is a new day }  { score: 0.07342924177646637  token: 2307  token_str: great  sequence: today is a great day }  { score: 0.0656224861741066  token: 2502  token_str: big  sequence: today is a big day }  { score: 0.03518620505928993  token: 3376  token_str: beautiful  sequence: today is a beautiful day } ]   ContainerizationCreated a docker image from huggingface GPU base image and pushed to dockerhub after testing.    Testing on docker container:   You can directly pull the image vaibhaw06/bert-kubernetes:latest   K8s deploymentUsed minikube and kubectl commands to create a single pod container for serving the model by configuring deployment and service config   deployment.yaml   apiVersion: apps/v1 kind: Deployment metadata: name: bert-deployment labels: app: bertapp spec: replicas: 1 selector: matchLabels: app: bertapp template: metadata: labels: app: bertapp spec: containers: - name: bertapp image: vaibhaw06/bert-kubernetes ports: - containerPort: 8080 --- apiVersion: v1 kind: Service metadata: name: bert-service spec: type: NodePort selector: app: bertapp ports: - protocol: TCP port: 8080 targetPort: 8080 nodePort: 30100Setting up minikube and running pods using kubectl and deployment.yaml   minikube start kubectl apply -f deployment.yamlFinal Testing:kubectl get allIt took around 15 mins to pull and create container pods.   ', 'original_text': 'I have deployed a basic BERT model from the huggingface transformer on Kubernetes with the help of docker  which will give a feel of how to deploy and manage pods on production.   '}, hash='15c4a83e59e2afad028542bcad93ba564295cd7432e63c7071baada9ecbdb7a6')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Github Repo : https://github.com/vaibhawkhemka/ML-Umbrella/tree/main/MLops/Model_Deployment/Bert_Kubernetes_deployment   Model development is useless if you dont deploy it to production  which comes with a lot of issues of scalability and portability.   ', mimetype='text/plain', start_char_idx=0, end_char_idx=254, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fPvSbtLUEzuE"
   },
   "source": [
    "## Index creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 369,
     "referenced_widgets": [
      "6e86f69ec24a4c0d9c675c1ddc4106ac",
      "c87b2830fd14473ea88d0a0e971e8050",
      "8c169751168644bf9494e75047561e97",
      "76078fe1af3a46aeb4c547409645d8e8",
      "c5cb9ad73616490580f4d29765fb8c82",
      "ff22cd743662412193aa6a0f9082d6e2",
      "7738c9f6005a41bb8e4e52e0462b6807",
      "4789255fa13a4be885a0307ccbbfb7a4",
      "000907913c8742949ce142a22ff37b9b",
      "c5057800bd36499cbb85c938daae0166",
      "456bac006ee1431a8bb2d3e6dbe92424",
      "f319380627984503aa456922d107d94e",
      "ba06ecff1d3f43bd9dee891f35d489ff",
      "0ebadb71dbc046beacb1e9a11e8c473f",
      "95dadc85239a460dba8adbe5a46ac0ff",
      "4973a07812a74d5fa90514716a3d91c9",
      "2e4c3bfd83f4467d90f9ee681d690604",
      "34d216f35ecd431e8edcca6ed21631a7",
      "12b5bb05b0ee413c88b0c223a8896be6",
      "b365ad38dc2646848df63a8cf9e1c0e1",
      "2d1a2b58e9ca4584891776e73714b0ca",
      "bac2515431664225be044d890ab6315e",
      "a989d81954d442b9a3b116d251efcb43",
      "a87128008c7f4455b889b17a2cc3cec1",
      "472e0c4c6a7e478fa96d544ad1461be9",
      "e8d2f8b73f514d909754f550e3032c20",
      "ed9553fb7ac4416f9972e87bc76d46b6",
      "db8b96d366f04ce08610be1eb58ca15e",
      "3181113124a34cf4bd0113b79c7c8b2d",
      "9f3cd12a396e4a29aef75ad0b1189e57",
      "9c17525cce4f410ab5fd7ae64b07a039",
      "dc1df1faffb5419a87684a5f49634bb3",
      "3bc4b3b803b8400c83721a422073c4a4",
      "e34a240047a142399c6594d7772efbd5",
      "42033905768542fdb8f512adc9ff16bd",
      "bd552a48414e4ab79dd1041d2c214259",
      "eace0976fc144a95b17c47008570f66b",
      "c105ca5048224fbca24576bba29ddf8f",
      "32fb8400f9d6475b86f529af46812c40",
      "f56c97d723634f3c81323ffb22afb34a",
      "cbcb32161d02472fbbb2008eb9279d4a",
      "050a06d1ab1246eda83115ba59061422",
      "922170e722a949e2ae2bcf5335206700",
      "e2a39c384b7d493f93c6f6d90b53961a",
      "e3e721bd0faf477fbee63d93956ff29e",
      "8cab4e1c2b674fc9bce124774b2fa888",
      "fb17c54a4de34ecb83bbb5e7be22a766",
      "725e465db1144f3c8ad8860e880ce320",
      "ae1c0adcb763439f81a922273759854e",
      "5b4c072bc93d4f2098181f8670a43d99",
      "c67440ca87164ae58c101838b225a5ad",
      "ac7d508afcd144bf81c3e6219abbfe7d",
      "4c54972197d04b59aec02efc818fa0b6",
      "b7a1d7d50202426e91ea1dac9d745a90",
      "ca5b34fc48344cde9d326a6a6cd1b710",
      "9c90d5c13e0c490795460fe9add84f20",
      "29ae1f79703440de962c8656a92712e7",
      "3a043576ee6f4ee5a48946e225769ab8",
      "d07c10bdb78a47dbb4f72a29f03b6e82",
      "1b264037fc9541c5aebf9997fef511b5",
      "ad63b4d67fb74320aefbd3de5b01966e",
      "1772b3d081444fa88bfcbb42cba89e94",
      "d03fa516ba894c59bafafcfbb654efa3",
      "0f154ab69557496da07c0caea823d534",
      "cbc6b9fbe28f46aaafe278c807313bfd",
      "079a3fb7bef14a7a968601763c361ea2",
      "93568961aec74972b9507656ff2a4c87",
      "0bd072c1f6554ab3be80eb6fd4479e92",
      "c4c51987a2d947efbc1c40b349e5cbe8",
      "c6705b3e9b6b439081c42938d3c5a116",
      "a2f9e588f91b4a7780b97a15c5fc6aaa",
      "67166e83dc244675a3be9cfe8f8c9923",
      "74f9df16b2814d9c90c689c4a8346403",
      "fd1ed0bc9fc7437cabf2a2486280373f",
      "29e8719d74dc4bdea8944adfdeb95a06",
      "fa30fded88c74795be5aef5788bd1e78",
      "3f122dc6e8ab4077b11a6387b80d5793",
      "f8c4b477f0db483380b841cdf7754a29",
      "e2fa1e498c664bb4b65264bd7b2bdbb7",
      "f0f8a517729e46e9a931464d92be5ed5",
      "35b2e31c9a5d49f7bfc1afe2333256f8",
      "c102e7fcd1b24069bf0b25e41831c5b1",
      "f0890e15383c41509bd1a38a78212712",
      "728cf541cb254f9088aa496b5dbec971",
      "aebef7603ceb455e8ad7f390b181790d",
      "b1bf295ad6864af1b0fbecd213a3cc1b",
      "3a14c0989edd4e2aa1c8fde852a52472",
      "00a6742d9f3f4c42b47d7db3cadc00a3",
      "89eba0024afc4a63a37a0a453494627a",
      "76d7ed8eb6d64d93851c70b92ecc56e5",
      "f8e020ed9b034bce819b0ad43716bce8",
      "72e61abddfba4fdf8d504319709492c0",
      "0fe45f7ba48e442e9c3a32ae1b346a09",
      "627424952aea4be28669bd4dce270607",
      "28cf7bca80be4c24a4ccf40d9b655047",
      "fdebdc2f17a04bbb982a357f32291654",
      "37836316cee64b27b658c092c7c538ca",
      "b7cdff7c2b384bea80007ecebc5138d5",
      "325c10622ae04b53ad8f4985c4d13ccd",
      "dc2808f9bf6749ff923bf5d115d3e455",
      "a99ff8696544491fab16f4f2861c6876",
      "a164e8fbaa6943ff816bdf9f6da20bb5",
      "dac6ebff0fb14a559e7eee4c6663820f",
      "e186d03a2cc340f298c63a92cdee30e8",
      "cfa4972e49b547f19536bb7eefa2da13",
      "7c8de834f3af409e983b4bd5cd5f6ffa",
      "ad02ee2d3e964e179dd2166c3f757a1c",
      "f6b3087d972e4b63a8dfe69f61e24a2d",
      "2a6f8f1372e548a58b94522cc1929519",
      "2459fa295b7b405eafa7f099bc9469a3",
      "0310456fabcd46fabbc779516b4413e4",
      "a5ff0086b23e42fab3f5e1d135782987",
      "d3f2f3edeffc49a0abfe392b7d1019c5",
      "29a3fb66d32b4b279aa69d3d2023954d",
      "9289c0d5a4b346c392c80a1d23bd037c",
      "f1e1ad593beb4416b2fb657847f18b45",
      "9a630e059aca4d5d92a3555fcfdca237",
      "bfcffbfbe10044bd9ab2df7737df97a4",
      "01d3eb64d6bc446a9f40cd6f3304e91c",
      "a7bcc66a60ce4def988d967d5692f56e",
      "ca65b4eeebc247278791e0b16a7f2885"
     ]
    },
    "id": "moNbizWrN2Tu",
    "outputId": "89c16693-9185-4c77-85dc-b251705be8db"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e86f69ec24a4c0d9c675c1ddc4106ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/2048 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f319380627984503aa456922d107d94e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/2048 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a989d81954d442b9a3b116d251efcb43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/2048 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e34a240047a142399c6594d7772efbd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/2048 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3e721bd0faf477fbee63d93956ff29e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/2048 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c90d5c13e0c490795460fe9add84f20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/2048 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93568961aec74972b9507656ff2a4c87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/2048 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8c4b477f0db483380b841cdf7754a29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/2048 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89eba0024afc4a63a37a0a453494627a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/2048 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc2808f9bf6749ff923bf5d115d3e455",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/2048 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0310456fabcd46fabbc779516b4413e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/1603 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "# Add the documents to the database and create Index / embeddings\n",
    "index = VectorStoreIndex(nodes, storage_context=storage_context,show_progress =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nz6dQtXzyWqK",
    "outputId": "b1d8b45d-8aac-4911-98f8-7762bb260b0d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: ai_tutor_knowledge/ (stored 0%)\n",
      "  adding: ai_tutor_knowledge/7b5f10b0-2d85-4217-ab0a-3f906f456884/ (stored 0%)\n",
      "  adding: ai_tutor_knowledge/7b5f10b0-2d85-4217-ab0a-3f906f456884/length.bin (deflated 73%)\n",
      "  adding: ai_tutor_knowledge/7b5f10b0-2d85-4217-ab0a-3f906f456884/header.bin (deflated 56%)\n",
      "  adding: ai_tutor_knowledge/7b5f10b0-2d85-4217-ab0a-3f906f456884/link_lists.bin (deflated 78%)\n",
      "  adding: ai_tutor_knowledge/7b5f10b0-2d85-4217-ab0a-3f906f456884/data_level0.bin (deflated 17%)\n",
      "  adding: ai_tutor_knowledge/7b5f10b0-2d85-4217-ab0a-3f906f456884/index_metadata.pickle (deflated 44%)\n",
      "  adding: ai_tutor_knowledge/chroma.sqlite3 (deflated 86%)\n"
     ]
    }
   ],
   "source": [
    "# Compress the vector store directory to a zip file to be able to download and use later.\n",
    "!zip -r vectorstore-windowed.zip ai_tutor_knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7qZY6xOYyjIX"
   },
   "source": [
    "# Load Indexes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e7hRxvh-SjtX"
   },
   "source": [
    "**Note: If you didn’t create the vector store from scratch, please uncomment the three code blocks/cells below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "1ppFu822FMSB"
   },
   "outputs": [],
   "source": [
    "# from huggingface_hub import hf_hub_download\n",
    "# vectorstore = hf_hub_download(repo_id=\"jaiganesan/ai_tutor_knowledge\", filename=\"vectorstore-windowed.zip\",repo_type=\"dataset\",local_dir=\"/content\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "wS-V6NhMymx8"
   },
   "outputs": [],
   "source": [
    "# !unzip vectorstore-windowed.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "zlJoIovXWKp6"
   },
   "outputs": [],
   "source": [
    "# import chromadb\n",
    "# from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "# from llama_index.core import VectorStoreIndex\n",
    "\n",
    "# # Create your index\n",
    "# db = chromadb.PersistentClient(path=\"./ai_tutor_knowledge\")\n",
    "# chroma_collection = db.get_or_create_collection(\"ai_tutor_knowledge\")\n",
    "# vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "\n",
    "# # Create your index\n",
    "# index = VectorStoreIndex.from_vector_store(vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fH2myF120oMi"
   },
   "outputs": [],
   "source": [
    "from llama_index.core.postprocessor import MetadataReplacementPostProcessor\n",
    "\n",
    "query_engine = index.as_query_engine(\n",
    "    llm=Settings.llm,\n",
    "    # the target key defaults to `window` to match the node_parser's default\n",
    "    node_postprocessors=[\n",
    "        MetadataReplacementPostProcessor(target_metadata_key=\"window\")\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EqNreFmE0vRb",
    "outputId": "a6430100-8d09-4b61-ac09-6ac772ef4b27"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:llama_index.llms.google_genai.utils:Retrying llama_index.llms.google_genai.base.GoogleGenAI._chat in 0.4822043481798398 seconds as it raised ServerError: 503 UNAVAILABLE. {'error': {'code': 503, 'message': 'The model is overloaded. Please try again later.', 'status': 'UNAVAILABLE'}}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The provided context does not explicitly define \"naive RAG.\" However, it refers to \"standard RAG approaches\" and \"the most competitive standard RAG model, Mistral-Instruc^{as}_{7B},\" which Speculative RAG is compared against and outperforms. These standard approaches likely represent a baseline RAG without the advanced features of iterative LLM refinement or self-critique mentioned as \"recent RAG advancements.\"\n",
      "\n",
      "SPECULATIVE RAG is a framework designed to accelerate and improve Retrieval Augmented Generation (RAG). It leverages a larger generalist Language Model (LM) to efficiently verify multiple RAG drafts. These drafts are produced in parallel by a smaller, distilled specialist LM. Each draft is generated from a distinct subset of retrieved documents, which offers diverse perspectives on the evidence and helps reduce input token counts per draft. This approach enhances comprehension of each subset and mitigates potential position bias over long contexts. The method accelerates RAG by delegating the drafting task to the smaller specialist LM, with the larger generalist LM performing a single verification pass over the drafts.\n",
      "\n",
      "SPECULATIVE RAG consistently outperforms all baselines, including standard RAG approaches like Mistral-Instruc^{as}_{7B}, across various benchmarks such as TriviaQA, MusiQue, PubHealth, and ARC-Challenge. For example, the M_{Verifier-13B/7B} + M_{Draft+7B} configuration surpassed Mistral-Instruc^{as}_{7B} by notable percentages (e.g., 2.15% on MusiQue, 12.97% on PubHealth).\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"Write about naive RAG and Speculative RAG?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "whdPLhVaMfOS",
    "outputId": "4016aff5-736f-4ae0-ea6d-d6e7cd1d1048"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source  1\n",
      "Original Text: In this work, we introduce SPECULATIVE RAG – a framework that leverages a larger generalist LM to efficiently verify multiple RAG drafts produced in parallel by a smaller, distilled specialist LM. \n",
      "Window: **Abstract**Retrieval augmented generation (RAG) combines the generative abilities of large language models (LLMs) with external knowledge sources to provide more accurate and up-to-date responses.  Recent RAG advancements focus on improving retrieval outcomes through iterative LLM refinement or self-critique capabilities acquired through additional instruction tuning of LLMs.  In this work, we introduce SPECULATIVE RAG – a framework that leverages a larger generalist LM to efficiently verify multiple RAG drafts produced in parallel by a smaller, distilled specialist LM.  Each draft is generated from a distinct subset of retrieved documents, offering diverse perspectives on the evidence while reducing input token counts per draft.  This approach enhances comprehension of each subset and mitigates potential position bias over long context.  Our method accelerates RAG by delegating drafting to the smaller specialist LM, with the larger generalist LM performing a single verification pass over the drafts. \n",
      "----\n",
      "Source  2\n",
      "Original Text: 4.3 Main ResultsWe compare SPECULATIVE RAG with standard RAG approaches, as well as the more advanced Self-Reflective RAG and Corrective RAG on four datasets: TriviaQA, MusiQue, PubHealth, and ARC-Challenge. \n",
      "Window: 4.3 Main ResultsWe compare SPECULATIVE RAG with standard RAG approaches, as well as the more advanced Self-Reflective RAG and Corrective RAG on four datasets: TriviaQA, MusiQue, PubHealth, and ARC-Challenge.  We report the performance of M_{Draft+7B} when used alone or paired with the RAG verifier (e.g., M_{Verifier-7B}, M_{Verifier-13B/7B}).  Following prior work (Asai et al., 2023; Yan et al., 2024), we report accuracy as the performance metric.Superior Performance over BaselinesTable 1 demonstrates that SPECULATIVE RAG consistently outperforms all baselines across all four benchmarks.  Particularly, M_{Verifier-13B/7B} + M_{Draft+7B} surpasses the most competitive standard RAG model, Mistral-Instruc^{as}_{7B}, by 0.33% on TriviaQA, 2.15% on MusiQue, 12.97% on PubHealth, and 2.14% on ARC-Challenge. \n",
      "----\n"
     ]
    }
   ],
   "source": [
    "for idx, item in enumerate(response.source_nodes):\n",
    "    print(\"Source \", idx + 1)\n",
    "    print(\"Original Text:\", item.node.metadata[\"original_text\"])\n",
    "    print(\"Window:\", item.node.metadata[\"window\"])\n",
    "    print(\"----\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
