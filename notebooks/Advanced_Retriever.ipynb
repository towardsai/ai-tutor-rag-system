{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UwtfgR2TAiLM"
   },
   "source": [
    "# Install Packages and Setup Variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "sbO5PUR3AL-i",
    "outputId": "508171f4-83c6-441a-f11b-dbe8e5b7e0c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.7/150.7 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m455.6/455.6 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m602.1/602.1 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m39.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m679.1/679.1 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m38.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m52.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.4/40.4 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m62.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.5/52.5 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.7/149.7 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.0/64.0 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m67.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.6/79.6 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.5/264.5 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.3/302.3 kB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m43.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m60.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m452.6/452.6 kB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "!pip install -q llama-index==0.12.21 llama-index-vector-stores-chroma==0.4.1 llama-index-llms-gemini==0.4.1 google-generativeai==0.5.4 openai==1.59.8 chromadb==0.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "39OAU5OlByI0"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set the following API Keys in the Python environment. Will be used later.\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"[OPENAI_API_KEY]\"\n",
    "# os.environ[\"GOOGLE_API_KEY\"] = \"<YOUR_API_KEY>\"\n",
    "\n",
    "from google.colab import userdata\n",
    "os.environ[\"OPENAI_API_KEY\"] = userdata.get('openai_api_key')\n",
    "os.environ[\"GOOGLE_API_KEY\"] = userdata.get('Google_api_key')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1rJCtcjwNIWg"
   },
   "source": [
    "## Language Model and Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Q_2p3SxfNHi7"
   },
   "outputs": [],
   "source": [
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.llms.gemini import Gemini\n",
    "from llama_index.core import Settings\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "# Settings.llm = OpenAI(temperature=0, model=\"gpt-4o-mini\")\n",
    "\n",
    "Settings.llm = Gemini(model=\"models/gemini-1.5-flash\", temperature=1, max_tokens=512)\n",
    "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KGzUXs_zWdKp"
   },
   "source": [
    "**Note: You can create a vector store from scratch using the code below, or you can load it from Hugging Face using the code provided in this notebook.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B2UvE-i9Nzon"
   },
   "source": [
    "# Create a Vector Store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "O2haexSAByDD"
   },
   "outputs": [],
   "source": [
    "import chromadb\n",
    "\n",
    "# create client and a new collection\n",
    "# chromadb.EphemeralClient saves data in-memory.\n",
    "chroma_client = chromadb.PersistentClient(path=\"./ai_tutor_knowledge\")\n",
    "chroma_collection = chroma_client.create_collection(\"ai_tutor_knowledge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "OHO6a-zaBxeG"
   },
   "outputs": [],
   "source": [
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core.storage.storage_context import StorageContext\n",
    "\n",
    "# Define a storage context object using the created vector database.\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hZz9_ZYNN4Kv"
   },
   "source": [
    "# Load the Dataset (JSON)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uvOjzNNAN4wg"
   },
   "source": [
    "## Download\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "6b38c10cd04243a6b2d98cad065f51e9",
      "48a04f24711044919a2609a2ba46cb46",
      "23b2d3bc692b458cbe78c1265e7ee123",
      "4bba9fa45d164fdc819e132bbd563af1",
      "9903af387bf54facad87510c91cbad54",
      "3525445c5c8849ccbfdd1ecf04ebc9d3",
      "8c1272d917d74a50bfb0a9e006cb4a32",
      "770c6313a1bc4c5596ce20f743d6c5b3",
      "1519093718d24b83924ae473b0a6ef6e",
      "3922ec5c2fb8435dba2f7c26b7e72fa2",
      "93a9c1e595eb4c17a0b166789c8844d6"
     ]
    },
    "id": "x4llz2lHN2ij",
    "outputId": "00cd656e-4236-4551-8ce8-6cc3ba98d4c8"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b38c10cd04243a6b2d98cad065f51e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ai_tutor_knowledge.jsonl:   0%|          | 0.00/6.96M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "file_path = hf_hub_download(repo_id=\"jaiganesan/ai_tutor_knowledge\", filename=\"ai_tutor_knowledge.jsonl\",repo_type=\"dataset\",local_dir=\"/content\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V-ezlgFaN-5u"
   },
   "source": [
    "# Read File\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_M-0-D4fN2fc",
    "outputId": "1d197882-4ea7-440d-a913-8f1b4db8b1bc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "762"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "with open(file_path, \"r\") as file:\n",
    "    ai_tutor_knowledge = [json.loads(line) for line in file]\n",
    "\n",
    "len(ai_tutor_knowledge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PBimOJVwOCjl"
   },
   "source": [
    "# Convert to Document obj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Ie--Y_3wN2c8"
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from llama_index.core import Document\n",
    "\n",
    "def create_docs_from_list(data_list: List[dict]) -> List[Document]:\n",
    "    documents = []\n",
    "    for data in data_list:\n",
    "        documents.append(\n",
    "            Document(\n",
    "                doc_id=data[\"doc_id\"],\n",
    "                text=data[\"content\"],\n",
    "                metadata={  # type: ignore\n",
    "                    \"url\": data[\"url\"],\n",
    "                    \"title\": data[\"name\"],\n",
    "                    \"tokens\": data[\"tokens\"],\n",
    "                    \"source\": data[\"source\"],\n",
    "                },\n",
    "                excluded_llm_metadata_keys=[\n",
    "                    \"title\",\n",
    "                    \"tokens\",\n",
    "                    \"source\",\n",
    "                ],\n",
    "                excluded_embed_metadata_keys=[\n",
    "                    \"url\",\n",
    "                    \"tokens\",\n",
    "                    \"source\",\n",
    "                ],\n",
    "            )\n",
    "        )\n",
    "    return documents\n",
    "\n",
    "doc = create_docs_from_list(ai_tutor_knowledge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lqQpen6bOEza"
   },
   "source": [
    "# Transforming\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "zVBkAg6eN2an"
   },
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceWindowNodeParser\n",
    "\n",
    "# create the sentence window node parser\n",
    "node_parser = SentenceWindowNodeParser.from_defaults(\n",
    "    window_size=3,\n",
    "    include_metadata=True,\n",
    "    window_metadata_key=\"window\",\n",
    "    original_text_metadata_key=\"original_text\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "KiDwIXFxN2YK"
   },
   "outputs": [],
   "source": [
    "documents = [i for i in doc if i.metadata['tokens']<8000]\n",
    "nodes = node_parser.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vGfVgqs2REzA",
    "outputId": "ebbd5808-17eb-4409-96ab-5d5c2a8f5061"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22083"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f1aZ4wYVN2V1",
    "outputId": "29c4ccf9-53eb-452e-f942-c023df99068f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextNode(id_='0aca9d92-f9b6-4595-8040-753fcd85a76d', embedding=None, metadata={'url': 'https://towardsai.net/p/machine-learning/bert-huggingface-model-deployment-using-kubernetes-github-repo-03-07-2024', 'title': 'BERT HuggingFace Model Deployment using Kubernetes [ Github Repo]  03/07/2024', 'tokens': 768, 'source': 'tai_blog', 'window': 'Github Repo : https://github.com/vaibhawkhemka/ML-Umbrella/tree/main/MLops/Model_Deployment/Bert_Kubernetes_deployment   Model development is useless if you dont deploy it to production  which comes with a lot of issues of scalability and portability.    I have deployed a basic BERT model from the huggingface transformer on Kubernetes with the help of docker  which will give a feel of how to deploy and manage pods on production.    Model Serving and Deployment:ML Pipeline:Workflow:   Model server (using FastAPI  uvicorn) for BERT uncased model    Containerize model and inference scripts to create a docker image    Kubernetes deployment for these model servers (for scalability)  Testing   Components:Model serverUsed BERT uncased model from hugging face for prediction of next word [MASK].  Inference is done using transformer-cli which uses fastapi and uvicorn to serve the model endpoints   Server streaming:   Testing: (fastapi docs)   http://localhost:8888/docs/   { output: [ { score: 0.21721847355365753  token: 2204  token_str: good  sequence: today is a good day }  { score: 0.16623663902282715  token: 2047  token_str: new  sequence: today is a new day }  { score: 0.07342924177646637  token: 2307  token_str: great  sequence: today is a great day }  { score: 0.0656224861741066  token: 2502  token_str: big  sequence: today is a big day }  { score: 0.03518620505928993  token: 3376  token_str: beautiful  sequence: today is a beautiful day } ]   ContainerizationCreated a docker image from huggingface GPU base image and pushed to dockerhub after testing.   ', 'original_text': 'Github Repo : https://github.com/vaibhawkhemka/ML-Umbrella/tree/main/MLops/Model_Deployment/Bert_Kubernetes_deployment   Model development is useless if you dont deploy it to production  which comes with a lot of issues of scalability and portability.   '}, excluded_embed_metadata_keys=['url', 'tokens', 'source', 'window', 'original_text'], excluded_llm_metadata_keys=['title', 'tokens', 'source', 'window', 'original_text'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='d72b4670-84f0-54a3-b259-7eb7f218674e', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'url': 'https://towardsai.net/p/machine-learning/bert-huggingface-model-deployment-using-kubernetes-github-repo-03-07-2024', 'title': 'BERT HuggingFace Model Deployment using Kubernetes [ Github Repo]  03/07/2024', 'tokens': 768, 'source': 'tai_blog'}, hash='60face68d22b29c35587c46c1cc2c768c5bba33113f9c705d2116fc860d12722'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='ae949bc9-15ad-4337-b570-65d25d9b3ca7', node_type=<ObjectType.TEXT: '1'>, metadata={'window': 'Github Repo : https://github.com/vaibhawkhemka/ML-Umbrella/tree/main/MLops/Model_Deployment/Bert_Kubernetes_deployment   Model development is useless if you dont deploy it to production  which comes with a lot of issues of scalability and portability.    I have deployed a basic BERT model from the huggingface transformer on Kubernetes with the help of docker  which will give a feel of how to deploy and manage pods on production.    Model Serving and Deployment:ML Pipeline:Workflow:   Model server (using FastAPI  uvicorn) for BERT uncased model    Containerize model and inference scripts to create a docker image    Kubernetes deployment for these model servers (for scalability)  Testing   Components:Model serverUsed BERT uncased model from hugging face for prediction of next word [MASK].  Inference is done using transformer-cli which uses fastapi and uvicorn to serve the model endpoints   Server streaming:   Testing: (fastapi docs)   http://localhost:8888/docs/   { output: [ { score: 0.21721847355365753  token: 2204  token_str: good  sequence: today is a good day }  { score: 0.16623663902282715  token: 2047  token_str: new  sequence: today is a new day }  { score: 0.07342924177646637  token: 2307  token_str: great  sequence: today is a great day }  { score: 0.0656224861741066  token: 2502  token_str: big  sequence: today is a big day }  { score: 0.03518620505928993  token: 3376  token_str: beautiful  sequence: today is a beautiful day } ]   ContainerizationCreated a docker image from huggingface GPU base image and pushed to dockerhub after testing.    Testing on docker container:   You can directly pull the image vaibhaw06/bert-kubernetes:latest   K8s deploymentUsed minikube and kubectl commands to create a single pod container for serving the model by configuring deployment and service config   deployment.yaml   apiVersion: apps/v1 kind: Deployment metadata: name: bert-deployment labels: app: bertapp spec: replicas: 1 selector: matchLabels: app: bertapp template: metadata: labels: app: bertapp spec: containers: - name: bertapp image: vaibhaw06/bert-kubernetes ports: - containerPort: 8080 --- apiVersion: v1 kind: Service metadata: name: bert-service spec: type: NodePort selector: app: bertapp ports: - protocol: TCP port: 8080 targetPort: 8080 nodePort: 30100Setting up minikube and running pods using kubectl and deployment.yaml   minikube start kubectl apply -f deployment.yamlFinal Testing:kubectl get allIt took around 15 mins to pull and create container pods.   ', 'original_text': 'I have deployed a basic BERT model from the huggingface transformer on Kubernetes with the help of docker  which will give a feel of how to deploy and manage pods on production.   '}, hash='15c4a83e59e2afad028542bcad93ba564295cd7432e63c7071baada9ecbdb7a6')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Github Repo : https://github.com/vaibhawkhemka/ML-Umbrella/tree/main/MLops/Model_Deployment/Bert_Kubernetes_deployment   Model development is useless if you dont deploy it to production  which comes with a lot of issues of scalability and portability.   ', mimetype='text/plain', start_char_idx=0, end_char_idx=254, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fPvSbtLUEzuE"
   },
   "source": [
    "## Index creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 369,
     "referenced_widgets": [
      "8cbceb9ce30f422eb2dddfcd801dd3ae",
      "33b24083e00f4f218c0dbaf3ce4a92e3",
      "8fc1a81fefa2491f9f89a8fca106b328",
      "015ed07272e44033b3cbadcc09ddb671",
      "a708efddb4654ce881513b311cde0e2f",
      "581e580be3af479a94ca4fd71e3c3431",
      "03a1a97fd87d4d0886f1942a02af660c",
      "de05568d1a414880a469b8acd9e50202",
      "6b698b74a8c1463887e76ce71605b493",
      "dca015032034442fa2017b70a72821ba",
      "167bc571dac14b11acf17e677f11b8af",
      "ed86bbdc36b244d1897133692e3a9bcc",
      "ad449d1225db4a26b160f9784e9154f5",
      "e0321fc48ca84bf68a53d3d3446b7e53",
      "5dfc4ea100594b299d00a5b8f56cfe9d",
      "14dede8a94f84e3cb923d3af1871abbe",
      "94ac477784b34182a3fd29f22d77132c",
      "550592b461b847d6b3d3998c99cf07ed",
      "f071a2d5e874466fa5a9b9f99011adfc",
      "23a09ee19a63495db853d930ab1a5b08",
      "f6638201d26b45df8a570d21d3f6885e",
      "3c9cbe50acf748c0b8e95f7e54eb0437",
      "0c2f48e154074ed9ad6447c17a089029",
      "7847333520fc41fa8fd0cfe5d6c5810e",
      "c349bc802307403aad7221a114e4be24",
      "7855caab7c2f4ce691a3ccd54e14d4ee",
      "7930050440f9442b804ab37abfc9707a",
      "b7f03c6b4c9248a6b80613602673d927",
      "be35c037306543f99ab2f350f2ce6b46",
      "6f27f13f6e4b47c8803e03c20a89a81a",
      "7735d90574ce46e0992ac52684b98178",
      "8948f5641d67438f9cdc3ce93963ecc1",
      "e960e528016545fda5f178c263c7da78",
      "86daa81ca7bb4792bdce58254a7e2d50",
      "957c9ed2e1ae4060a234cf6298a4d7e5",
      "082c041af54f445086dc034d4d078d22",
      "6dc15a709e9e4d0f9ecb60074175507f",
      "39e2fe7766f74697943e6b975e504376",
      "7093f6b4f9ae40c7a728b086e8d6ba23",
      "de4c83a42f1c49a5a16cd264119fe09c",
      "6d7332ccbe6c422cb3efe43d3116d249",
      "baea9b22c5aa4c2584466f4ff51ba1ac",
      "c2739cbd408d48f3ba9d7bb05996650d",
      "d7ee8a8969e9408d9bbe2ee7c884fa9f",
      "80e5427e555546f5a088be7f43be4a53",
      "3d7495b64ac143f19130c89df502fc0d",
      "c5f96861845d42ec9344be4c46f3cefa",
      "a11eeecc812644818099e91aaee34043",
      "9e64d44b56254997a39bbcbede5247ba",
      "32fa6d119a5441c3adf4b66dfc8643b0",
      "857c4f65477644a1ab2d263e100d1634",
      "88b8a7c4f7f242c7ad7bc0895d0f6b06",
      "9cd30aeda5b64f899098158f8cb49269",
      "c2447e4e7d9046a4ab7d9ae033568ab8",
      "52b86ab4ac234f64b2588075f89d4826",
      "ab16b2f1b4264e1d8e94514d83acc0f8",
      "ac46e567c3614ce5becaf30ebb63f062",
      "c4fa93dddfd14cbfb2c7b5a38c8f0bee",
      "4f7906bb56fb452496e253b600864225",
      "2943f3dc26e04542beb5af4b494672ab",
      "ab71c9d5b93c4816b311d4f5bf682572",
      "61f1da15a44f436dbbe7413db38e97f5",
      "64d77ec02c9c4e3c89047c41ec118064",
      "ca0157a70a0b46db9a4da8363a5f2893",
      "f0c498b3b3d44a75bbf621a243329c09",
      "b669b4a9956f442f8159dc7454bee00a",
      "a2ed1e23e001465fb1e7c867be9d5306",
      "e94dd6b8e1454e18b1871f18f5fcfaed",
      "cfacfbf6dd4a490bbdffb973172d0372",
      "339ca3c5754b4c2cbde147c2fcfdc995",
      "c3e4657e6ce74467ab65074d33c2c65b",
      "e7986e3c207341beacd50526aa298110",
      "acb989733202427c8baa5eae4e3d2c5c",
      "4662a135e9524a21a87b272442b15a4c",
      "f427e7d338044702a44277cc8c66db27",
      "fda1237d329e44c4aa848794bdc43bae",
      "810880f66e814c589bc95bca1dfb8075",
      "6430226e9ced408a87d097855305477b",
      "1e62095b20a147fa92724d88184628d5",
      "e496496d95d545b8a0ab03b63a12f51f",
      "f7300b8d8ef648d8a5da1ba22d8c0fd4",
      "626b651649b84f4a99e0d89d6192b149",
      "4b0067c818224896970b4af64bc7507c",
      "1f29e45e85524b44bb2514c145d24d3e",
      "35755b831f254667a4f5b447a9045b19",
      "bc04102c0b404a4e867afc9534f812b5",
      "ea425a95a4524bd1b9bf610a9124daf7",
      "5eb995b3acb04224bdaf3fa9ba5a703e",
      "00d7cf55b73142968cad3bf1ab9d1fd0",
      "8dc0c59063be4bb4ba84e63ba55e5e81",
      "4bd66027ab554a338b539b49b476a47e",
      "31d6484c2bd943feb2f5e7729350d54d",
      "1014a99200894e48975e935276467d0f",
      "eebfce69b3854104a02bcb7dd6a457b5",
      "6632d58176824aeb8b0d1a93f102e27d",
      "da7c1cbccc9545c3bf85607e0e850520",
      "ebf81061a37b47fc8bdbac174f205c1a",
      "b234f59c847642bdbd0951f1ada58fb2",
      "c17e3a354f474954b4bc942f295db391",
      "a28a820d713f406988aea4ea47969782",
      "9b8f7bf5347e4465af7e3ae38d037860",
      "dc34dac9e5724b09b1228bea1e756398",
      "da0cc192495c4103a9e0c1656c66881f",
      "1db5d147dc9d44abab2c87fb21b24924",
      "81807289e5f44b2a8752ae7b7fe6676a",
      "f0a0e0f978f04372a9bdd62719a6e70c",
      "5a42fc1001854646955f38570d76557a",
      "39957a03bb6843209c415d2c4f21550f",
      "6af375bdd087420ab69ed244b4a85d00",
      "cf41f728b4224f82a95f35230cdec945",
      "f63929c654fb4fb09d2c0be621cf25bf",
      "602a4ebde5134ac6ad0b94916b3124db",
      "b0a2ef8fb237475aa6536cc1ffc4cad9",
      "196d6446201d4a17af88ad905f9585aa",
      "b2f231db9bfd4c0ab5210abc8cd80b04",
      "7fcec0fe3b4f46e6a772ea6c56c817d7",
      "cfe719acb03147d8bd113d38fb2bf9f3",
      "a2b3b24062524a118cff6babdc9de800",
      "110286c7773e49d4a3a0498a2c8f470c",
      "afb07092c60d433aa36884f0e4660294",
      "d7c05576ed5047c7a8b7363dc3a56dba"
     ]
    },
    "id": "moNbizWrN2Tu",
    "outputId": "2e052104-e077-436c-a29c-ecf345a85110"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cbceb9ce30f422eb2dddfcd801dd3ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/2048 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed86bbdc36b244d1897133692e3a9bcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/2048 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c2f48e154074ed9ad6447c17a089029",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/2048 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86daa81ca7bb4792bdce58254a7e2d50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/2048 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80e5427e555546f5a088be7f43be4a53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/2048 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab16b2f1b4264e1d8e94514d83acc0f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/2048 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2ed1e23e001465fb1e7c867be9d5306",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/2048 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6430226e9ced408a87d097855305477b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/2048 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00d7cf55b73142968cad3bf1ab9d1fd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/2048 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a28a820d713f406988aea4ea47969782",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/2048 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f63929c654fb4fb09d2c0be621cf25bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/1603 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "# Add the documents to the database and create Index / embeddings\n",
    "index = VectorStoreIndex(nodes, storage_context=storage_context,show_progress =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nz6dQtXzyWqK",
    "outputId": "767f7a7e-fb0c-4cf2-ce9f-5af126d8f202"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: ai_tutor_knowledge/ (stored 0%)\n",
      "  adding: ai_tutor_knowledge/8a0fd1e7-21aa-432e-853a-8cade26410a5/ (stored 0%)\n",
      "  adding: ai_tutor_knowledge/8a0fd1e7-21aa-432e-853a-8cade26410a5/header.bin (deflated 55%)\n",
      "  adding: ai_tutor_knowledge/8a0fd1e7-21aa-432e-853a-8cade26410a5/link_lists.bin (deflated 78%)\n",
      "  adding: ai_tutor_knowledge/8a0fd1e7-21aa-432e-853a-8cade26410a5/data_level0.bin (deflated 17%)\n",
      "  adding: ai_tutor_knowledge/8a0fd1e7-21aa-432e-853a-8cade26410a5/index_metadata.pickle (deflated 44%)\n",
      "  adding: ai_tutor_knowledge/8a0fd1e7-21aa-432e-853a-8cade26410a5/length.bin (deflated 53%)\n",
      "  adding: ai_tutor_knowledge/chroma.sqlite3 (deflated 90%)\n"
     ]
    }
   ],
   "source": [
    "# Compress the vector store directory to a zip file to be able to download and use later.\n",
    "!zip -r vectorstore-windowed.zip ai_tutor_knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7qZY6xOYyjIX"
   },
   "source": [
    "# Load Indexes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e7hRxvh-SjtX"
   },
   "source": [
    "**Note: If you didn’t create the vector store from scratch, please uncomment the three code blocks/cells below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "0cd8cbd789cb4b3694032416246636cc",
      "5cf0d92855ba4d86b02e68b74d5b0a72",
      "1860ffd7acce4093a2a831cb31ab6a0f",
      "9070ef56d7de430483bea26a2d2c17dc",
      "a45c212e01784546a3aa4803263b8054",
      "36065ec094974087b1a48e07d2deeb05",
      "7fc5a3263e4546ad8f6bca1dfa6ba9da",
      "1f1fed6a62874312ba7c36156f4b4bc5",
      "50a0632c4b9146f68b25cda473b2e10c",
      "431c411510a44aea92c53b17fb8e302f",
      "7e9303f026794f98a719ecbf8edf489c"
     ]
    },
    "collapsed": true,
    "id": "1ppFu822FMSB",
    "outputId": "9664c1dd-3b7d-477c-ede2-71fb65d39691"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cd8cbd789cb4b3694032416246636cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vectorstore-windowed.zip:   0%|          | 0.00/148M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from huggingface_hub import hf_hub_download\n",
    "# vectorstore = hf_hub_download(repo_id=\"jaiganesan/ai_tutor_knowledge\", filename=\"vectorstore-windowed.zip\",repo_type=\"dataset\",local_dir=\"/content\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "wS-V6NhMymx8",
    "outputId": "0a88bb2a-205f-4f4e-ca0e-8816b973dc24"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  vectorstore-windowed.zip\n",
      "   creating: ai_tutor_knowledge/\n",
      "   creating: ai_tutor_knowledge/0201aaa7-f5de-4efd-8594-fd5c929d6fdf/\n",
      "  inflating: ai_tutor_knowledge/0201aaa7-f5de-4efd-8594-fd5c929d6fdf/header.bin  \n",
      "  inflating: ai_tutor_knowledge/0201aaa7-f5de-4efd-8594-fd5c929d6fdf/data_level0.bin  \n",
      "  inflating: ai_tutor_knowledge/0201aaa7-f5de-4efd-8594-fd5c929d6fdf/link_lists.bin  \n",
      "  inflating: ai_tutor_knowledge/0201aaa7-f5de-4efd-8594-fd5c929d6fdf/length.bin  \n",
      "  inflating: ai_tutor_knowledge/0201aaa7-f5de-4efd-8594-fd5c929d6fdf/index_metadata.pickle  \n",
      "  inflating: ai_tutor_knowledge/chroma.sqlite3  \n"
     ]
    }
   ],
   "source": [
    "# !unzip vectorstore-windowed.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "zlJoIovXWKp6",
    "outputId": "dba5c3b7-f12b-4244-cef7-4a2ebe1d9243"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:chromadb.db.impl.sqlite:⚠️ It looks like you upgraded from a version below 0.6 and could benefit from vacuuming your database. Run chromadb utils vacuum --help for more information.\n"
     ]
    }
   ],
   "source": [
    "# import chromadb\n",
    "# from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "# from llama_index.core import VectorStoreIndex\n",
    "\n",
    "# # Create your index\n",
    "# db = chromadb.PersistentClient(path=\"./ai_tutor_knowledge\")\n",
    "# chroma_collection = db.get_or_create_collection(\"ai_tutor_knowledge\")\n",
    "# vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "\n",
    "# # Create your index\n",
    "# index = VectorStoreIndex.from_vector_store(vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "fH2myF120oMi"
   },
   "outputs": [],
   "source": [
    "from llama_index.core.postprocessor import MetadataReplacementPostProcessor\n",
    "\n",
    "query_engine = index.as_query_engine(\n",
    "    llm=Settings.llm,\n",
    "    # the target key defaults to `window` to match the node_parser's default\n",
    "    node_postprocessors=[\n",
    "        MetadataReplacementPostProcessor(target_metadata_key=\"window\")\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "id": "EqNreFmE0vRb",
    "outputId": "a739091f-838b-4a34-e4a4-693d8393c2aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard RAG methods and a more advanced approach called Self-Reflective RAG or Corrective RAG were compared to Speculative RAG.  Speculative RAG uses a smaller, specialized language model to create multiple drafts from different subsets of retrieved documents.  A larger, more general language model then verifies these drafts. This approach improves understanding of each document subset, reduces the effect of bias from long contexts, and speeds up the process by only having the larger model do a single verification step.  Speculative RAG consistently outperformed the other methods across multiple benchmarks, showing significant improvements in accuracy.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"Write about naive RAG and Speculative RAG?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "whdPLhVaMfOS",
    "outputId": "141cca4f-51c5-4add-e174-855caf469404"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source  1\n",
      "Original Text: In this work, we introduce SPECULATIVE RAG – a framework that leverages a larger generalist LM to efficiently verify multiple RAG drafts produced in parallel by a smaller, distilled specialist LM. \n",
      "Window: **Abstract**Retrieval augmented generation (RAG) combines the generative abilities of large language models (LLMs) with external knowledge sources to provide more accurate and up-to-date responses.  Recent RAG advancements focus on improving retrieval outcomes through iterative LLM refinement or self-critique capabilities acquired through additional instruction tuning of LLMs.  In this work, we introduce SPECULATIVE RAG – a framework that leverages a larger generalist LM to efficiently verify multiple RAG drafts produced in parallel by a smaller, distilled specialist LM.  Each draft is generated from a distinct subset of retrieved documents, offering diverse perspectives on the evidence while reducing input token counts per draft.  This approach enhances comprehension of each subset and mitigates potential position bias over long context.  Our method accelerates RAG by delegating drafting to the smaller specialist LM, with the larger generalist LM performing a single verification pass over the drafts. \n",
      "----\n",
      "Source  2\n",
      "Original Text: 4.3 Main ResultsWe compare SPECULATIVE RAG with standard RAG approaches, as well as the more advanced Self-Reflective RAG and Corrective RAG on four datasets: TriviaQA, MusiQue, PubHealth, and ARC-Challenge. \n",
      "Window: 4.3 Main ResultsWe compare SPECULATIVE RAG with standard RAG approaches, as well as the more advanced Self-Reflective RAG and Corrective RAG on four datasets: TriviaQA, MusiQue, PubHealth, and ARC-Challenge.  We report the performance of M_{Draft+7B} when used alone or paired with the RAG verifier (e.g., M_{Verifier-7B}, M_{Verifier-13B/7B}).  Following prior work (Asai et al., 2023; Yan et al., 2024), we report accuracy as the performance metric.Superior Performance over BaselinesTable 1 demonstrates that SPECULATIVE RAG consistently outperforms all baselines across all four benchmarks.  Particularly, M_{Verifier-13B/7B} + M_{Draft+7B} surpasses the most competitive standard RAG model, Mistral-Instruc^{as}_{7B}, by 0.33% on TriviaQA, 2.15% on MusiQue, 12.97% on PubHealth, and 2.14% on ARC-Challenge. \n",
      "----\n"
     ]
    }
   ],
   "source": [
    "for idx, item in enumerate(response.source_nodes):\n",
    "    print(\"Source \", idx + 1)\n",
    "    print(\"Original Text:\", item.node.metadata[\"original_text\"])\n",
    "    print(\"Window:\", item.node.metadata[\"window\"])\n",
    "    print(\"----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "dQBrOUYrLA76"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "29eHO2RNUSjw"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
