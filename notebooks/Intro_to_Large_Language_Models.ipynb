{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction to LLMs and How To Use via API: Taking Control and Building a Conversation"
      ],
      "metadata": {
        "id": "pdyupkTqsz6j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the specific version of the openai library used in this lesson Because We pin\n",
        "# the version to ensure the code works exactly as shown, as library updates can sometimes\n",
        "# introduce changes.\n",
        "\n",
        "# The '-q' flag makes the installation quieter (less output)\n",
        "!pip install -q openai==1.107.0"
      ],
      "metadata": {
        "collapsed": true,
        "id": "eiqwUbfcShuQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "799a6226-134f-40a3-9901-4b498c6b90e8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m951.0/951.0 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Code Block 1"
      ],
      "metadata": {
        "id": "fv7XCV6DRmRR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import os\n",
        "import openai\n",
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "QlkldrBdzaMC"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OCoxlgoCSD2P",
        "outputId": "da76af6a-8bc7-4dbd-d75d-5a70795319a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "API key loaded from Colab userdata.\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    # Try to get API key from Google Colab's userdata\n",
        "\n",
        "    # Make sure to replace 'OPENAI_API_KEY' with the actual name you gave your secret key in Google Colab secrets.\n",
        "    api_key = userdata.get('OPENAI_API_KEY')\n",
        "    if api_key:\n",
        "        print(\"API key loaded from Colab userdata.\")\n",
        "\n",
        "    if not api_key:\n",
        "        print(\"OpenAI API key not found in Colab secrets.\")\n",
        "        api_key = input(\"Please enter your OpenAI API key manually: \")\n",
        "\n",
        "except ImportError:\n",
        "    print(\"Not running in Colab environment.\")\n",
        "    api_key = input(\"Please enter your OpenAI API key manually: \")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Final API_KEY validation\n",
        "if not api_key:\n",
        "    raise ValueError(\"API Key not provided. Please ensure it's set.\")\n",
        "else:\n",
        "    print(f\"API Key loaded successfully (starting with: {api_key[:4]}...).\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hd80wdPFzErB",
        "outputId": "a6f7e4a5-8140-407d-df18-6378ffc7506b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "API Key loaded successfully (starting with: sk-p...).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# All subsequent API calls will be made through this 'client' object.\n",
        "try:\n",
        "    client = openai.OpenAI(api_key=api_key)\n",
        "    print(\"OpenAI client initialized successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error initializing OpenAI client: {e}\")\n",
        "    # You might want to exit or raise the error here depending on desired behavior\n",
        "    raise"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OCTwrKEOTEc1",
        "outputId": "e333a32a-1438-4b01-91ff-f699c52335bf"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OpenAI client initialized successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Code Block 2"
      ],
      "metadata": {
        "id": "AEJwU3sUUTMk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Block 2: First Turn - Asking the Initial Question ---\n",
        "print(\"\\n--- Starting Conversation: Turn 1 ---\")\n",
        "\n",
        "# Define the system message (persona) for the AI Tutor\n",
        "system_instructions = \"You are a helpful AI Tutor explaining Large Language Model concepts simply.\"\n",
        "\n",
        "# Define the user's first question\n",
        "user_input_1 = \"Can you explain what 'tokens' are in the context of LLMs, like I'm new to this?\"\n",
        "\n",
        "print(f\"System Instructions: {system_instructions}\")\n",
        "print(f\"User Input: {user_input_1}\")\n",
        "\n",
        "# Define parameters for this call\n",
        "MODEL = \"gpt-5-mini\"\n",
        "MAX_OUTPUT_TOKENS=500 # (GPT 5 & GPT 5 mini are reasoning model, so includes visible output tokens and reasoning tokens)\n",
        "REASONING_EFFORT =\"minimal\" # Constrains effort on reasoning for reasoning models. Currently supported values are minimal, low, medium, and high"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PQwTVEM3TFeL",
        "outputId": "7aa8a043-1ca4-411d-8fd9-0e29f40d9097"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Starting Conversation: Turn 1 ---\n",
            "System Instructions: You are a helpful AI Tutor explaining Large Language Model concepts simply.\n",
            "User Input: Can you explain what 'tokens' are in the context of LLMs, like I'm new to this?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    print(f\"\\nMaking API call to {MODEL}...\")\n",
        "    # Use the Response API\n",
        "    response_1 = client.responses.create(\n",
        "        model=MODEL,\n",
        "        instructions=system_instructions,\n",
        "        input=user_input_1,\n",
        "        max_output_tokens=MAX_OUTPUT_TOKENS,\n",
        "        reasoning= {'effort':REASONING_EFFORT},\n",
        "    )\n",
        "    print(\"API call successful.\")\n",
        "    # --- Process the response from the first turn ---\n",
        "    # Extract the assistant's reply content\n",
        "    assistant_response_1 = response_1.output[1].content[0].text\n",
        "\n",
        "    # Save the response ID for conversation continuity\n",
        "    response_id_1 = response_1.id\n",
        "\n",
        "    print(\"\\nAI Tutor (Turn 1):\")\n",
        "    print(assistant_response_1)\n",
        "\n",
        "    # Print token usage for this call\n",
        "    usage_1 = response_1.usage\n",
        "    print(\"\\n\\n===========================================\")\n",
        "    print(f\"\\nToken Usage (Turn 1): Input={usage_1.input_tokens}, Output={usage_1.output_tokens}, Total={usage_1.total_tokens}\")\n",
        "\n",
        "except openai.APIError as e:\n",
        "    print(f\"OpenAI API returned an API Error: {e}\")\n",
        "except openai.AuthenticationError as e:\n",
        "    print(f\"OpenAI Authentication Error: {e}\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-rCEBIxfWHrX",
        "outputId": "5084537a-14a3-4a72-9545-1b4c2399f983"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Making API call to gpt-5-mini...\n",
            "API call successful.\n",
            "\n",
            "AI Tutor (Turn 1):\n",
            "Sure — here’s a simple explanation.\n",
            "\n",
            "What a token is\n",
            "- A token is a piece of text that an LLM (large language model) processes at one time. It can be a whole word, part of a word, punctuation, or even a single character, depending on the language and tokenization method.\n",
            "- Models don’t think in characters or words the way people do; they convert text into tokens (numbers) before processing.\n",
            "\n",
            "Why tokens exist\n",
            "- Tokens let the model handle a wide variety of languages and word forms efficiently. Instead of having a gigantic entry for every possible word, tokenizers break text into reusable subword parts.\n",
            "- This reduces vocabulary size while still allowing the model to represent rare or new words by combining subwords.\n",
            "\n",
            "Examples\n",
            "- Short word: “cat” might be one token.\n",
            "- Compound or rare word: “unhappiness” might be split into tokens like “un”, “happi”, “ness”.\n",
            "- Punctuation and spaces often are tokens too: “Hello, world!” could split into “Hello”, “,”, “ world”, “!”.\n",
            "\n",
            "Tokenization process (brief)\n",
            "1. Text is split into tokens by a tokenizer (rules learned from training data).\n",
            "2. Each token is mapped to a numeric ID.\n",
            "3. The model processes the sequence of token IDs.\n",
            "4. When generating text, the model outputs token IDs that are converted back into characters/words.\n",
            "\n",
            "Why this matters for users\n",
            "- Limits and costs: Models have a maximum token limit (context window). Longer inputs use more tokens and may be truncated. Many API costs are billed per token.\n",
            "- Prompt design: Short, focused prompts use fewer tokens. If you need long output, both input and output tokens count toward limits/cost.\n",
            "- Exact counts are tokenization-dependent — a sentence can be a different number of tokens for different models/tokenizers.\n",
            "\n",
            "A quick rule of thumb\n",
            "- For English, 1 token ≈ ¾ of a word on average. So 100 tokens ≈ 75–100 words (very approximate).\n",
            "\n",
            "If you want, I can:\n",
            "- Show how a short paragraph is tokenized by a real tokenizer, or\n",
            "- Explain how token limits affect prompts and responses.\n",
            "\n",
            "\n",
            "===========================================\n",
            "\n",
            "Token Usage (Turn 1): Input=45, Output=455, Total=500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Code Block 3"
      ],
      "metadata": {
        "id": "H8ILfDt3m-Ok"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Block 3: Second Turn - Asking a Follow-up Question ---\n",
        "print(\"\\n--- Continuing Conversation: Turn 2 ---\")\n",
        "\n",
        "# Define the user's second question\n",
        "user_input_2 = \"Thanks! So, based on your explanation, are common words like 'the' or 'is' usually single tokens?\"\n",
        "\n",
        "print(f\"\\nUser Input (Turn 2): {user_input_2}\")\n",
        "\n",
        "\n",
        "try:\n",
        "    print(f\"\\nMaking API call to {MODEL} (Turn 2)...\")\n",
        "    response_2 = client.responses.create(\n",
        "        model=MODEL,\n",
        "        instructions=system_instructions,\n",
        "        input=user_input_2,\n",
        "        max_output_tokens=MAX_OUTPUT_TOKENS,\n",
        "        reasoning= {'effort':REASONING_EFFORT},\n",
        "        previous_response_id=response_id_1  # Link to previous response for context\n",
        "    )\n",
        "    print(\"API call successful.\")\n",
        "\n",
        "    # --- Process the response from the second turn ---\n",
        "    assistant_response_2 = response_2.output[1].content[0].text\n",
        "\n",
        "    print(\"\\nAI Tutor (Turn 2):\")\n",
        "    print(assistant_response_2)\n",
        "\n",
        "    # Print token usage for this call\n",
        "    usage_2 = response_2.usage\n",
        "\n",
        "    print(\"\\n\\n===========================================\")\n",
        "    print(f\"\\nToken Usage (Turn 2): Input={usage_2.input_tokens}, Output={usage_2.output_tokens}, Total={usage_2.total_tokens}\")\n",
        "\n",
        "except openai.APIError as e:\n",
        "    print(f\"OpenAI API returned an API Error: {e}\")\n",
        "except openai.AuthenticationError as e:\n",
        "    print(f\"OpenAI Authentication Error: {e}\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I9UTAfU71Q4M",
        "outputId": "2ff59ef5-1633-4880-b9e1-f31090eda806"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Continuing Conversation: Turn 2 ---\n",
            "\n",
            "User Input (Turn 2): Thanks! So, based on your explanation, are common words like 'the' or 'is' usually single tokens?\n",
            "\n",
            "Making API call to gpt-5-mini (Turn 2)...\n",
            "API call successful.\n",
            "\n",
            "AI Tutor (Turn 2):\n",
            "Yes — common English words like \"the\" and \"is\" are almost always single tokens in commonly used tokenizers for LLMs.\n",
            "\n",
            "Why:\n",
            "- Tokenizers are trained on large text corpora and include frequent words as whole-token entries in the vocabulary. Representing highly frequent words as single tokens is efficient for both storage and processing.\n",
            "- Short, common words are simple and appear often, so the tokenizer doesn't need to split them into subword pieces.\n",
            "\n",
            "A couple of caveats:\n",
            "- Tokenization depends on the tokenizer and language. For standard English tokenizers used with popular LLMs (Byte-Pair Encoding, Unigram, or similar schemes), \"the\" and \"is\" are single tokens. In other languages or with unusual tokenizers, behavior can differ.\n",
            "- Surrounding characters can affect tokenization. For example, \"the\" in \"theater\" may be part of a larger token (\"theater\") rather than the separate token \"the\" + \"ater\". Punctuation or spaces typically separate tokens, so \"the.\" (with a period attached) might be a single token containing the punctuation, or two tokens (\"the\" + \".\") depending on the tokenizer.\n",
            "\n",
            "If you'd like, I can show exact token IDs and splits for a small sentence using a real tokenizer.\n",
            "\n",
            "\n",
            "===========================================\n",
            "\n",
            "Token Usage (Turn 2): Input=528, Output=267, Total=795\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Example usage object from response_1 or response_2:\n",
        "print(usage_1.input_tokens)  # -> number of input tokens\n",
        "print(usage_1.output_tokens) # -> number of output tokens\n",
        "print(usage_1.total_tokens) # -> sum of both"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oBQWJIeRoJ7n",
        "outputId": "e5a8b756-0e91-4cca-f9a1-35f4b02b57c9"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "45\n",
            "455\n",
            "500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Code Block 4"
      ],
      "metadata": {
        "id": "N0RI6UGhoDtC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Block 4: Cost Calculation Function & Example ---\n",
        "def calculate_cost(usage, input_price_per_mil, output_price_per_mil):\n",
        "    \"\"\"Calculates the cost of an API call based on token usage and prices.\n",
        "\n",
        "    Args:\n",
        "        usage: The usage object from the OpenAI response\n",
        "               (e.g., response.usage). It should have attributes\n",
        "               'input_tokens' and 'output_tokens'.\n",
        "        input_price_per_mil: Cost in USD per 1 million input tokens.\n",
        "        output_price_per_mil: Cost in USD per 1 million output tokens.\n",
        "\n",
        "    Returns:\n",
        "        The total cost in USD for the API call, or None if usage is invalid.\n",
        "    \"\"\"\n",
        "    if not usage or not hasattr(usage, 'input_tokens') or not hasattr(usage, 'output_tokens'):\n",
        "        print(\"Warning: Invalid usage object provided for cost calculation.\")\n",
        "        return None\n",
        "\n",
        "    input_cost = (usage.input_tokens / 1_000_000) * input_price_per_mil\n",
        "    output_cost = (usage.output_tokens / 1_000_000) * output_price_per_mil\n",
        "    total_cost = input_cost + output_cost\n",
        "\n",
        "    return total_cost\n",
        "\n",
        "# --- Current Prices for GPT-5-mini ---\n",
        "# IMPORTANT: Always verify at https://openai.com/pricing\n",
        "PRICE_INPUT_PER_MIL = 0.250\n",
        "PRICE_OUTPUT_PER_MIL = 2.000\n",
        "\n",
        "print(f\"\\n--- Cost Calculations (GPT-5-mini) ---\")\n",
        "print(f\"Prices: Input=${PRICE_INPUT_PER_MIL:.3f}/1M, Output=${PRICE_OUTPUT_PER_MIL:.3f}/1M\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YrN9B0wQ1cLK",
        "outputId": "dcbfffa4-b7d9-47d9-cb66-40eca7775228"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Cost Calculations (GPT-5-mini) ---\n",
            "Prices: Input=$0.250/1M, Output=$2.000/1M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate cost for Turn 1\n",
        "try:\n",
        "    if 'usage_1' in locals():\n",
        "        cost_1 = calculate_cost(usage_1, PRICE_INPUT_PER_MIL, PRICE_OUTPUT_PER_MIL)\n",
        "        if cost_1 is not None:\n",
        "            print(f\"\\nCost for Turn 1:\")\n",
        "            print(f\"  Input Tokens: {usage_1.input_tokens}, Output Tokens: {usage_1.output_tokens}\")\n",
        "            print(f\"  Total Cost: ${cost_1:.8f}\")\n",
        "    else:\n",
        "        print(\"\\nSkipping Turn 1 cost calculation (usage_1 not found).\")\n",
        "\n",
        "    # Calculate cost for Turn 2\n",
        "    if 'usage_2' in locals():\n",
        "        cost_2 = calculate_cost(usage_2, PRICE_INPUT_PER_MIL, PRICE_OUTPUT_PER_MIL)\n",
        "        if cost_2 is not None:\n",
        "            print(f\"\\nCost for Turn 2:\")\n",
        "            print(f\"  Input Tokens: {usage_2.input_tokens}, Output Tokens: {usage_2.output_tokens}\")\n",
        "            print(f\"  Total Cost: ${cost_2:.8f}\")\n",
        "    else:\n",
        "        print(\"\\nSkipping Turn 2 cost calculation (usage_2 not found).\")\n",
        "\n",
        "    # Calculate total conversation cost\n",
        "    if 'cost_1' in locals() and 'cost_2' in locals() and cost_1 is not None and cost_2 is not None:\n",
        "        total_conversation_cost = cost_1 + cost_2\n",
        "        print(f\"\\nTotal Conversation Cost (Turn 1 + Turn 2): ${total_conversation_cost:.8f}\")\n",
        "\n",
        "except NameError as e:\n",
        "    print(f\"\\nCould not calculate costs, a required variable is missing: {e}\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during cost calculation: {e}\")"
      ],
      "metadata": {
        "id": "vzk2VbYfW8sx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe973c3f-fabb-4d8d-8f1d-24db130ccd59"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Cost for Turn 1:\n",
            "  Input Tokens: 45, Output Tokens: 455\n",
            "  Total Cost: $0.00092125\n",
            "\n",
            "Cost for Turn 2:\n",
            "  Input Tokens: 528, Output Tokens: 252\n",
            "  Total Cost: $0.00063600\n",
            "\n",
            "Total Conversation Cost (Turn 1 + Turn 2): $0.00155725\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gLFCrHGN0vmm"
      },
      "execution_count": 10,
      "outputs": []
    }
  ]
}