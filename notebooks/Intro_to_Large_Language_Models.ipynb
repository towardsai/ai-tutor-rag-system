{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XclVB4pvLyed"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/towardsai/ai-tutor-rag-system/blob/main/notebooks/Intro_to_Large_Language_Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pdyupkTqsz6j"
   },
   "source": [
    "## Introduction to LLMs and How To Use via API: Taking Control and Building a Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "eiqwUbfcShuQ",
    "outputId": "799a6226-134f-40a3-9901-4b498c6b90e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m951.0/951.0 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "# Install the specific version of the openai library used in this lesson Because We pin\n",
    "# the version to ensure the code works exactly as shown, as library updates can sometimes\n",
    "# introduce changes.\n",
    "\n",
    "# The '-q' flag makes the installation quieter (less output)\n",
    "!pip install -q openai==1.107.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fv7XCV6DRmRR"
   },
   "source": [
    "### Code Block 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QlkldrBdzaMC"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import openai\n",
    "from google.colab import userdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OCoxlgoCSD2P",
    "outputId": "da76af6a-8bc7-4dbd-d75d-5a70795319a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key loaded from Colab userdata.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Try to get API key from Google Colab's userdata\n",
    "\n",
    "    # Make sure to replace 'OPENAI_API_KEY' with the actual name you gave your secret key in Google Colab secrets.\n",
    "    api_key = userdata.get('OPENAI_API_KEY')\n",
    "    if api_key:\n",
    "        print(\"API key loaded from Colab userdata.\")\n",
    "\n",
    "    if not api_key:\n",
    "        print(\"OpenAI API key not found in Colab secrets.\")\n",
    "        api_key = input(\"Please enter your OpenAI API key manually: \")\n",
    "\n",
    "except ImportError:\n",
    "    print(\"Not running in Colab environment.\")\n",
    "    api_key = input(\"Please enter your OpenAI API key manually: \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hd80wdPFzErB",
    "outputId": "a6f7e4a5-8140-407d-df18-6378ffc7506b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Key loaded successfully (starting with: sk-p...).\n"
     ]
    }
   ],
   "source": [
    "# Final API_KEY validation\n",
    "if not api_key:\n",
    "    raise ValueError(\"API Key not provided. Please ensure it's set.\")\n",
    "else:\n",
    "    print(f\"API Key loaded successfully (starting with: {api_key[:4]}...).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OCTwrKEOTEc1",
    "outputId": "e333a32a-1438-4b01-91ff-f699c52335bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI client initialized successfully.\n"
     ]
    }
   ],
   "source": [
    "# All subsequent API calls will be made through this 'client' object.\n",
    "try:\n",
    "    client = openai.OpenAI(api_key=api_key)\n",
    "    print(\"OpenAI client initialized successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing OpenAI client: {e}\")\n",
    "    # You might want to exit or raise the error here depending on desired behavior\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AEJwU3sUUTMk"
   },
   "source": [
    "### Code Block 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PQwTVEM3TFeL",
    "outputId": "7aa8a043-1ca4-411d-8fd9-0e29f40d9097"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Conversation: Turn 1 ---\n",
      "System Instructions: You are a helpful AI Tutor explaining Large Language Model concepts simply.\n",
      "User Input: Can you explain what 'tokens' are in the context of LLMs, like I'm new to this?\n"
     ]
    }
   ],
   "source": [
    "# --- Block 2: First Turn - Asking the Initial Question ---\n",
    "print(\"\\n--- Starting Conversation: Turn 1 ---\")\n",
    "\n",
    "# Define the system message (persona) for the AI Tutor\n",
    "system_instructions = \"You are a helpful AI Tutor explaining Large Language Model concepts simply.\"\n",
    "\n",
    "# Define the user's first question\n",
    "user_input_1 = \"Can you explain what 'tokens' are in the context of LLMs, like I'm new to this?\"\n",
    "\n",
    "print(f\"System Instructions: {system_instructions}\")\n",
    "print(f\"User Input: {user_input_1}\")\n",
    "\n",
    "# Define parameters for this call\n",
    "MODEL = \"gpt-5-mini\"\n",
    "MAX_OUTPUT_TOKENS=500 # (GPT 5 & GPT 5 mini are reasoning model, so includes visible output tokens and reasoning tokens)\n",
    "REASONING_EFFORT =\"minimal\" # Constrains effort on reasoning for reasoning models. Currently supported values are minimal, low, medium, and high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-rCEBIxfWHrX",
    "outputId": "5084537a-14a3-4a72-9545-1b4c2399f983"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Making API call to gpt-5-mini...\n",
      "API call successful.\n",
      "\n",
      "AI Tutor (Turn 1):\n",
      "Sure — here’s a simple explanation.\n",
      "\n",
      "What a token is\n",
      "- A token is a piece of text that an LLM (large language model) processes at one time. It can be a whole word, part of a word, punctuation, or even a single character, depending on the language and tokenization method.\n",
      "- Models don’t think in characters or words the way people do; they convert text into tokens (numbers) before processing.\n",
      "\n",
      "Why tokens exist\n",
      "- Tokens let the model handle a wide variety of languages and word forms efficiently. Instead of having a gigantic entry for every possible word, tokenizers break text into reusable subword parts.\n",
      "- This reduces vocabulary size while still allowing the model to represent rare or new words by combining subwords.\n",
      "\n",
      "Examples\n",
      "- Short word: “cat” might be one token.\n",
      "- Compound or rare word: “unhappiness” might be split into tokens like “un”, “happi”, “ness”.\n",
      "- Punctuation and spaces often are tokens too: “Hello, world!” could split into “Hello”, “,”, “ world”, “!”.\n",
      "\n",
      "Tokenization process (brief)\n",
      "1. Text is split into tokens by a tokenizer (rules learned from training data).\n",
      "2. Each token is mapped to a numeric ID.\n",
      "3. The model processes the sequence of token IDs.\n",
      "4. When generating text, the model outputs token IDs that are converted back into characters/words.\n",
      "\n",
      "Why this matters for users\n",
      "- Limits and costs: Models have a maximum token limit (context window). Longer inputs use more tokens and may be truncated. Many API costs are billed per token.\n",
      "- Prompt design: Short, focused prompts use fewer tokens. If you need long output, both input and output tokens count toward limits/cost.\n",
      "- Exact counts are tokenization-dependent — a sentence can be a different number of tokens for different models/tokenizers.\n",
      "\n",
      "A quick rule of thumb\n",
      "- For English, 1 token ≈ ¾ of a word on average. So 100 tokens ≈ 75–100 words (very approximate).\n",
      "\n",
      "If you want, I can:\n",
      "- Show how a short paragraph is tokenized by a real tokenizer, or\n",
      "- Explain how token limits affect prompts and responses.\n",
      "\n",
      "\n",
      "===========================================\n",
      "\n",
      "Token Usage (Turn 1): Input=45, Output=455, Total=500\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print(f\"\\nMaking API call to {MODEL}...\")\n",
    "    # Use the Response API\n",
    "    response_1 = client.responses.create(\n",
    "        model=MODEL,\n",
    "        instructions=system_instructions,\n",
    "        input=user_input_1,\n",
    "        max_output_tokens=MAX_OUTPUT_TOKENS,\n",
    "        reasoning= {'effort':REASONING_EFFORT},\n",
    "    )\n",
    "    print(\"API call successful.\")\n",
    "    # --- Process the response from the first turn ---\n",
    "    # Extract the assistant's reply content\n",
    "    assistant_response_1 = response_1.output[1].content[0].text\n",
    "\n",
    "    # Save the response ID for conversation continuity\n",
    "    response_id_1 = response_1.id\n",
    "\n",
    "    print(\"\\nAI Tutor (Turn 1):\")\n",
    "    print(assistant_response_1)\n",
    "\n",
    "    # Print token usage for this call\n",
    "    usage_1 = response_1.usage\n",
    "    print(\"\\n\\n===========================================\")\n",
    "    print(f\"\\nToken Usage (Turn 1): Input={usage_1.input_tokens}, Output={usage_1.output_tokens}, Total={usage_1.total_tokens}\")\n",
    "\n",
    "except openai.APIError as e:\n",
    "    print(f\"OpenAI API returned an API Error: {e}\")\n",
    "except openai.AuthenticationError as e:\n",
    "    print(f\"OpenAI Authentication Error: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H8ILfDt3m-Ok"
   },
   "source": [
    "### Code Block 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I9UTAfU71Q4M",
    "outputId": "2ff59ef5-1633-4880-b9e1-f31090eda806"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Continuing Conversation: Turn 2 ---\n",
      "\n",
      "User Input (Turn 2): Thanks! So, based on your explanation, are common words like 'the' or 'is' usually single tokens?\n",
      "\n",
      "Making API call to gpt-5-mini (Turn 2)...\n",
      "API call successful.\n",
      "\n",
      "AI Tutor (Turn 2):\n",
      "Yes — common English words like \"the\" and \"is\" are almost always single tokens in commonly used tokenizers for LLMs.\n",
      "\n",
      "Why:\n",
      "- Tokenizers are trained on large text corpora and include frequent words as whole-token entries in the vocabulary. Representing highly frequent words as single tokens is efficient for both storage and processing.\n",
      "- Short, common words are simple and appear often, so the tokenizer doesn't need to split them into subword pieces.\n",
      "\n",
      "A couple of caveats:\n",
      "- Tokenization depends on the tokenizer and language. For standard English tokenizers used with popular LLMs (Byte-Pair Encoding, Unigram, or similar schemes), \"the\" and \"is\" are single tokens. In other languages or with unusual tokenizers, behavior can differ.\n",
      "- Surrounding characters can affect tokenization. For example, \"the\" in \"theater\" may be part of a larger token (\"theater\") rather than the separate token \"the\" + \"ater\". Punctuation or spaces typically separate tokens, so \"the.\" (with a period attached) might be a single token containing the punctuation, or two tokens (\"the\" + \".\") depending on the tokenizer.\n",
      "\n",
      "If you'd like, I can show exact token IDs and splits for a small sentence using a real tokenizer.\n",
      "\n",
      "\n",
      "===========================================\n",
      "\n",
      "Token Usage (Turn 2): Input=528, Output=267, Total=795\n"
     ]
    }
   ],
   "source": [
    "# --- Block 3: Second Turn - Asking a Follow-up Question ---\n",
    "print(\"\\n--- Continuing Conversation: Turn 2 ---\")\n",
    "\n",
    "# Define the user's second question\n",
    "user_input_2 = \"Thanks! So, based on your explanation, are common words like 'the' or 'is' usually single tokens?\"\n",
    "\n",
    "print(f\"\\nUser Input (Turn 2): {user_input_2}\")\n",
    "\n",
    "\n",
    "try:\n",
    "    print(f\"\\nMaking API call to {MODEL} (Turn 2)...\")\n",
    "    response_2 = client.responses.create(\n",
    "        model=MODEL,\n",
    "        instructions=system_instructions,\n",
    "        input=user_input_2,\n",
    "        max_output_tokens=MAX_OUTPUT_TOKENS,\n",
    "        reasoning= {'effort':REASONING_EFFORT},\n",
    "        previous_response_id=response_id_1  # Link to previous response for context\n",
    "    )\n",
    "    print(\"API call successful.\")\n",
    "\n",
    "    # --- Process the response from the second turn ---\n",
    "    assistant_response_2 = response_2.output[1].content[0].text\n",
    "\n",
    "    print(\"\\nAI Tutor (Turn 2):\")\n",
    "    print(assistant_response_2)\n",
    "\n",
    "    # Print token usage for this call\n",
    "    usage_2 = response_2.usage\n",
    "\n",
    "    print(\"\\n\\n===========================================\")\n",
    "    print(f\"\\nToken Usage (Turn 2): Input={usage_2.input_tokens}, Output={usage_2.output_tokens}, Total={usage_2.total_tokens}\")\n",
    "\n",
    "except openai.APIError as e:\n",
    "    print(f\"OpenAI API returned an API Error: {e}\")\n",
    "except openai.AuthenticationError as e:\n",
    "    print(f\"OpenAI Authentication Error: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oBQWJIeRoJ7n",
    "outputId": "e5a8b756-0e91-4cca-f9a1-35f4b02b57c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n",
      "455\n",
      "500\n"
     ]
    }
   ],
   "source": [
    "# # Example usage object from response_1 or response_2:\n",
    "print(usage_1.input_tokens)  # -> number of input tokens\n",
    "print(usage_1.output_tokens) # -> number of output tokens\n",
    "print(usage_1.total_tokens) # -> sum of both"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N0RI6UGhoDtC"
   },
   "source": [
    "### Code Block 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YrN9B0wQ1cLK",
    "outputId": "dcbfffa4-b7d9-47d9-cb66-40eca7775228"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Cost Calculations (GPT-5-mini) ---\n",
      "Prices: Input=$0.250/1M, Output=$2.000/1M\n"
     ]
    }
   ],
   "source": [
    "# --- Block 4: Cost Calculation Function & Example ---\n",
    "def calculate_cost(usage, input_price_per_mil, output_price_per_mil):\n",
    "    \"\"\"Calculates the cost of an API call based on token usage and prices.\n",
    "\n",
    "    Args:\n",
    "        usage: The usage object from the OpenAI response\n",
    "               (e.g., response.usage). It should have attributes\n",
    "               'input_tokens' and 'output_tokens'.\n",
    "        input_price_per_mil: Cost in USD per 1 million input tokens.\n",
    "        output_price_per_mil: Cost in USD per 1 million output tokens.\n",
    "\n",
    "    Returns:\n",
    "        The total cost in USD for the API call, or None if usage is invalid.\n",
    "    \"\"\"\n",
    "    if not usage or not hasattr(usage, 'input_tokens') or not hasattr(usage, 'output_tokens'):\n",
    "        print(\"Warning: Invalid usage object provided for cost calculation.\")\n",
    "        return None\n",
    "\n",
    "    input_cost = (usage.input_tokens / 1_000_000) * input_price_per_mil\n",
    "    output_cost = (usage.output_tokens / 1_000_000) * output_price_per_mil\n",
    "    total_cost = input_cost + output_cost\n",
    "\n",
    "    return total_cost\n",
    "\n",
    "# --- Current Prices for GPT-5-mini ---\n",
    "# IMPORTANT: Always verify at https://openai.com/pricing\n",
    "PRICE_INPUT_PER_MIL = 0.250\n",
    "PRICE_OUTPUT_PER_MIL = 2.000\n",
    "\n",
    "print(f\"\\n--- Cost Calculations (GPT-5-mini) ---\")\n",
    "print(f\"Prices: Input=${PRICE_INPUT_PER_MIL:.3f}/1M, Output=${PRICE_OUTPUT_PER_MIL:.3f}/1M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vzk2VbYfW8sx",
    "outputId": "fe973c3f-fabb-4d8d-8f1d-24db130ccd59"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cost for Turn 1:\n",
      "  Input Tokens: 45, Output Tokens: 455\n",
      "  Total Cost: $0.00092125\n",
      "\n",
      "Cost for Turn 2:\n",
      "  Input Tokens: 528, Output Tokens: 252\n",
      "  Total Cost: $0.00063600\n",
      "\n",
      "Total Conversation Cost (Turn 1 + Turn 2): $0.00155725\n"
     ]
    }
   ],
   "source": [
    "# Calculate cost for Turn 1\n",
    "try:\n",
    "    if 'usage_1' in locals():\n",
    "        cost_1 = calculate_cost(usage_1, PRICE_INPUT_PER_MIL, PRICE_OUTPUT_PER_MIL)\n",
    "        if cost_1 is not None:\n",
    "            print(f\"\\nCost for Turn 1:\")\n",
    "            print(f\"  Input Tokens: {usage_1.input_tokens}, Output Tokens: {usage_1.output_tokens}\")\n",
    "            print(f\"  Total Cost: ${cost_1:.8f}\")\n",
    "    else:\n",
    "        print(\"\\nSkipping Turn 1 cost calculation (usage_1 not found).\")\n",
    "\n",
    "    # Calculate cost for Turn 2\n",
    "    if 'usage_2' in locals():\n",
    "        cost_2 = calculate_cost(usage_2, PRICE_INPUT_PER_MIL, PRICE_OUTPUT_PER_MIL)\n",
    "        if cost_2 is not None:\n",
    "            print(f\"\\nCost for Turn 2:\")\n",
    "            print(f\"  Input Tokens: {usage_2.input_tokens}, Output Tokens: {usage_2.output_tokens}\")\n",
    "            print(f\"  Total Cost: ${cost_2:.8f}\")\n",
    "    else:\n",
    "        print(\"\\nSkipping Turn 2 cost calculation (usage_2 not found).\")\n",
    "\n",
    "    # Calculate total conversation cost\n",
    "    if 'cost_1' in locals() and 'cost_2' in locals() and cost_1 is not None and cost_2 is not None:\n",
    "        total_conversation_cost = cost_1 + cost_2\n",
    "        print(f\"\\nTotal Conversation Cost (Turn 1 + Turn 2): ${total_conversation_cost:.8f}\")\n",
    "\n",
    "except NameError as e:\n",
    "    print(f\"\\nCould not calculate costs, a required variable is missing: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during cost calculation: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gLFCrHGN0vmm"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
