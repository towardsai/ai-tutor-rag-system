{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-XjXwFZPMQ6O"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/towardsai/ai-tutor-rag-system/blob/main/notebooks/Long_Context_Caching_vs_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v9bpz99INAc1"
   },
   "source": [
    "## Install Packages and Setup Variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "BeuFJKlj9jKz",
    "outputId": "58b6ad29-8f52-481a-f41f-f7514506cf54"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.3/244.3 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.9/11.9 MB\u001b[0m \u001b[31m101.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m303.3/303.3 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.1/56.1 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.9/63.9 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.5/310.5 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m52.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.4/144.4 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -q google-genai==1.35.0 llama-index-llms-google-genai==0.5.0 \\\n",
    "                llama-index==0.14.0 openai==1.107.0 jedi==0.19.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CWholrWlt2OQ"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Set the following API Keys in the Python environment. Will be used later.\n",
    "# We use OpenAI for the embedding model and Gemini-2.5-flash as our LLM.\n",
    "\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"<YOUR_OPENAI_KEY>\"\n",
    "# os.environ[\"GOOGLE_API_KEY\"] = \"<YOUR_API_KEY>\"\n",
    "\n",
    "from google.colab import userdata\n",
    "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
    "os.environ[\"GOOGLE_API_KEY\"] = userdata.get('GOOGLE_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VOf2NodvWnKu"
   },
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f5eV5EnvNCMM"
   },
   "source": [
    "# Load Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q-7mRQ-mNJlm"
   },
   "source": [
    "## Download\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3PsdOdMUNmEi"
   },
   "source": [
    "The dataset includes a subset of the documentation from the Llama-index library.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3ImRCP7pACaI",
    "outputId": "97789195-7613-4174-c4a8-9e674935cf12"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   115  100   115    0     0    495      0 --:--:-- --:--:-- --:--:--   497\n",
      "100  570k  100  570k    0     0  1291k      0 --:--:-- --:--:-- --:--:-- 1291k\n"
     ]
    }
   ],
   "source": [
    "!curl -L -o ./llama_index_150k.jsonl https://huggingface.co/datasets/towardsai-buster/llama-index-docs/raw/main/llama_index_data_150k.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bZZLK_wyEc-L"
   },
   "source": [
    "## Read File and create LlamaIndex Documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "miUqycqAEfr7",
    "outputId": "988594d2-e445-44ca-fefa-47bc2e2d1cfb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 56\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import Document\n",
    "import json\n",
    "\n",
    "\n",
    "def create_docs(input_file: str) -> list[Document]:\n",
    "    documents = []\n",
    "    with open(input_file, \"r\") as f:\n",
    "        for idx, line in enumerate(f, start=1):\n",
    "\n",
    "          data = json.loads(line)\n",
    "\n",
    "          required_keys = {\"doc_id\", \"content\", \"url\", \"name\", \"tokens\", \"source\"}\n",
    "          if not required_keys.issubset(data):\n",
    "              print(f\"Missing keys in line {idx}: {required_keys - set(data)}\")\n",
    "              continue\n",
    "\n",
    "          documents.append(\n",
    "              Document(\n",
    "                  doc_id=data[\"doc_id\"],\n",
    "                  text=data[\"content\"],\n",
    "                  metadata={  # type: ignore\n",
    "                      \"url\": data[\"url\"],\n",
    "                      \"title\": data[\"name\"],\n",
    "                      \"tokens\": data[\"tokens\"],\n",
    "                      \"source\": data[\"source\"],\n",
    "                  },\n",
    "                  excluded_llm_metadata_keys=[\n",
    "                      \"title\",\n",
    "                      \"tokens\",\n",
    "                      \"source\",\n",
    "                  ],\n",
    "                  excluded_embed_metadata_keys=[\n",
    "                      \"url\",\n",
    "                      \"tokens\",\n",
    "                      \"source\",\n",
    "                  ],\n",
    "              )\n",
    "          )\n",
    "\n",
    "    return documents\n",
    "\n",
    "\n",
    "# Convert the texts to Document objects.\n",
    "documents = create_docs(\"llama_index_150k.jsonl\")\n",
    "print(f\"Number of documents: {len(documents)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f86yksB9K571"
   },
   "source": [
    "# Generate Embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "d495b1369a584fa9bb70604141cdaa5a",
      "448f1399383b4138805c3e629694e06c",
      "826d33cf073a41118c987059d9b0cff0",
      "d25c30b452db4180956d559084f7058d",
      "3fae8c5d806e4539ad429fed79bd0b65",
      "7ff473bee19c4df682000ad18cefa83e",
      "ef088b3aaf3c4e21b83a395b7e38beaf",
      "8fdfbce52b4447d2a2c4b11c9aa32d0a",
      "919817d0a97747a585262bec2ab19d97",
      "bc6209ca50674fe0b3b7d06629999617",
      "837acffc66464f92a5f58205a00329c9",
      "b05e80430942486d8cb9139a9b93b158",
      "91a0e22cc8f047ddb593934aa988fcbe",
      "5dedad13b89a407ebcd085cec53781e9",
      "269569127219431ca5ca99a45a416f20",
      "021bb47f4b0140b6ac563c5f52979b57",
      "4938f98717ae4984af1067d986ed3e15",
      "1aaaad4bb26446b5ac98083fc071e886",
      "489ce02daae8477e91a8c56f968e2fe6",
      "63404c8d9f764a4fb8d6c862f7f2b50a",
      "fe4cc714376542aea642bd374673e1ba",
      "b5a5e2e28e39414b962e26a60590d05e"
     ]
    },
    "id": "Bsa7Q-DoNWBk",
    "outputId": "a410cf58-51b8-4793-d732-0895436b59b0"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d495b1369a584fa9bb70604141cdaa5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/56 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b05e80430942486d8cb9139a9b93b158",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/447 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "\n",
    "# Build index / generate embeddings using OpenAI embedding model\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    "    embed_model=OpenAIEmbedding(model=\"text-embedding-3-small\"),\n",
    "    transformations=[SentenceSplitter(chunk_size=512, chunk_overlap=128)],\n",
    "    show_progress=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3DoUxd8KK--Q"
   },
   "source": [
    "# Query Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bUaNH97dEfh9"
   },
   "outputs": [],
   "source": [
    "# Define a query engine that is responsible for retrieving related pieces of text,\n",
    "# and using a LLM to formulate the final answer.\n",
    "\n",
    "from llama_index.llms.google_genai import GoogleGenAI\n",
    "\n",
    "llm = GoogleGenAI(model=\"models/gemini-2.5-flash\", temperature=1)\n",
    "\n",
    "query_engine = index.as_query_engine(llm=llm, similarity_top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "KHK4V_GRR6ZG",
    "outputId": "fe22c9c4-5d42-4f07-d4bb-5e8e4fb486c0"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "A query engine can be set up in several ways, depending on the complexity and specific needs of the application.\n",
       "\n",
       "The simplest approach is to have an index create a query engine directly:\n",
       "```python\n",
       "query_engine = index.as_query_engine()\n",
       "```\n",
       "\n",
       "For more granular control over the querying process, including retrieval, postprocessing, and response synthesis, a `RetrieverQueryEngine` can be assembled:\n",
       "```python\n",
       "from llama_index.core import VectorStoreIndex, get_response_synthesizer\n",
       "from llama_index.core.retrievers import VectorIndexRetriever\n",
       "from llama_index.core.query_engine import RetrieverQueryEngine\n",
       "from llama_index.core.postprocessor import SimilarityPostprocessor\n",
       "\n",
       "# build index (assuming 'documents' are already loaded)\n",
       "index = VectorStoreIndex.from_documents(documents)\n",
       "\n",
       "# configure retriever\n",
       "retriever = VectorIndexRetriever(\n",
       "    index=index,\n",
       "    similarity_top_k=10,\n",
       ")\n",
       "\n",
       "# configure response synthesizer\n",
       "response_synthesizer = get_response_synthesizer()\n",
       "\n",
       "# assemble query engine\n",
       "query_engine = RetrieverQueryEngine(\n",
       "    retriever=retriever,\n",
       "    response_synthesizer=response_synthesizer,\n",
       "    node_postprocessors=[SimilarityPostprocessor(similarity_cutoff=0.7)],\n",
       ")\n",
       "```\n",
       "\n",
       "For natural language SQL queries against known tables, the `NLSQLTableQueryEngine` can be used:\n",
       "```python\n",
       "from llama_index import SQLDatabase\n",
       "from llama_index.indices.struct_store.sql_query import NLSQLTableQueryEngine\n",
       "\n",
       "sql_database = SQLDatabase(engine) # 'engine' needs to be defined\n",
       "query_engine = NLSQLTableQueryEngine(\n",
       "    sql_database=sql_database,\n",
       "    tables=[\"github_issues\", \"github_comments\", \"github_users\"],\n",
       ")\n",
       "```\n",
       "\n",
       "For multi-document queries or those requiring sub-questions against different data sources, the `SubQuestionQueryEngine` can be defined with a list of `QueryEngineTool` objects:\n",
       "```python\n",
       "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
       "from llama_index.core.query_engine import SubQuestionQueryEngine\n",
       "\n",
       "# Assuming sept_engine, june_engine, march_engine are pre-defined query engines\n",
       "query_engine_tools = [\n",
       "    QueryEngineTool(\n",
       "        query_engine=sept_engine,\n",
       "        metadata=ToolMetadata(name=\"sept_22\", description=\"...\"),\n",
       "    ),\n",
       "    QueryEngineTool(\n",
       "        query_engine=june_engine,\n",
       "        metadata=ToolMetadata(name=\"june_22\", description=\"...\"),\n",
       "    ),\n",
       "    QueryEngineTool(\n",
       "        query_engine=march_engine,\n",
       "        metadata=ToolMetadata(name=\"march_22\", description=\"...\"),\n",
       "    ),\n",
       "]\n",
       "\n",
       "query_engine = SubQuestionQueryEngine.from_defaults(\n",
       "    query_engine_tools=query_engine_tools\n",
       ")\n",
       "```\n",
       "\n",
       "Additionally, when dealing with structured data and not knowing which table to use beforehand, a `SQLTableRetrieverQueryEngine` can be constructed by passing a `SQLDatabase` and a retriever built from an `ObjectIndex` that stores table schemas."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken:  8.35205078125\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "response = query_engine.query(\"How to setup a query engine in code?\")\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "display(Markdown(response.response))\n",
    "print(\"time taken: \", end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 310
    },
    "id": "S-BmyTBbNd9y",
    "outputId": "14b5aafb-ddbf-44e3-e9ad-0321d452964d"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "To set up an agent, begin by installing the necessary libraries, `llama-index` and `python-dotenv`. You will also need to configure an API key for your chosen language model; for OpenAI, this involves creating a `.env` file in your project's root directory containing `OPENAI_API_KEY=sk-proj-xxxx`.\n",
       "\n",
       "Next, import the required components in your Python code: `load_dotenv` to load environment variables, `ReActAgent` for the agent itself, `OpenAI` for the language model (or `Ollama` if using a local model), and `FunctionTool` for defining tools.\n",
       "\n",
       "Define your custom tools by creating Python functions (e.g., `multiply`, `add`) and then wrapping them using `FunctionTool.from_defaults`. These tools are standard Python functions, where docstrings provide metadata for the agent to understand their purpose.\n",
       "\n",
       "After defining tools, initialize your Large Language Model (LLM). For example, with OpenAI, you would create an instance like `llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0)`. If opting for local models, you would use `Ollama` after installing `llama-index-llms-ollama` and running a model like `mixtral:8x7b`.\n",
       "\n",
       "Finally, initialize the agent by instantiating `ReActAgent.from_tools`, passing it an array of your created tools and the initialized LLM. Setting `verbose=True` can help in observing the agent's internal thought process. An example setup would be `agent = ReActAgent.from_tools([multiply_tool, add_tool], llm=llm, verbose=True)`."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken:  3.91007661819458\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "response = query_engine.query(\"How to setup an agent in code?\")\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "display(Markdown(response.response))\n",
    "print(\"time taken: \", end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b_55vnPoSlID"
   },
   "source": [
    "# Setup Long Context Caching\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oBSZTxjfSlID"
   },
   "source": [
    "For this section, we will be using the Gemini API\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GgY1ySml4wci"
   },
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "client = genai.Client(api_key=userdata.get('GOOGLE_API_KEY'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Dep30SSV4wYR",
    "outputId": "9daf0783-62d0-4256-f7a5-1a9fd572eacf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contents saved to llama_index_contents.txt\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def create_text_file(input_file: str, output_file: str) -> None:\n",
    "    with open(input_file, \"r\") as f, open(output_file, \"w\") as out:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            out.write(data[\"content\"] + \"\\n\\n\")  # Add two newlines between documents\n",
    "\n",
    "    print(f\"Contents saved to {output_file}\")\n",
    "\n",
    "\n",
    "create_text_file(\"llama_index_150k.jsonl\", \"llama_index_contents.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FEivUijQ5CEq"
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "# Update the cache's time-to-live (ttl)\n",
    "\n",
    "ttl = f\"{int(datetime.timedelta(minutes=10).total_seconds())}s\"\n",
    "\n",
    "document = client.files.upload(file=\"llama_index_contents.txt\")\n",
    "\n",
    "model_name = \"models/gemini-2.0-flash-001\"\n",
    "\n",
    "cache = client.caches.create(\n",
    "    model=model_name,\n",
    "    config=types.CreateCachedContentConfig(\n",
    "        contents=[document],\n",
    "        system_instruction=\"You answer questions about the LlamaIndex framework.\",\n",
    "        ttl=ttl,\n",
    "    ),\n",
    ")\n",
    "\n",
    "# # To Update the cache\n",
    "\n",
    "# # Alternatively, you update the expire_time directly\n",
    "# # Update the expire_time directly in valid RFC 3339 format (UTC with a \"Z\" suffix)\n",
    "\n",
    "# expire_time = (\n",
    "#     (\n",
    "#         datetime.datetime.now(datetime.timezone.utc)\n",
    "#         + datetime.timedelta(minutes=15)\n",
    "#     )\n",
    "#     .isoformat()\n",
    "#     .replace(\"+00:00\", \"Z\")\n",
    "# )\n",
    "\n",
    "# client.caches.update(\n",
    "#              name=cache.name,\n",
    "#             config=types.UpdateCachedContentConfig(expire_time=expire_time),\n",
    "# )\n",
    "\n",
    "# # To delete cache\n",
    "\n",
    "# client.caches.delete(name=cache.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "HHyk7MDr5CAk",
    "outputId": "2b4ecd3d-7dbc-4bd0-f218-07e7fb735cbe"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Okay, I can help you understand LlamaParse and how to set it up.\n",
       "\n",
       "**What is LlamaParse?**\n",
       "\n",
       "LlamaParse is a state-of-the-art document parsing solution developed by LlamaIndex. It's designed to reliably extract text and structure from documents, especially those with complex layouts like PDFs. It's offered as part of LlamaCloud and also as a self-serve API. The goal is to provide production-quality data for your LLM applications.\n",
       "***Key Features and Benefits:***\n",
       "\n",
       "1.  **High Accuracy:** LlamaParse excels at accurately extracting content from documents, even those with complex layouts (multi-column layouts, tables, figures, etc.) where other parsers often fail.\n",
       "2.  **Structure Preservation:** It attempts to preserve the structural elements of the document, not just the raw text. This means it can help you understand headings, tables, and other layout features.\n",
       "3.  **Production-Ready Data:** The parsing quality makes it suitable for use in production LLM applications, where the accuracy of the data is crucial.\n",
       "4.  **Easy Integration:** LlamaParse is designed to integrate seamlessly with the LlamaIndex framework.\n",
       "\n",
       "**How to set up LlamaParse:**\n",
       "***Option 1: Using LlamaCloud (Hosted Service)***\n",
       "\n",
       "This is the easiest way to get started, especially for enterprise developers.\n",
       "\n",
       "**Steps:**\n",
       "\n",
       "1.  **Sign Up:**  Go to the LlamaCloud website ([https://cloud.llamaindex.ai/](https://cloud.llamaindex.ai/)) and sign up for an account.\n",
       "2.  **Get an API Key:** Once you're logged in, you'll find your LlamaCloud API key in your account dashboard.\n",
       "3.  **Set the API Key in your Environment:**  In your Python environment, set the `LLAMA_CLOUD_API_KEY` environment variable:\n",
       "\n",
       "```bash\n",
       "export LLAMA_CLOUD_API_KEY=llx-your_api_key\n",
       "```\n",
       "\n",
       "4.  **Install `llama-parse` Package:**\n",
       "```bash\n",
       "pip install llama-parse\n",
       "```\n",
       "5.  **Using the LlamaParse Loader**\n",
       "```python\n",
       "from llama_parse import LlamaParse\n",
       "documents = LlamaParse(result_type=\"markdown\").load_data(\n",
       "    \"./data/2023_canadian_budget.pdf\"\n",
       ")\n",
       "```\n",
       "***Option 2: Managed services***\n",
       "\n",
       "If you're an enterprise developer, check out [**LlamaCloud**](https://llamaindex.ai/enterprise). It is an end-to-end managed service for data parsing, ingestion, indexing, and retrieval, allowing you to get production-quality data for your production LLM application. It's available both hosted on our servers or as a self-hosted solution.\n",
       "\n",
       "***LlamaParse***\n",
       "\n",
       "LlamaParse is our state-of-the-art document parsing solution. It's available as part of LlamaCloud and also available as a self-serve API. You can [sign up](https://cloud.llamaindex.ai/) and parse up to 1000 pages/day for free, or enter a credit card for unlimited parsing. [Learn more](https://llamaindex.ai/enterprise).\n",
       "\n",
       "**Example Code (Python)**\n",
       "\n",
       "Here's how you would typically use LlamaParse within a LlamaIndex pipeline:\n",
       "\n",
       "```python\n",
       "import os\n",
       "from llama_index.core import VectorStoreIndex\n",
       "from llama_index.core import SimpleDirectoryReader, Settings\n",
       "\n",
       "# Set the API key (if you haven't already)\n",
       "os.environ[\"LLAMA_CLOUD_API_KEY\"] = \"llx-your_api_key\" #replace with your actual API key\n",
       "\n",
       "from llama_parse import LlamaParse\n",
       "\n",
       "# Load data using LlamaParse\n",
       "documents = LlamaParse(result_type=\"markdown\").load_data(\n",
       "    \"./data/your_document.pdf\"  # Replace with your PDF file\n",
       ")\n",
       "\n",
       "# Create an index from the parsed documents\n",
       "index = VectorStoreIndex.from_documents(documents)\n",
       "\n",
       "# Now you can query the index as usual\n",
       "query_engine = index.as_query_engine()\n",
       "response = query_engine.query(\"Your question about the document\")\n",
       "print(response)\n",
       "```\n",
       "\n",
       "**Important Notes:**\n",
       "\n",
       "*   **Cost:**  Be aware of the pricing for LlamaParse, especially if you're using the self-serve API for a large volume of documents.\n",
       "*   **Alternatives:**  If LlamaParse doesn't meet your needs (e.g., due to cost or specific document formats), explore other data connectors/readers in LlamaHub ([https://llamahub.ai/](https://llamahub.ai/)). There are many options for different data sources and parsing methods.\n",
       "*   **Error Handling:** Implement robust error handling in your code to catch potential issues during the parsing process.\n",
       "\n",
       "Let me know if you have other questions.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken:  6.857192277908325\n"
     ]
    }
   ],
   "source": [
    "# Use the cache for generation\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=model_name,\n",
    "    contents=\"What is LlamaParse, How to setup? Explain detail\",\n",
    "    config=types.GenerateContentConfig(cached_content=cache.name),\n",
    ")\n",
    "\n",
    "end = time.time()\n",
    "display(Markdown(response.text))\n",
    "print(\"time taken: \", end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qbyW-W7P5B8w",
    "outputId": "1af264b5-a315-4634-c2de-a141db229d7e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerateContentResponseUsageMetadata(\n",
       "  cache_tokens_details=[\n",
       "    ModalityTokenCount(\n",
       "      modality=<MediaModality.TEXT: 'TEXT'>,\n",
       "      token_count=212635\n",
       "    ),\n",
       "  ],\n",
       "  cached_content_token_count=212635,\n",
       "  candidates_token_count=1044,\n",
       "  candidates_tokens_details=[\n",
       "    ModalityTokenCount(\n",
       "      modality=<MediaModality.TEXT: 'TEXT'>,\n",
       "      token_count=1044\n",
       "    ),\n",
       "  ],\n",
       "  prompt_token_count=212646,\n",
       "  prompt_tokens_details=[\n",
       "    ModalityTokenCount(\n",
       "      modality=<MediaModality.TEXT: 'TEXT'>,\n",
       "      token_count=212646\n",
       "    ),\n",
       "  ],\n",
       "  total_token_count=213690\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.usage_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "skIXhzGyLixv"
   },
   "source": [
    "## First token response time in Straming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "dIIspB7d4v3-",
    "outputId": "5b3730c3-ffa5-4dbd-e209-337ede79081f"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken:  3.200701951980591\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=model_name,\n",
    "    contents=\"How to setup a Router query engine?\",\n",
    "    config=types.GenerateContentConfig(cached_content=cache.name,max_output_tokens=1),\n",
    ")\n",
    "end = time.time()\n",
    "display(Markdown(response.text))\n",
    "print(\"time taken: \", end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U3PKSPhof079"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
