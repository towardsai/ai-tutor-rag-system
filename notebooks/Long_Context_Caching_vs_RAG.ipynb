{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9bpz99INAc1"
      },
      "source": [
        "# Install Packages and Setup Variables\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "BeuFJKlj9jKz",
        "outputId": "58b6ad29-8f52-481a-f41f-f7514506cf54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.3/244.3 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.9/11.9 MB\u001b[0m \u001b[31m101.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m303.3/303.3 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.1/56.1 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.9/63.9 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.5/310.5 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m52.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.4/144.4 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q google-genai==1.35.0 llama-index-llms-google-genai==0.5.0 \\\n",
        "                llama-index==0.14.0 openai==1.107.0 jedi==0.19.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "CWholrWlt2OQ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "# Set the following API Keys in the Python environment. Will be used later.\n",
        "# We use OpenAI for the embedding model and Gemini-2.5-flash as our LLM.\n",
        "\n",
        "# os.environ[\"OPENAI_API_KEY\"] = \"<YOUR_OPENAI_KEY>\"\n",
        "# os.environ[\"GOOGLE_API_KEY\"] = \"<YOUR_API_KEY>\"\n",
        "\n",
        "from google.colab import userdata\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
        "os.environ[\"GOOGLE_API_KEY\"] = userdata.get('GOOGLE_API_KEY')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "VOf2NodvWnKu"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5eV5EnvNCMM"
      },
      "source": [
        "# Load Dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-7mRQ-mNJlm"
      },
      "source": [
        "## Download\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3PsdOdMUNmEi"
      },
      "source": [
        "The dataset includes a subset of the documentation from the Llama-index library.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ImRCP7pACaI",
        "outputId": "97789195-7613-4174-c4a8-9e674935cf12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   115  100   115    0     0    495      0 --:--:-- --:--:-- --:--:--   497\n",
            "100  570k  100  570k    0     0  1291k      0 --:--:-- --:--:-- --:--:-- 1291k\n"
          ]
        }
      ],
      "source": [
        "!curl -L -o ./llama_index_150k.jsonl https://huggingface.co/datasets/towardsai-buster/llama-index-docs/raw/main/llama_index_data_150k.jsonl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZZLK_wyEc-L"
      },
      "source": [
        "## Read File and create LlamaIndex Documents\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "miUqycqAEfr7",
        "outputId": "988594d2-e445-44ca-fefa-47bc2e2d1cfb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of documents: 56\n"
          ]
        }
      ],
      "source": [
        "from llama_index.core import Document\n",
        "import json\n",
        "\n",
        "\n",
        "def create_docs(input_file: str) -> list[Document]:\n",
        "    documents = []\n",
        "    with open(input_file, \"r\") as f:\n",
        "        for idx, line in enumerate(f, start=1):\n",
        "\n",
        "          data = json.loads(line)\n",
        "\n",
        "          required_keys = {\"doc_id\", \"content\", \"url\", \"name\", \"tokens\", \"source\"}\n",
        "          if not required_keys.issubset(data):\n",
        "              print(f\"Missing keys in line {idx}: {required_keys - set(data)}\")\n",
        "              continue\n",
        "\n",
        "          documents.append(\n",
        "              Document(\n",
        "                  doc_id=data[\"doc_id\"],\n",
        "                  text=data[\"content\"],\n",
        "                  metadata={  # type: ignore\n",
        "                      \"url\": data[\"url\"],\n",
        "                      \"title\": data[\"name\"],\n",
        "                      \"tokens\": data[\"tokens\"],\n",
        "                      \"source\": data[\"source\"],\n",
        "                  },\n",
        "                  excluded_llm_metadata_keys=[\n",
        "                      \"title\",\n",
        "                      \"tokens\",\n",
        "                      \"source\",\n",
        "                  ],\n",
        "                  excluded_embed_metadata_keys=[\n",
        "                      \"url\",\n",
        "                      \"tokens\",\n",
        "                      \"source\",\n",
        "                  ],\n",
        "              )\n",
        "          )\n",
        "\n",
        "    return documents\n",
        "\n",
        "\n",
        "# Convert the texts to Document objects.\n",
        "documents = create_docs(\"llama_index_150k.jsonl\")\n",
        "print(f\"Number of documents: {len(documents)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f86yksB9K571"
      },
      "source": [
        "# Generate Embedding\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "d495b1369a584fa9bb70604141cdaa5a",
            "448f1399383b4138805c3e629694e06c",
            "826d33cf073a41118c987059d9b0cff0",
            "d25c30b452db4180956d559084f7058d",
            "3fae8c5d806e4539ad429fed79bd0b65",
            "7ff473bee19c4df682000ad18cefa83e",
            "ef088b3aaf3c4e21b83a395b7e38beaf",
            "8fdfbce52b4447d2a2c4b11c9aa32d0a",
            "919817d0a97747a585262bec2ab19d97",
            "bc6209ca50674fe0b3b7d06629999617",
            "837acffc66464f92a5f58205a00329c9",
            "b05e80430942486d8cb9139a9b93b158",
            "91a0e22cc8f047ddb593934aa988fcbe",
            "5dedad13b89a407ebcd085cec53781e9",
            "269569127219431ca5ca99a45a416f20",
            "021bb47f4b0140b6ac563c5f52979b57",
            "4938f98717ae4984af1067d986ed3e15",
            "1aaaad4bb26446b5ac98083fc071e886",
            "489ce02daae8477e91a8c56f968e2fe6",
            "63404c8d9f764a4fb8d6c862f7f2b50a",
            "fe4cc714376542aea642bd374673e1ba",
            "b5a5e2e28e39414b962e26a60590d05e"
          ]
        },
        "id": "Bsa7Q-DoNWBk",
        "outputId": "a410cf58-51b8-4793-d732-0895436b59b0"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Parsing nodes:   0%|          | 0/56 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d495b1369a584fa9bb70604141cdaa5a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating embeddings:   0%|          | 0/447 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b05e80430942486d8cb9139a9b93b158"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from llama_index.core import VectorStoreIndex\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "\n",
        "\n",
        "# Build index / generate embeddings using OpenAI embedding model\n",
        "index = VectorStoreIndex.from_documents(\n",
        "    documents,\n",
        "    embed_model=OpenAIEmbedding(model=\"text-embedding-3-small\"),\n",
        "    transformations=[SentenceSplitter(chunk_size=512, chunk_overlap=128)],\n",
        "    show_progress=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3DoUxd8KK--Q"
      },
      "source": [
        "# Query Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "bUaNH97dEfh9"
      },
      "outputs": [],
      "source": [
        "# Define a query engine that is responsible for retrieving related pieces of text,\n",
        "# and using a LLM to formulate the final answer.\n",
        "\n",
        "from llama_index.llms.google_genai import GoogleGenAI\n",
        "\n",
        "llm = GoogleGenAI(model=\"models/gemini-2.5-flash\", temperature=1)\n",
        "\n",
        "query_engine = index.as_query_engine(llm=llm, similarity_top_k=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "KHK4V_GRR6ZG",
        "outputId": "fe22c9c4-5d42-4f07-d4bb-5e8e4fb486c0"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "A query engine can be set up in several ways, depending on the complexity and specific needs of the application.\n\nThe simplest approach is to have an index create a query engine directly:\n```python\nquery_engine = index.as_query_engine()\n```\n\nFor more granular control over the querying process, including retrieval, postprocessing, and response synthesis, a `RetrieverQueryEngine` can be assembled:\n```python\nfrom llama_index.core import VectorStoreIndex, get_response_synthesizer\nfrom llama_index.core.retrievers import VectorIndexRetriever\nfrom llama_index.core.query_engine import RetrieverQueryEngine\nfrom llama_index.core.postprocessor import SimilarityPostprocessor\n\n# build index (assuming 'documents' are already loaded)\nindex = VectorStoreIndex.from_documents(documents)\n\n# configure retriever\nretriever = VectorIndexRetriever(\n    index=index,\n    similarity_top_k=10,\n)\n\n# configure response synthesizer\nresponse_synthesizer = get_response_synthesizer()\n\n# assemble query engine\nquery_engine = RetrieverQueryEngine(\n    retriever=retriever,\n    response_synthesizer=response_synthesizer,\n    node_postprocessors=[SimilarityPostprocessor(similarity_cutoff=0.7)],\n)\n```\n\nFor natural language SQL queries against known tables, the `NLSQLTableQueryEngine` can be used:\n```python\nfrom llama_index import SQLDatabase\nfrom llama_index.indices.struct_store.sql_query import NLSQLTableQueryEngine\n\nsql_database = SQLDatabase(engine) # 'engine' needs to be defined\nquery_engine = NLSQLTableQueryEngine(\n    sql_database=sql_database,\n    tables=[\"github_issues\", \"github_comments\", \"github_users\"],\n)\n```\n\nFor multi-document queries or those requiring sub-questions against different data sources, the `SubQuestionQueryEngine` can be defined with a list of `QueryEngineTool` objects:\n```python\nfrom llama_index.core.tools import QueryEngineTool, ToolMetadata\nfrom llama_index.core.query_engine import SubQuestionQueryEngine\n\n# Assuming sept_engine, june_engine, march_engine are pre-defined query engines\nquery_engine_tools = [\n    QueryEngineTool(\n        query_engine=sept_engine,\n        metadata=ToolMetadata(name=\"sept_22\", description=\"...\"),\n    ),\n    QueryEngineTool(\n        query_engine=june_engine,\n        metadata=ToolMetadata(name=\"june_22\", description=\"...\"),\n    ),\n    QueryEngineTool(\n        query_engine=march_engine,\n        metadata=ToolMetadata(name=\"march_22\", description=\"...\"),\n    ),\n]\n\nquery_engine = SubQuestionQueryEngine.from_defaults(\n    query_engine_tools=query_engine_tools\n)\n```\n\nAdditionally, when dealing with structured data and not knowing which table to use beforehand, a `SQLTableRetrieverQueryEngine` can be constructed by passing a `SQLDatabase` and a retriever built from an `ObjectIndex` that stores table schemas."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time taken:  8.35205078125\n"
          ]
        }
      ],
      "source": [
        "start = time.time()\n",
        "\n",
        "response = query_engine.query(\"How to setup a query engine in code?\")\n",
        "\n",
        "end = time.time()\n",
        "\n",
        "display(Markdown(response.response))\n",
        "print(\"time taken: \", end - start)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310
        },
        "id": "S-BmyTBbNd9y",
        "outputId": "14b5aafb-ddbf-44e3-e9ad-0321d452964d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "To set up an agent, begin by installing the necessary libraries, `llama-index` and `python-dotenv`. You will also need to configure an API key for your chosen language model; for OpenAI, this involves creating a `.env` file in your project's root directory containing `OPENAI_API_KEY=sk-proj-xxxx`.\n\nNext, import the required components in your Python code: `load_dotenv` to load environment variables, `ReActAgent` for the agent itself, `OpenAI` for the language model (or `Ollama` if using a local model), and `FunctionTool` for defining tools.\n\nDefine your custom tools by creating Python functions (e.g., `multiply`, `add`) and then wrapping them using `FunctionTool.from_defaults`. These tools are standard Python functions, where docstrings provide metadata for the agent to understand their purpose.\n\nAfter defining tools, initialize your Large Language Model (LLM). For example, with OpenAI, you would create an instance like `llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0)`. If opting for local models, you would use `Ollama` after installing `llama-index-llms-ollama` and running a model like `mixtral:8x7b`.\n\nFinally, initialize the agent by instantiating `ReActAgent.from_tools`, passing it an array of your created tools and the initialized LLM. Setting `verbose=True` can help in observing the agent's internal thought process. An example setup would be `agent = ReActAgent.from_tools([multiply_tool, add_tool], llm=llm, verbose=True)`."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time taken:  3.91007661819458\n"
          ]
        }
      ],
      "source": [
        "start = time.time()\n",
        "\n",
        "response = query_engine.query(\"How to setup an agent in code?\")\n",
        "\n",
        "end = time.time()\n",
        "\n",
        "display(Markdown(response.response))\n",
        "print(\"time taken: \", end - start)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_55vnPoSlID"
      },
      "source": [
        "# Setup Long Context Caching\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBSZTxjfSlID"
      },
      "source": [
        "For this section, we will be using the Gemini API\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "GgY1ySml4wci"
      },
      "outputs": [],
      "source": [
        "from google import genai\n",
        "from google.genai import types\n",
        "\n",
        "client = genai.Client(api_key=userdata.get('GOOGLE_API_KEY'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dep30SSV4wYR",
        "outputId": "9daf0783-62d0-4256-f7a5-1a9fd572eacf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Contents saved to llama_index_contents.txt\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "\n",
        "def create_text_file(input_file: str, output_file: str) -> None:\n",
        "    with open(input_file, \"r\") as f, open(output_file, \"w\") as out:\n",
        "        for line in f:\n",
        "            data = json.loads(line)\n",
        "            out.write(data[\"content\"] + \"\\n\\n\")  # Add two newlines between documents\n",
        "\n",
        "    print(f\"Contents saved to {output_file}\")\n",
        "\n",
        "\n",
        "create_text_file(\"llama_index_150k.jsonl\", \"llama_index_contents.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "FEivUijQ5CEq"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "\n",
        "# Update the cache's time-to-live (ttl)\n",
        "\n",
        "ttl = f\"{int(datetime.timedelta(minutes=10).total_seconds())}s\"\n",
        "\n",
        "document = client.files.upload(file=\"llama_index_contents.txt\")\n",
        "\n",
        "model_name = \"models/gemini-2.0-flash-001\"\n",
        "\n",
        "cache = client.caches.create(\n",
        "    model=model_name,\n",
        "    config=types.CreateCachedContentConfig(\n",
        "        contents=[document],\n",
        "        system_instruction=\"You answer questions about the LlamaIndex framework.\",\n",
        "        ttl=ttl,\n",
        "    ),\n",
        ")\n",
        "\n",
        "# # To Update the cache\n",
        "\n",
        "# # Alternatively, you update the expire_time directly\n",
        "# # Update the expire_time directly in valid RFC 3339 format (UTC with a \"Z\" suffix)\n",
        "\n",
        "# expire_time = (\n",
        "#     (\n",
        "#         datetime.datetime.now(datetime.timezone.utc)\n",
        "#         + datetime.timedelta(minutes=15)\n",
        "#     )\n",
        "#     .isoformat()\n",
        "#     .replace(\"+00:00\", \"Z\")\n",
        "# )\n",
        "\n",
        "# client.caches.update(\n",
        "#              name=cache.name,\n",
        "#             config=types.UpdateCachedContentConfig(expire_time=expire_time),\n",
        "# )\n",
        "\n",
        "# # To delete cache\n",
        "\n",
        "# client.caches.delete(name=cache.name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "HHyk7MDr5CAk",
        "outputId": "2b4ecd3d-7dbc-4bd0-f218-07e7fb735cbe"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Okay, I can help you understand LlamaParse and how to set it up.\n\n**What is LlamaParse?**\n\nLlamaParse is a state-of-the-art document parsing solution developed by LlamaIndex. It's designed to reliably extract text and structure from documents, especially those with complex layouts like PDFs. It's offered as part of LlamaCloud and also as a self-serve API. The goal is to provide production-quality data for your LLM applications.\n***Key Features and Benefits:***\n\n1.  **High Accuracy:** LlamaParse excels at accurately extracting content from documents, even those with complex layouts (multi-column layouts, tables, figures, etc.) where other parsers often fail.\n2.  **Structure Preservation:** It attempts to preserve the structural elements of the document, not just the raw text. This means it can help you understand headings, tables, and other layout features.\n3.  **Production-Ready Data:** The parsing quality makes it suitable for use in production LLM applications, where the accuracy of the data is crucial.\n4.  **Easy Integration:** LlamaParse is designed to integrate seamlessly with the LlamaIndex framework.\n\n**How to set up LlamaParse:**\n***Option 1: Using LlamaCloud (Hosted Service)***\n\nThis is the easiest way to get started, especially for enterprise developers.\n\n**Steps:**\n\n1.  **Sign Up:**  Go to the LlamaCloud website ([https://cloud.llamaindex.ai/](https://cloud.llamaindex.ai/)) and sign up for an account.\n2.  **Get an API Key:** Once you're logged in, you'll find your LlamaCloud API key in your account dashboard.\n3.  **Set the API Key in your Environment:**  In your Python environment, set the `LLAMA_CLOUD_API_KEY` environment variable:\n\n```bash\nexport LLAMA_CLOUD_API_KEY=llx-your_api_key\n```\n\n4.  **Install `llama-parse` Package:**\n```bash\npip install llama-parse\n```\n5.  **Using the LlamaParse Loader**\n```python\nfrom llama_parse import LlamaParse\ndocuments = LlamaParse(result_type=\"markdown\").load_data(\n    \"./data/2023_canadian_budget.pdf\"\n)\n```\n***Option 2: Managed services***\n\nIf you're an enterprise developer, check out [**LlamaCloud**](https://llamaindex.ai/enterprise). It is an end-to-end managed service for data parsing, ingestion, indexing, and retrieval, allowing you to get production-quality data for your production LLM application. It's available both hosted on our servers or as a self-hosted solution.\n\n***LlamaParse***\n\nLlamaParse is our state-of-the-art document parsing solution. It's available as part of LlamaCloud and also available as a self-serve API. You can [sign up](https://cloud.llamaindex.ai/) and parse up to 1000 pages/day for free, or enter a credit card for unlimited parsing. [Learn more](https://llamaindex.ai/enterprise).\n\n**Example Code (Python)**\n\nHere's how you would typically use LlamaParse within a LlamaIndex pipeline:\n\n```python\nimport os\nfrom llama_index.core import VectorStoreIndex\nfrom llama_index.core import SimpleDirectoryReader, Settings\n\n# Set the API key (if you haven't already)\nos.environ[\"LLAMA_CLOUD_API_KEY\"] = \"llx-your_api_key\" #replace with your actual API key\n\nfrom llama_parse import LlamaParse\n\n# Load data using LlamaParse\ndocuments = LlamaParse(result_type=\"markdown\").load_data(\n    \"./data/your_document.pdf\"  # Replace with your PDF file\n)\n\n# Create an index from the parsed documents\nindex = VectorStoreIndex.from_documents(documents)\n\n# Now you can query the index as usual\nquery_engine = index.as_query_engine()\nresponse = query_engine.query(\"Your question about the document\")\nprint(response)\n```\n\n**Important Notes:**\n\n*   **Cost:**  Be aware of the pricing for LlamaParse, especially if you're using the self-serve API for a large volume of documents.\n*   **Alternatives:**  If LlamaParse doesn't meet your needs (e.g., due to cost or specific document formats), explore other data connectors/readers in LlamaHub ([https://llamahub.ai/](https://llamahub.ai/)). There are many options for different data sources and parsing methods.\n*   **Error Handling:** Implement robust error handling in your code to catch potential issues during the parsing process.\n\nLet me know if you have other questions.\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time taken:  6.857192277908325\n"
          ]
        }
      ],
      "source": [
        "# Use the cache for generation\n",
        "import time\n",
        "start = time.time()\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=model_name,\n",
        "    contents=\"What is LlamaParse, How to setup? Explain detail\",\n",
        "    config=types.GenerateContentConfig(cached_content=cache.name),\n",
        ")\n",
        "\n",
        "end = time.time()\n",
        "display(Markdown(response.text))\n",
        "print(\"time taken: \", end - start)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qbyW-W7P5B8w",
        "outputId": "1af264b5-a315-4634-c2de-a141db229d7e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GenerateContentResponseUsageMetadata(\n",
              "  cache_tokens_details=[\n",
              "    ModalityTokenCount(\n",
              "      modality=<MediaModality.TEXT: 'TEXT'>,\n",
              "      token_count=212635\n",
              "    ),\n",
              "  ],\n",
              "  cached_content_token_count=212635,\n",
              "  candidates_token_count=1044,\n",
              "  candidates_tokens_details=[\n",
              "    ModalityTokenCount(\n",
              "      modality=<MediaModality.TEXT: 'TEXT'>,\n",
              "      token_count=1044\n",
              "    ),\n",
              "  ],\n",
              "  prompt_token_count=212646,\n",
              "  prompt_tokens_details=[\n",
              "    ModalityTokenCount(\n",
              "      modality=<MediaModality.TEXT: 'TEXT'>,\n",
              "      token_count=212646\n",
              "    ),\n",
              "  ],\n",
              "  total_token_count=213690\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "response.usage_metadata"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skIXhzGyLixv"
      },
      "source": [
        "## First token response time in Straming"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "dIIspB7d4v3-",
        "outputId": "5b3730c3-ffa5-4dbd-e209-337ede79081f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "```"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time taken:  3.200701951980591\n"
          ]
        }
      ],
      "source": [
        "start = time.time()\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=model_name,\n",
        "    contents=\"How to setup a Router query engine?\",\n",
        "    config=types.GenerateContentConfig(cached_content=cache.name,max_output_tokens=1),\n",
        ")\n",
        "end = time.time()\n",
        "display(Markdown(response.text))\n",
        "print(\"time taken: \", end - start)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "U3PKSPhof079"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d495b1369a584fa9bb70604141cdaa5a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_448f1399383b4138805c3e629694e06c",
              "IPY_MODEL_826d33cf073a41118c987059d9b0cff0",
              "IPY_MODEL_d25c30b452db4180956d559084f7058d"
            ],
            "layout": "IPY_MODEL_3fae8c5d806e4539ad429fed79bd0b65"
          }
        },
        "448f1399383b4138805c3e629694e06c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7ff473bee19c4df682000ad18cefa83e",
            "placeholder": "​",
            "style": "IPY_MODEL_ef088b3aaf3c4e21b83a395b7e38beaf",
            "value": "Parsing nodes: 100%"
          }
        },
        "826d33cf073a41118c987059d9b0cff0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8fdfbce52b4447d2a2c4b11c9aa32d0a",
            "max": 56,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_919817d0a97747a585262bec2ab19d97",
            "value": 56
          }
        },
        "d25c30b452db4180956d559084f7058d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bc6209ca50674fe0b3b7d06629999617",
            "placeholder": "​",
            "style": "IPY_MODEL_837acffc66464f92a5f58205a00329c9",
            "value": " 56/56 [00:08&lt;00:00,  9.90it/s]"
          }
        },
        "3fae8c5d806e4539ad429fed79bd0b65": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7ff473bee19c4df682000ad18cefa83e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ef088b3aaf3c4e21b83a395b7e38beaf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8fdfbce52b4447d2a2c4b11c9aa32d0a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "919817d0a97747a585262bec2ab19d97": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bc6209ca50674fe0b3b7d06629999617": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "837acffc66464f92a5f58205a00329c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b05e80430942486d8cb9139a9b93b158": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_91a0e22cc8f047ddb593934aa988fcbe",
              "IPY_MODEL_5dedad13b89a407ebcd085cec53781e9",
              "IPY_MODEL_269569127219431ca5ca99a45a416f20"
            ],
            "layout": "IPY_MODEL_021bb47f4b0140b6ac563c5f52979b57"
          }
        },
        "91a0e22cc8f047ddb593934aa988fcbe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4938f98717ae4984af1067d986ed3e15",
            "placeholder": "​",
            "style": "IPY_MODEL_1aaaad4bb26446b5ac98083fc071e886",
            "value": "Generating embeddings: 100%"
          }
        },
        "5dedad13b89a407ebcd085cec53781e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_489ce02daae8477e91a8c56f968e2fe6",
            "max": 447,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_63404c8d9f764a4fb8d6c862f7f2b50a",
            "value": 447
          }
        },
        "269569127219431ca5ca99a45a416f20": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fe4cc714376542aea642bd374673e1ba",
            "placeholder": "​",
            "style": "IPY_MODEL_b5a5e2e28e39414b962e26a60590d05e",
            "value": " 447/447 [00:05&lt;00:00, 91.46it/s]"
          }
        },
        "021bb47f4b0140b6ac563c5f52979b57": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4938f98717ae4984af1067d986ed3e15": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1aaaad4bb26446b5ac98083fc071e886": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "489ce02daae8477e91a8c56f968e2fe6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "63404c8d9f764a4fb8d6c862f7f2b50a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fe4cc714376542aea642bd374673e1ba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b5a5e2e28e39414b962e26a60590d05e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}