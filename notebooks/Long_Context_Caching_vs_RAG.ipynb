{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v9bpz99INAc1"
   },
   "source": [
    "# Install Packages and Setup Variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "BeuFJKlj9jKz",
    "outputId": "277a7e39-0d7d-4a50-88f0-70dc386e8024"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.7/150.7 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.8/454.8 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m679.1/679.1 kB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m40.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.9/253.9 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m46.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.3/302.3 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -q google-generativeai==0.5.4 llama-index-llms-gemini==0.3.7 llama-index==0.11.23 openai==1.59.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "CWholrWlt2OQ"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Set the following API Keys in the Python environment. Will be used later.\n",
    "# We use OpenAI for the embedding model and Gemini-1.5-flash as our LLM.\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"<YOUR_OPENAI_KEY>\"\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"<YOUR_API_KEY>\"\n",
    "\n",
    "# from google.colab import userdata\n",
    "# os.environ[\"OPENAI_API_KEY\"] = userdata.get('openai_api_key')\n",
    "# os.environ[\"GOOGLE_API_KEY\"] = userdata.get('Google_api_key')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f5eV5EnvNCMM"
   },
   "source": [
    "# Load Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q-7mRQ-mNJlm"
   },
   "source": [
    "## Download\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3PsdOdMUNmEi"
   },
   "source": [
    "The dataset includes a subset of the documentation from the Llama-index library.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3ImRCP7pACaI",
    "outputId": "d688c11d-e2fd-40d7-f5b1-1173f065a8e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   115  100   115    0     0    487      0 --:--:-- --:--:-- --:--:--   489\n",
      "100  570k  100  570k    0     0  1508k      0 --:--:-- --:--:-- --:--:-- 1508k\n"
     ]
    }
   ],
   "source": [
    "!curl -L -o ./llama_index_150k.jsonl https://huggingface.co/datasets/towardsai-buster/llama-index-docs/raw/main/llama_index_data_150k.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bZZLK_wyEc-L"
   },
   "source": [
    "## Read File and create LlamaIndex Documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "miUqycqAEfr7",
    "outputId": "8e00fae4-60c8-421b-c756-fe8918c1b22c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 56\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import Document\n",
    "import json\n",
    "\n",
    "\n",
    "def create_docs(input_file: str) -> list[Document]:\n",
    "    documents = []\n",
    "    with open(input_file, \"r\") as f:\n",
    "        for idx, line in enumerate(f, start=1):\n",
    "\n",
    "          data = json.loads(line)\n",
    "\n",
    "          required_keys = {\"doc_id\", \"content\", \"url\", \"name\", \"tokens\", \"source\"}\n",
    "          if not required_keys.issubset(data):\n",
    "              print(f\"Missing keys in line {idx}: {required_keys - set(data)}\")\n",
    "              continue\n",
    "\n",
    "          documents.append(\n",
    "              Document(\n",
    "                  doc_id=data[\"doc_id\"],\n",
    "                  text=data[\"content\"],\n",
    "                  metadata={  # type: ignore\n",
    "                      \"url\": data[\"url\"],\n",
    "                      \"title\": data[\"name\"],\n",
    "                      \"tokens\": data[\"tokens\"],\n",
    "                      \"source\": data[\"source\"],\n",
    "                  },\n",
    "                  excluded_llm_metadata_keys=[\n",
    "                      \"title\",\n",
    "                      \"tokens\",\n",
    "                      \"source\",\n",
    "                  ],\n",
    "                  excluded_embed_metadata_keys=[\n",
    "                      \"url\",\n",
    "                      \"tokens\",\n",
    "                      \"source\",\n",
    "                  ],\n",
    "              )\n",
    "          )\n",
    "\n",
    "    return documents\n",
    "\n",
    "\n",
    "# Convert the texts to Document objects.\n",
    "documents = create_docs(\"llama_index_150k.jsonl\")\n",
    "print(f\"Number of documents: {len(documents)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f86yksB9K571"
   },
   "source": [
    "# Generate Embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "f6e74cdf647b4b8f90c383e9c256151b",
      "c8e986d25df0489c859b7ab8cd773506",
      "f6f1bdd7637a479db404f52dae7f94dc",
      "548eb1ecc1274cbb9403637ff3d4e8de",
      "744d40b7094c4db887b8a9f775a2d5f6",
      "789e5a6987fb479881ce8c9a167017e5",
      "cc23b4626d6e4226959ae466b58b18ab",
      "6227ad0df2ab4f96ab5b654d82a54739",
      "c7e9e2a5d7944f0a949fd445e3781bee",
      "4931fbd55b2e47e7b8c458e826374883",
      "b116f991e83d45468cebf3519ba1db49",
      "c28f45068270474aaa0ca3e97e448b19",
      "5e74393a13b349fab230e94f68582a11",
      "571277ff2d5642d1b9fa9aa12bfb5fe5",
      "f5470408279545b684aaed9f779df88c",
      "9171ab608bad4689ad9f319d22baa609",
      "9707f4c389ad41c1b54ac3c6da5c65f4",
      "e020024a74d24809b3308351b44d88fb",
      "2936add699974173a52d788a7b24039f",
      "60f0cca82a0f4e3f927d0ba74d4f4e46",
      "a5779c8aceb14b798b1a316bb16e608d",
      "59e227aa1c9a429e86284f7f5b9e0abd"
     ]
    },
    "id": "Bsa7Q-DoNWBk",
    "outputId": "90e458c5-fd9c-479a-e37b-8e9ec16202bc"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6e74cdf647b4b8f90c383e9c256151b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/56 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c28f45068270474aaa0ca3e97e448b19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/447 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "\n",
    "# Build index / generate embeddings using OpenAI embedding model\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    "    embed_model=OpenAIEmbedding(model=\"text-embedding-3-small\"),\n",
    "    transformations=[SentenceSplitter(chunk_size=512, chunk_overlap=128)],\n",
    "    show_progress=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3DoUxd8KK--Q"
   },
   "source": [
    "# Query Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "bUaNH97dEfh9"
   },
   "outputs": [],
   "source": [
    "# Define a query engine that is responsible for retrieving related pieces of text,\n",
    "# and using a LLM to formulate the final answer.\n",
    "\n",
    "from llama_index.llms.gemini import Gemini\n",
    "\n",
    "llm = Gemini(model=\"models/gemini-1.5-flash\", temperature=1, max_tokens=1000)\n",
    "\n",
    "query_engine = index.as_query_engine(llm=llm, similarity_top_k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 168
    },
    "id": "KHK4V_GRR6ZG",
    "outputId": "5e106706-9293-479a-ae79-4c32c4478072"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "There are several ways to set up a query engine, depending on your data and needs.  If you know which tables you'll query beforehand, and the combined size of the table schema and prompt fits within your context window, use a query engine directly.  Otherwise, if the table schema size exceeds your context window, store it in an index using `SQLTableNodeMapping` and `ObjectIndex`, then build a `SQLTableRetrieverQueryEngine`.  For simpler cases, use `index.as_query_engine()`.  More complex scenarios involving multiple documents or data sources can utilize a `SubQuestionQueryEngine` or `RouterQueryEngine`, combining individual query engines into tools and defining the overall engine over these tools.  Customizing retrieval, post-processing, and response synthesis is also possible using a low-level composition API, allowing granular control over the querying process.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken:  4.233275890350342\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "response = query_engine.query(\"How to setup a query engine in code?\")\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "display(Markdown(response.response))\n",
    "print(\"time taken: \", end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 116
    },
    "id": "S-BmyTBbNd9y",
    "outputId": "feddd576-731a-48d9-e34b-438fdfa41c3f"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "To create an agent, you'll need to import necessary components from LlamaIndex, load environment variables (if using a `.env` file), and define tools as Python functions wrapped in `FunctionTool` objects.  Then, initialize the LLM (e.g., using `OpenAI` for OpenAI models or `Ollama` for local models) and create the agent using `ReActAgent.from_tools()`, providing the tools and LLM.  Finally, you can interact with the agent using `.chat()`.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken:  5.146414756774902\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "response = query_engine.query(\"How to setup an agent in code?\")\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "display(Markdown(response.response))\n",
    "print(\"time taken: \", end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b_55vnPoSlID"
   },
   "source": [
    "# Setup Long Context Caching\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oBSZTxjfSlID"
   },
   "source": [
    "For this section, we will be using the Gemini API\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X_ehbBc8dGLQ"
   },
   "source": [
    "Note: You might encounter dependency issues, which may require restarting the session(delete the run time and reinstall). Please reinstall google-generativeai to the latest version. To use long-context caching in google-generativeai, ensure you have version 0.7.2 or higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V-xVY9mXk-eN",
    "outputId": "728b3d42-eda9-41b7-91f9-10fa4aad8bad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/160.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m160.8/160.8 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m760.0/760.0 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m599.1/599.1 kB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.2/129.2 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -q google-generativeai==0.8.3 llama-index==0.12.12 llama-index-llms-gemini==0.4.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "cPm3PFcy3SKp"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"<YOUR_API_KEY>\"\n",
    "\n",
    "# from google.colab import userdata\n",
    "# os.environ[\"GOOGLE_API_KEY\"] = userdata.get('Google_api_key')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WHlXwVnJ3V8V",
    "outputId": "8584e039-1c94-482e-b0f0-e01f9112292c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   115  100   115    0     0    565      0 --:--:-- --:--:-- --:--:--   563\n",
      "100  570k  100  570k    0     0  1663k      0 --:--:-- --:--:-- --:--:-- 1663k\n"
     ]
    }
   ],
   "source": [
    "!curl -L -o ./llama_index_150k.jsonl https://huggingface.co/datasets/towardsai-buster/llama-index-docs/raw/main/llama_index_data_150k.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QGuvt0-N3WsP",
    "outputId": "e5580d23-d701-4c19-cd18-957bd2099b87"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 56\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import json\n",
    "from llama_index.core import Document\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "\n",
    "def create_docs(input_file: str) -> list[Document]:\n",
    "    documents = []\n",
    "    with open(input_file, \"r\") as f:\n",
    "        for idx, line in enumerate(f, start=1):\n",
    "\n",
    "          data = json.loads(line)\n",
    "\n",
    "          required_keys = {\"doc_id\", \"content\", \"url\", \"name\", \"tokens\", \"source\"}\n",
    "          if not required_keys.issubset(data):\n",
    "              print(f\"Missing keys in line {idx}: {required_keys - set(data)}\")\n",
    "              continue\n",
    "\n",
    "          documents.append(\n",
    "              Document(\n",
    "                  doc_id=data[\"doc_id\"],\n",
    "                  text=data[\"content\"],\n",
    "                  metadata={  # type: ignore\n",
    "                      \"url\": data[\"url\"],\n",
    "                      \"title\": data[\"name\"],\n",
    "                      \"tokens\": data[\"tokens\"],\n",
    "                      \"source\": data[\"source\"],\n",
    "                  },\n",
    "                  excluded_llm_metadata_keys=[\n",
    "                      \"title\",\n",
    "                      \"tokens\",\n",
    "                      \"source\",\n",
    "                  ],\n",
    "                  excluded_embed_metadata_keys=[\n",
    "                      \"url\",\n",
    "                      \"tokens\",\n",
    "                      \"source\",\n",
    "                  ],\n",
    "              )\n",
    "          )\n",
    "\n",
    "    return documents\n",
    "\n",
    "\n",
    "# Convert the texts to Document objects.\n",
    "documents = create_docs(\"llama_index_150k.jsonl\")\n",
    "print(f\"Number of documents: {len(documents)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "GgY1ySml4wci"
   },
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "client = genai.Client(api_key=os.environ[\"GOOGLE_API_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Dep30SSV4wYR",
    "outputId": "fcbdaad1-8549-42af-f3c1-b1702eb72e47"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contents saved to llama_index_contents.txt\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def create_text_file(input_file: str, output_file: str) -> None:\n",
    "    with open(input_file, \"r\") as f, open(output_file, \"w\") as out:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            out.write(data[\"content\"] + \"\\n\\n\")  # Add two newlines between documents\n",
    "\n",
    "    print(f\"Contents saved to {output_file}\")\n",
    "\n",
    "\n",
    "create_text_file(\"llama_index_150k.jsonl\", \"llama_index_contents.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "FEivUijQ5CEq"
   },
   "outputs": [],
   "source": [
    "document = client.files.upload(file=\"llama_index_contents.txt\")\n",
    "\n",
    "model_name = \"gemini-1.5-flash-001\"\n",
    "\n",
    "cache = client.caches.create(\n",
    "    model=model_name,\n",
    "    config=types.CreateCachedContentConfig(\n",
    "        contents=[document],\n",
    "        system_instruction=\"You answer questions about the LlamaIndex framework.\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "# # To Update the cache\n",
    "\n",
    "# import datetime\n",
    "\n",
    "# # Update the cache's time-to-live (ttl)\n",
    "\n",
    "# ttl = f\"{int(datetime.timedelta(hours=2).total_seconds())}s\"\n",
    "# client.caches.update(\n",
    "#     name=cache.name, config=types.UpdateCachedContentConfig(ttl=ttl)\n",
    "# )\n",
    "\n",
    "# print(f\"After update:\\n {cache}\")\n",
    "\n",
    "# # Alternatively, update the expire_time directly\n",
    "# # Update the expire_time directly in valid RFC 3339 format (UTC with a \"Z\" suffix)\n",
    "\n",
    "# expire_time = (\n",
    "#     (\n",
    "#         datetime.datetime.now(datetime.timezone.utc)\n",
    "#         + datetime.timedelta(minutes=15)\n",
    "#     )\n",
    "#     .isoformat()\n",
    "#     .replace(\"+00:00\", \"Z\")\n",
    "# )\n",
    "\n",
    "# client.caches.update(\n",
    "#              name=cache.name,\n",
    "#             config=types.UpdateCachedContentConfig(expire_time=expire_time),\n",
    "# )\n",
    "\n",
    "# # To delete cache\n",
    "\n",
    "# client.caches.delete(name=cache.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 858
    },
    "id": "HHyk7MDr5CAk",
    "outputId": "dfeab4ec-7c94-4ed6-e9c8-4cbf9236d866"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "LlamaParse is a state-of-the-art document parsing solution developed by the LlamaIndex team. It's designed to extract structured information from various document formats, making it easier to analyze and utilize content from sources like PDFs, Word documents, and more. \n",
       "\n",
       "Here's a breakdown of LlamaParse:\n",
       "\n",
       "**Key Features:**\n",
       "\n",
       "* **Powerful Parsing:**  LlamaParse leverages advanced parsing techniques to handle complex document layouts, tables, images, and even nested structures.\n",
       "* **Accurate Extraction:** It aims to extract meaningful text and data elements from documents, including tables, lists, headings, and other important components. \n",
       "* **Customizability:** You can tailor the parsing process to suit your specific needs by configuring options for formatting, extraction rules, and more.\n",
       "\n",
       "**Setup:**\n",
       "\n",
       "1. **Sign Up for LlamaCloud:** LlamaParse is part of the LlamaCloud managed service. You can sign up for a free trial on the LlamaCloud website.\n",
       "2. **Create an API Key:** Once signed up, you'll receive an API key. This key is essential for accessing the LlamaParse service.\n",
       "3. **Environment Variable:** Set the `LLAMA_CLOUD_API_KEY` environment variable with the API key you received. \n",
       "4. **Install the LlamaParse library:** \n",
       "   ```bash\n",
       "   pip install llama-parse \n",
       "   ```\n",
       "\n",
       "**Example Usage:**\n",
       "\n",
       "```python\n",
       "from llama_parse import LlamaParse\n",
       "\n",
       "# Load a PDF\n",
       "documents = LlamaParse(result_type=\"markdown\").load_data(\"./data/my_document.pdf\")\n",
       "\n",
       "# Parse a Word document (docx)\n",
       "documents = LlamaParse(result_type=\"text\").load_data(\"./data/my_document.docx\")\n",
       "\n",
       "# Parse a webpage\n",
       "documents = LlamaParse(result_type=\"json\").load_data(\"https://www.example.com\")\n",
       "```\n",
       "\n",
       "**Key Points:**\n",
       "\n",
       "* **`result_type`:** Specifies the output format you want. Options include:\n",
       "    * `markdown`: For nicely formatted output.\n",
       "    * `text`:  For plain text output.\n",
       "    * `json`: For structured JSON output.\n",
       "* **Free Tier:** You can parse up to 1,000 pages per day for free on LlamaCloud. \n",
       "\n",
       "**Benefits of Using LlamaParse:**\n",
       "\n",
       "* **Increased Accuracy:**  Improved parsing leads to more accurate data extraction.\n",
       "* **Simplified Workflow:**  It automates a complex process, allowing you to focus on higher-level tasks.\n",
       "* **Diverse Formats:**  Supports a wide range of document formats. \n",
       "* **Scalability:**  Built for handling large volumes of documents efficiently. \n",
       "\n",
       "Let me know if you have any other questions. \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken:  5.435086727142334\n"
     ]
    }
   ],
   "source": [
    "# Use the cache for generation\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=model_name,\n",
    "    contents=\"What is LlamaParse, How to setup?\",\n",
    "    config=types.GenerateContentConfig(cached_content=cache.name,max_output_tokens=1000),\n",
    ")\n",
    "\n",
    "end = time.time()\n",
    "display(Markdown(response.text))\n",
    "print(\"time taken: \", end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qbyW-W7P5B8w",
    "outputId": "d8e53a3f-6f94-4052-f4af-1cae76e30403"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerateContentResponseUsageMetadata(cache_tokens_details=[ModalityTokenCount(modality=<MediaModality.TEXT: 'TEXT'>, token_count=212088)], cached_content_token_count=212088, candidates_token_count=844, candidates_tokens_details=None, prompt_token_count=212098, prompt_tokens_details=[ModalityTokenCount(modality=<MediaModality.TEXT: 'TEXT'>, token_count=212098)], thoughts_token_count=None, tool_use_prompt_token_count=None, tool_use_prompt_tokens_details=None, total_token_count=212942)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.usage_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "skIXhzGyLixv"
   },
   "source": [
    "## First token response time in Straming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 64
    },
    "id": "dIIspB7d4v3-",
    "outputId": "20430bc7-815f-44fa-9e20-164ba659c8fc"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken:  1.902991771697998\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=model_name,\n",
    "    contents=\"How to setup a Router query engine?\",\n",
    "    config=types.GenerateContentConfig(cached_content=cache.name,max_output_tokens=1),\n",
    ")\n",
    "end = time.time()\n",
    "display(Markdown(response.text))\n",
    "print(\"time taken: \", end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U3PKSPhof079"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
