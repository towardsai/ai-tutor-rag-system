{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pwculRmmrCZq"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/towardsai/ai-tutor-rag-system/blob/main/notebooks/Web_Search_API.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JboB5VaCJUrb",
    "outputId": "bd446db7-d574-478c-bcdc-0d1fe56f5f53"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.9/42.9 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m306.2/306.2 kB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m60.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.5/81.5 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.9/11.9 MB\u001b[0m \u001b[31m117.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m303.3/303.3 kB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.9/105.9 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.1/92.1 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.9/63.9 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m329.6/329.6 kB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m67.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.0/88.0 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.0/51.0 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.7/150.7 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "!pip install -q llama-index==0.14.12 openai==2.14.0 llama-index-tools-google==0.6.2 \\\n",
    "                newspaper4k==0.9.4.1 lxml-html-clean==0.4.3 jedi==0.19.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "1NKAn5scN_g9"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set the following API Keys in the Python environment. Will be used later.\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"[OPENAI_API_KEY]\"\n",
    "# GOOGLE_SEARCH_KEY = \"GOOGLE_SEARCH_KEY\"\n",
    "# GOOGLE_SEARCH_ENGINE = \"GOOGLE_SEARCH_ENGINE\" # Search Engine ID\n",
    "\n",
    "from google.colab import userdata\n",
    "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
    "GOOGLE_SEARCH_KEY = userdata.get('GOOGLE_SEARCH_KEY')\n",
    "GOOGLE_SEARCH_ENGINE = userdata.get('GOOGLE_SEARCH_ENGINE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yp-tXudCc3Zy"
   },
   "source": [
    "## LLM and Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "dYmz-uAIc2rb"
   },
   "outputs": [],
   "source": [
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core import Settings\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "Settings.llm = OpenAI(model=\"gpt-5-mini\", additional_kwargs={'reasoning_effort':'minimal'})\n",
    "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ex1gQVHvITMI"
   },
   "source": [
    "# Using Agents/Tools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "OHd8WbUoZHWi"
   },
   "outputs": [],
   "source": [
    "from llama_index.core.agent.workflow import ReActAgent\n",
    "from llama_index.core.workflow import Context\n",
    "\n",
    "# define sample Tool\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply two integers and returns the result integer\"\"\"\n",
    "    return a * b\n",
    "\n",
    "# initialize ReAct agent\n",
    "agent = ReActAgent(tools=[multiply], verbose=True)\n",
    "\n",
    "# Create a context to store the conversation history/session state\n",
    "ctx = Context(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9JGVdS6AZGve",
    "outputId": "c08cc499-2bd2-4296-e2e1-5cb76534073a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thought: The current language of the user is: English. I need to use a tool to help me answer the question.\n",
      "Action: multiply\n",
      "Action Input: {\"a\": 43, \"b\": 45}Thought: I can answer without using any more tools. I'll use the user's language to answer\n",
      "Answer: 43 × 45 = 1,935"
     ]
    }
   ],
   "source": [
    "from llama_index.core.agent.workflow import AgentStream, ToolCallResult\n",
    "\n",
    "handler = agent.run(\"What is the multiplication of 43 and 45?\", ctx=ctx)\n",
    "\n",
    "async for ev in handler.stream_events():\n",
    "    # if isinstance(ev, ToolCallResult):\n",
    "    #     print(f\"\\nCall {ev.tool_name} with {ev.tool_kwargs}\\nReturned: {ev.tool_output}\")\n",
    "    if isinstance(ev, AgentStream):\n",
    "        print(f\"{ev.delta}\", end=\"\", flush=True)\n",
    "\n",
    "response = await handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CNwoA4dqi0ak",
    "outputId": "71828f4a-67e5-4a59-90a9-71284b4c7b3e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43 × 45 = 1,935\n"
     ]
    }
   ],
   "source": [
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0LMypoqUyuXq"
   },
   "source": [
    "## Define Google Search Tool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "4Q7sc69nJvWI"
   },
   "outputs": [],
   "source": [
    "from llama_index.tools.google import GoogleSearchToolSpec\n",
    "\n",
    "tool_spec = GoogleSearchToolSpec(key=GOOGLE_SEARCH_KEY, engine=GOOGLE_SEARCH_ENGINE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "VrbuIOaMeOIf"
   },
   "outputs": [],
   "source": [
    "# Import and initialize our tool spec\n",
    "from llama_index.core.tools.tool_spec.load_and_search import LoadAndSearchToolSpec\n",
    "\n",
    "# Wrap the google search tool to create an index on top of the returned Google search\n",
    "wrapped_search_tool = LoadAndSearchToolSpec.from_defaults(\n",
    "    tool_spec.to_tool_list()[0],\n",
    ").to_tool_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T3ENpLyBy7UL"
   },
   "source": [
    "## Create the Agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "POKCHXZcowG1",
    "outputId": "a1997b90-5522-47d0-b5da-3dd49f756585"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Call google_search with {'query': 'LLaMA 4 model parameters list LLaMA 4 sizes how many parameters LLaMA 4 models list', 'num': 10}\n",
      "Returned: GoogleSearchToolSpec.google_search() got an unexpected keyword argument 'num'\n",
      "\n",
      "Call google_search with {'query': 'LLaMA 4 model parameters list LLaMA 4 sizes how many parameters LLaMA 4 models list'}\n",
      "Returned: Content loaded! You can now search the information using read_google_search\n",
      "\n",
      "Call read_google_search with {'query': \"LLaMA 4 model sizes list parameters 'LLaMA 4' 'Llama 4' parameter counts 'Llama 4' models sizes 2024 2025 announcement\"}\n",
      "Returned: - Llama 4 Scout — 17 billion parameters (with 128 experts)\n",
      "- Llama 4 Maverick — 109 billion and 400 billion total parameters (note: models described include totals of 109B and 400B)\n",
      "Llama 4 (also written LLaMA 4) is released in multiple sizes. Reported parameter counts across the announced lineup are:\n",
      "\n",
      "- Llama 4 Scout — 17 billion parameters (dense)  \n",
      "- Llama 4 Maverick — variants with 109 billion and 400 billion total parameters (these descriptions have been presented as Maverick-109B and Maverick-400B)  \n",
      "- Llama 4 (base family) — commonly reported dense sizes include 34B and 70B (these are the typical dense sizes in the Llama family; some Llama 4 announcements reference similarly sized dense models)  \n",
      "- Llama 4 with mixture-of-experts (MoE) / experted variants — larger effective total-parameter counts are reported for some configurations (for example Maverick-400B above is an MoE-style total); exact dense vs. total/expert counts vary by variant.\n",
      "\n",
      "Notes and caveats:\n",
      "- Different sources and announcements sometimes list sizes as either “dense” (actual model parameters) or “total” (counting experts or aggregate across routing). MoE variants report much larger “total” parameter numbers even though a given forward pass only activates a subset of experts.\n",
      "- Official documentation or the release notes from Meta/owners should be consulted for exact naming and whether the reported number is dense parameters or total/expert-count. If you want, I can fetch and quote the specific official release page(s) and give a definitive table showing dense vs total counts for each named variant. Which level of detail would you like?"
     ]
    }
   ],
   "source": [
    "from llama_index.core.agent.workflow import FunctionAgent\n",
    "\n",
    "# System prompt encouraging tool usage\n",
    "system_prompt = \"\"\"You are a helpful assistant that can search the web for current information.\n",
    "When you don't have information about recent events or models released after your knowledge cutoff,\n",
    "use the available search tools to find accurate, up-to-date information.\"\"\"\n",
    "\n",
    "# Create agent with proper configuration\n",
    "search_agent = FunctionAgent(\n",
    "    tools=wrapped_search_tool,\n",
    "    llm=Settings.llm,\n",
    "    system_prompt=system_prompt,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "ctx = Context(search_agent)\n",
    "\n",
    "handler = search_agent.run(\"How many parameters LLaMA 4 model has? List the models with parameters\", ctx=ctx)\n",
    "\n",
    "async for ev in handler.stream_events():\n",
    "    if isinstance(ev, ToolCallResult):\n",
    "        print(f\"\\nCall {ev.tool_name} with {ev.tool_kwargs}\\nReturned: {ev.tool_output}\")\n",
    "    if isinstance(ev, AgentStream):\n",
    "        print(f\"{ev.delta}\", end=\"\", flush=True)\n",
    "\n",
    "response = await handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3fSOHUAepeWl",
    "outputId": "c3521395-2f9d-4aa9-b7d1-051829c02396"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Response: Llama 4 (also written LLaMA 4) is released in multiple sizes. Reported parameter counts across the announced lineup are:\n",
      "\n",
      "- Llama 4 Scout — 17 billion parameters (dense)  \n",
      "- Llama 4 Maverick — variants with 109 billion and 400 billion total parameters (these descriptions have been presented as Maverick-109B and Maverick-400B)  \n",
      "- Llama 4 (base family) — commonly reported dense sizes include 34B and 70B (these are the typical dense sizes in the Llama family; some Llama 4 announcements reference similarly sized dense models)  \n",
      "- Llama 4 with mixture-of-experts (MoE) / experted variants — larger effective total-parameter counts are reported for some configurations (for example Maverick-400B above is an MoE-style total); exact dense vs. total/expert counts vary by variant.\n",
      "\n",
      "Notes and caveats:\n",
      "- Different sources and announcements sometimes list sizes as either “dense” (actual model parameters) or “total” (counting experts or aggregate across routing). MoE variants report much larger “total” parameter numbers even though a given forward pass only activates a subset of experts.\n",
      "- Official documentation or the release notes from Meta/owners should be consulted for exact naming and whether the reported number is dense parameters or total/expert-count. If you want, I can fetch and quote the specific official release page(s) and give a definitive table showing dense vs total counts for each named variant. Which level of detail would you like?\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nFinal Response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "baZAoQpY4fvz",
    "outputId": "017a6c4c-58d6-4e28-cf35-a7ebd11b58d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool Calls Made: [ToolCallResult(tool_name='google_search', tool_kwargs={'query': 'LLaMA 4 model parameters list LLaMA 4 sizes how many parameters LLaMA 4 models list', 'num': 10}, tool_id='call_wm1UfutVflUmLlfPcFQNRCAI', tool_output=ToolOutput(blocks=[TextBlock(block_type='text', text=\"GoogleSearchToolSpec.google_search() got an unexpected keyword argument 'num'\")], tool_name='google_search', raw_input={'query': 'LLaMA 4 model parameters list LLaMA 4 sizes how many parameters LLaMA 4 models list', 'num': 10}, raw_output=\"GoogleSearchToolSpec.google_search() got an unexpected keyword argument 'num'\", is_error=True), return_direct=False), ToolCallResult(tool_name='google_search', tool_kwargs={'query': 'LLaMA 4 model parameters list LLaMA 4 sizes how many parameters LLaMA 4 models list'}, tool_id='call_N1IN6j33XWpYdCYeLgOpolwZ', tool_output=ToolOutput(blocks=[TextBlock(block_type='text', text='Content loaded! You can now search the information using read_google_search')], tool_name='google_search', raw_input={'args': (), 'kwargs': {'query': 'LLaMA 4 model parameters list LLaMA 4 sizes how many parameters LLaMA 4 models list'}}, raw_output='Content loaded! You can now search the information using read_google_search', is_error=False), return_direct=False), ToolCallResult(tool_name='read_google_search', tool_kwargs={'query': \"LLaMA 4 model sizes list parameters 'LLaMA 4' 'Llama 4' parameter counts 'Llama 4' models sizes 2024 2025 announcement\"}, tool_id='call_sKSKAK2Bl6ume3Opd2hP1gxD', tool_output=ToolOutput(blocks=[TextBlock(block_type='text', text='- Llama 4 Scout — 17 billion parameters (with 128 experts)\\n- Llama 4 Maverick — 109 billion and 400 billion total parameters (note: models described include totals of 109B and 400B)')], tool_name='read_google_search', raw_input={'args': (), 'kwargs': {'query': \"LLaMA 4 model sizes list parameters 'LLaMA 4' 'Llama 4' parameter counts 'Llama 4' models sizes 2024 2025 announcement\"}}, raw_output='- Llama 4 Scout — 17 billion parameters (with 128 experts)\\n- Llama 4 Maverick — 109 billion and 400 billion total parameters (note: models described include totals of 109B and 400B)', is_error=False), return_direct=False)]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Tool Calls Made: {response.tool_calls}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "who-NM4pIhPn"
   },
   "source": [
    "# Using Tools w/ VectorStoreIndex\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9g9cTM9GI-19"
   },
   "source": [
    "A limitation of the current agent/tool in LlamaIndex is that it **relies solely on the page description from the retrieved pages** to answer questions. This approach will miss answers that are not visible in the page's description tag. To address this, a possible workaround is to fetch the page results, extract the page content using the newspaper3k library, and then create an index based on the downloaded content. Also, the previous method stacks all retrieved items from the search engine into a single document, making it **difficult to pinpoint the exact source** of the response. However, the following method will enable us to present the sources easily.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "31G_fxxJIsbC"
   },
   "source": [
    "## Define Google Search Tool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "lwRmj2odIHxt"
   },
   "outputs": [],
   "source": [
    "from llama_index.tools.google import GoogleSearchToolSpec\n",
    "\n",
    "tool_spec = GoogleSearchToolSpec(key=GOOGLE_SEARCH_KEY, engine=GOOGLE_SEARCH_ENGINE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UVIxdj04Bsf2",
    "outputId": "c8ce0c7c-400b-41c5-c6fb-02803e192ce5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10 results\n"
     ]
    }
   ],
   "source": [
    "search_results = tool_spec.google_search(\"LLaMA 4 model details\")\n",
    "\n",
    "print(f\"Found {len(search_results)} results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u0ealQh6re7t",
    "outputId": "74f34765-5e5a-4fc2-8014-ad111c6a84f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'title': 'The Llama 4 herd: The beginning of a new era of natively ...', 'link': 'https://ai.meta.com/blog/llama-4-multimodal-intelligence/', 'snippet': 'Apr 5, 2025 ... Llama 4 models are designed with native multimodality, incorporating early fusion to seamlessly integrate text and vision tokens into a unified\\xa0...'}, {'title': 'Unmatched Performance and Efficiency | Llama 4', 'link': 'https://www.llama.com/models/llama-4/', 'snippet': 'Meet Llama 4, the latest multimodal AI model offering cost efficiency, 10M context window and easy deployment. Start building advanced personalized\\xa0...'}, {'title': \"Inside Llama 4: How Meta's New Open-Source AI Crushes GPT-4o ...\", 'link': 'https://machine-learning-made-simple.medium.com/inside-llama-4-how-metas-new-open-source-ai-crushes-gpt-4o-and-gemini-e3265f914599', 'snippet': 'Apr 6, 2025 ... Llama 4 models are designed with native multimodality, incorporating early fusion to seamlessly integrate text and vision tokens into a unified\\xa0...'}, {'title': 'Llama 4 | Model Cards and Prompt formats', 'link': 'https://www.llama.com/docs/model-cards-and-prompt-formats/llama4/', 'snippet': 'Technical details and prompt guidance for Llama 4 Maverick and Llama 4 Scout.'}, {'title': 'Llama (language model) - Wikipedia', 'link': 'https://en.wikipedia.org/wiki/Llama_(language_model)', 'snippet': 'Llama models come in different sizes, ranging from 1 billion to 2 trillion parameters. Initially only a foundation model, starting with Llama 2, Meta AI\\xa0...'}, {'title': 'Model tree for meta-llama/Llama-4-Scout-17B-16E', 'link': 'https://huggingface.co/meta-llama/Llama-4-Scout-17B-16E', 'snippet': 'Apr 5, 2025 ... \"Llama 4\" means the foundational large language models and software and algorithms, including machine-learning model code, trained model weights\\xa0...'}, {'title': \"Meta's Llama 4 models now available on AWS\", 'link': 'https://www.aboutamazon.com/news/aws/aws-meta-llama-4-models-available', 'snippet': 'Meet the AI: What is Llama 4 Scout 17B and Llama 4 Maverick 17B? If Llama 4 models were people, Scout would be that detail-oriented research assistant with a\\xa0...'}, {'title': 'meta-llama (Meta Llama)', 'link': 'https://huggingface.co/meta-llama', 'snippet': '... Llama 4 Maverick, a 17 billion parameter model with 128 experts. History ... Llama 3.1 Evals: a collection that provides detailed information on how we\\xa0...'}, {'title': 'Llama 4 family of models from Meta are now available in SageMaker ...', 'link': 'https://aws.amazon.com/blogs/machine-learning/llama-4-family-of-models-from-meta-are-now-available-in-sagemaker-jumpstart/', 'snippet': 'Apr 7, 2025 ... The model details page includes the following information: The model ... Llama 4 model deployments using Sagemaker JumpStart. Text-only\\xa0...'}, {'title': 'llama-models/models/llama4/LICENSE at main · meta-llama/llama ...', 'link': 'https://github.com/meta-llama/llama-models/blob/main/models/llama4/LICENSE', 'snippet': '“Documentation” means the specifications, manuals and documentation accompanying Llama 4 distributed by Meta at https://www.llama.com/docs/overview. “Licensee”\\xa0...'}]\n"
     ]
    }
   ],
   "source": [
    "print(search_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yg2SYgqltQ94",
    "outputId": "678c9912-ab87-4843-f1b4-7d3b698a435d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:newspaper.network:get_html_status(): bad status code 403 on URL: https://machine-learning-made-simple.medium.com/inside-llama-4-how-metas-new-open-source-ai-crushes-gpt-4o-and-gemini-e3265f914599, html: <!DOCTYPE html><html lang=\"en-US\"><head><title>Just a moment...</title><meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\"><meta http-equiv=\"X-UA-Compatible\" content=\"IE=Edge\"><meta nam\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to fetch https://machine-learning-made-simple.medium.com/inside-llama-4-how-metas-new-open-source-ai-crushes-gpt-4o-and-gemini-e3265f914599: Article `download()` failed with Status code 403 for url None on URL https://machine-learning-made-simple.medium.com/inside-llama-4-how-metas-new-open-source-ai-crushes-gpt-4o-and-gemini-e3265f914599\n",
      "Fetched content from 7 pages\n"
     ]
    }
   ],
   "source": [
    "import newspaper\n",
    "\n",
    "pages_content = []\n",
    "\n",
    "for item in search_results:\n",
    "    url = item.get(\"link\")\n",
    "    title = item.get(\"title\", \"\")\n",
    "    try:\n",
    "        article = newspaper.Article(url)\n",
    "        article.download()\n",
    "        article.parse()\n",
    "        if article.text:\n",
    "            pages_content.append({\n",
    "                \"url\": url,\n",
    "                \"title\": title,\n",
    "                \"text\": article.text\n",
    "            })\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to fetch {url}: {e}\")\n",
    "\n",
    "print(f\"Fetched content from {len(pages_content)} pages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "CajsEP_ytXRW"
   },
   "outputs": [],
   "source": [
    "# Build the index\n",
    "from llama_index.core import Document\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "documents = [\n",
    "    Document(text=doc[\"text\"], metadata={\"title\": doc[\"title\"], \"url\": doc[\"url\"]})\n",
    "    for doc in pages_content\n",
    "]\n",
    "\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    "    transformations=[SentenceSplitter(chunk_size=512, chunk_overlap=128)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OlRR3iEntAc_",
    "outputId": "6b66f28f-3895-4dfa-964f-76dfa1a97fc2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama 4 Scout (activated / active parameters): 17 billion (17B) active parameters; total parameters: 109 billion (109B)  \n",
      "Llama 4 Maverick (activated / active parameters): 17 billion (17B) active parameters; total parameters: 400 billion (400B)\n"
     ]
    }
   ],
   "source": [
    "# Query\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\n",
    "    \"How many parameters does LLaMA 4 have? List exact sizes of each variant.\"\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZiJ042vDta-b",
    "outputId": "c168249c-77f6-4238-d254-66e823415cb8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title:  Llama (language model) - Wikipedia\n",
      "Source: https://en.wikipedia.org/wiki/Llama_(language_model)\n",
      "Score:  0.5514\n",
      "----------------------------------------\n",
      "Title:  Model tree for meta-llama/Llama-4-Scout-17B-16E\n",
      "Source: https://huggingface.co/meta-llama/Llama-4-Scout-17B-16E\n",
      "Score:  0.5287\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Show sources\n",
    "for node in response.source_nodes:\n",
    "    print(f\"Title:  {node.metadata['title']}\")\n",
    "    print(f\"Source: {node.metadata['url']}\")\n",
    "    print(f\"Score:  {node.score:.4f}\")\n",
    "    print(\"-\" * 40)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
