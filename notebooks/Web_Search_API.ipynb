{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JboB5VaCJUrb",
        "outputId": "a2e9b2a6-5235-4122-da26-15694eb70db6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.6/296.6 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m61.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.5/81.5 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m101.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m303.3/303.3 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.4/107.4 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.5/42.5 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.9/63.9 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.5/310.5 kB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m59.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.4/144.4 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q llama-index==0.14.0 openai==1.107.0 llama-index-tools-google==0.6.0 \\\n",
        "                newspaper4k==0.9.3.1 lxml-html-clean==0.4.2 jedi==0.19.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1NKAn5scN_g9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Set the following API Keys in the Python environment. Will be used later.\n",
        "# os.environ[\"OPENAI_API_KEY\"] = \"[OPENAI_API_KEY]\"\n",
        "# GOOGLE_SEARCH_KEY = \"GOOGLE_SEARCH_KEY\"\n",
        "# GOOGLE_SEARCH_ENGINE = \"GOOGLE_SEARCH_ENGINE\" # Search Engine ID\n",
        "\n",
        "from google.colab import userdata\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
        "GOOGLE_SEARCH_KEY = userdata.get('GOOGLE_SEARCH_KEY')\n",
        "GOOGLE_SEARCH_ENGINE = userdata.get('GOOGLE_SEARCH_ENGINE')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LLM and Embedding Model"
      ],
      "metadata": {
        "id": "Yp-tXudCc3Zy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "from llama_index.core import Settings\n",
        "from llama_index.llms.openai import OpenAI\n",
        "\n",
        "Settings.llm = OpenAI(model=\"gpt-5-mini\", additional_kwargs={'reasoning_effort':'minimal'})\n",
        "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")"
      ],
      "metadata": {
        "id": "dYmz-uAIc2rb"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ex1gQVHvITMI"
      },
      "source": [
        "# Using Agents/Tools\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.agent.workflow import ReActAgent\n",
        "from llama_index.core.workflow import Context\n",
        "\n",
        "# define sample Tool\n",
        "def multiply(a: int, b: int) -> int:\n",
        "    \"\"\"Multiply two integers and returns the result integer\"\"\"\n",
        "    return a * b\n",
        "\n",
        "# initialize ReAct agent\n",
        "agent = ReActAgent(tools=[multiply], verbose=True)\n",
        "\n",
        "# Create a context to store the conversation history/session state\n",
        "ctx = Context(agent)"
      ],
      "metadata": {
        "id": "OHd8WbUoZHWi"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.agent.workflow import AgentStream, ToolCallResult\n",
        "\n",
        "handler = agent.run(\"What is the multiplication of 43 and 45?\", ctx=ctx)\n",
        "\n",
        "async for ev in handler.stream_events():\n",
        "    # if isinstance(ev, ToolCallResult):\n",
        "    #     print(f\"\\nCall {ev.tool_name} with {ev.tool_kwargs}\\nReturned: {ev.tool_output}\")\n",
        "    if isinstance(ev, AgentStream):\n",
        "        print(f\"{ev.delta}\", end=\"\", flush=True)\n",
        "\n",
        "response = await handler"
      ],
      "metadata": {
        "id": "9JGVdS6AZGve",
        "outputId": "34384b39-7c3d-4cea-94fc-bf292731677e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running step init_run\n",
            "Step init_run produced event AgentInput\n",
            "Running step setup_agent\n",
            "Step setup_agent produced event AgentSetup\n",
            "Running step run_agent_step\n",
            "Thought: The current language of the user is: English. I need to use a tool to help me answer the question.\n",
            "Action: multiply\n",
            "Action Input: {\"a\": 43, \"b\": 45}Step run_agent_step produced event AgentOutput\n",
            "Running step parse_agent_output\n",
            "Step parse_agent_output produced no event\n",
            "Running step call_tool\n",
            "Step call_tool produced event ToolCallResult\n",
            "Running step aggregate_tool_results\n",
            "Step aggregate_tool_results produced event AgentInput\n",
            "Running step setup_agent\n",
            "Step setup_agent produced event AgentSetup\n",
            "Running step run_agent_step\n",
            "Thought: I can answer without using any more tools. I'll use the user's language to answer\n",
            "Answer: 43 × 45 = 1,935Step run_agent_step produced event AgentOutput\n",
            "Running step parse_agent_output\n",
            "Step parse_agent_output produced event StopEvent\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(str(response))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CNwoA4dqi0ak",
        "outputId": "fc1d4101-ccc1-46d2-ee4a-a206c7c5fa0c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "43 × 45 = 1,935\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LMypoqUyuXq"
      },
      "source": [
        "## Define Google Search Tool\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "4Q7sc69nJvWI"
      },
      "outputs": [],
      "source": [
        "from llama_index.tools.google import GoogleSearchToolSpec\n",
        "\n",
        "tool_spec = GoogleSearchToolSpec(key=GOOGLE_SEARCH_KEY, engine=GOOGLE_SEARCH_ENGINE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "VrbuIOaMeOIf"
      },
      "outputs": [],
      "source": [
        "# Import and initialize our tool spec\n",
        "from llama_index.core.tools.tool_spec.load_and_search import LoadAndSearchToolSpec\n",
        "\n",
        "# Wrap the google search tool to create an index on top of the returned Google search\n",
        "wrapped_search_tool = LoadAndSearchToolSpec.from_defaults(\n",
        "    tool_spec.to_tool_list()[0],\n",
        ").to_tool_list()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3ENpLyBy7UL"
      },
      "source": [
        "## Create the Agent\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.agent.workflow import FunctionAgent\n",
        "\n",
        "# System prompt encouraging tool usage\n",
        "system_prompt = \"\"\"You are a helpful assistant that can search the web for current information.\n",
        "When you don't have information about recent events or models released after your knowledge cutoff,\n",
        "use the available search tools to find accurate, up-to-date information.\"\"\"\n",
        "\n",
        "# Create agent with proper configuration\n",
        "search_agent = FunctionAgent(\n",
        "    tools=wrapped_search_tool,\n",
        "    llm=Settings.llm,\n",
        "    system_prompt=system_prompt,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "ctx = Context(search_agent)\n",
        "\n",
        "handler = search_agent.run(\"How many parameters LLaMA 4 model has? List the models with parameters\", ctx=ctx)\n",
        "\n",
        "async for ev in handler.stream_events():\n",
        "    if isinstance(ev, ToolCallResult):\n",
        "        print(f\"\\nCall {ev.tool_name} with {ev.tool_kwargs}\\nReturned: {ev.tool_output}\")\n",
        "    if isinstance(ev, AgentStream):\n",
        "        print(f\"{ev.delta}\", end=\"\", flush=True)\n",
        "\n",
        "response = await handler"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "POKCHXZcowG1",
        "outputId": "7e60eb37-6096-4f9b-f6f8-2bd3fa25510d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running step init_run\n",
            "Step init_run produced event AgentInput\n",
            "Running step setup_agent\n",
            "Step setup_agent produced event AgentSetup\n",
            "Running step run_agent_step\n",
            "Step run_agent_step produced event AgentOutput\n",
            "Running step parse_agent_output\n",
            "Step parse_agent_output produced no event\n",
            "Running step call_tool\n",
            "Step call_tool produced event ToolCallResult\n",
            "\n",
            "Call google_search with {'query': \"LLaMA 4 model parameter counts list models 'LLaMA 4' sizes\"}\n",
            "Returned: Content loaded! You can now search the information using read_google_search\n",
            "Running step aggregate_tool_results\n",
            "Step aggregate_tool_results produced event AgentInput\n",
            "Running step setup_agent\n",
            "Step setup_agent produced event AgentSetup\n",
            "Running step run_agent_step\n",
            "Step run_agent_step produced event AgentOutput\n",
            "Running step parse_agent_output\n",
            "Step parse_agent_output produced no event\n",
            "Running step call_tool\n",
            "Step call_tool produced event ToolCallResult\n",
            "\n",
            "Call read_google_search with {'query': \"LLaMA 4 models and parameter counts list 'LLaMA 4' 'parameter' 'models' 'sizes'\"}\n",
            "Returned: LLaMA 4 comes in two sizes: Llama 4 Scout (109B parameters) and Llama 4 Maverick (400B parameters).\n",
            "Running step aggregate_tool_results\n",
            "Step aggregate_tool_results produced event AgentInput\n",
            "Running step setup_agent\n",
            "Step setup_agent produced event AgentSetup\n",
            "Running step run_agent_step\n",
            "LLaMA 4 is released in two sizes:\n",
            "\n",
            "- Llama 4 Scout — 109 billion parameters  \n",
            "- Llama 4 Maverick — 400 billion parametersStep run_agent_step produced event AgentOutput\n",
            "Running step parse_agent_output\n",
            "Step parse_agent_output produced event StopEvent\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\\nFinal Response: {response}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3fSOHUAepeWl",
        "outputId": "5291e053-de64-4996-87be-606a565c6200"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final Response: LLaMA 4 is released in two sizes:\n",
            "\n",
            "- Llama 4 Scout — 109 billion parameters  \n",
            "- Llama 4 Maverick — 400 billion parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Tool Calls Made: {response.tool_calls}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "baZAoQpY4fvz",
        "outputId": "e21bdcac-9e54-48a3-aeb5-253a57861f18"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tool Calls Made: [ToolCallResult(tool_name='google_search', tool_kwargs={'query': \"LLaMA 4 model parameter counts list models 'LLaMA 4' sizes\"}, tool_id='call_EpCwVMc5sQw1lYkLVqXNnD92', tool_output=ToolOutput(blocks=[TextBlock(block_type='text', text='Content loaded! You can now search the information using read_google_search')], tool_name='google_search', raw_input={'args': (), 'kwargs': {'query': \"LLaMA 4 model parameter counts list models 'LLaMA 4' sizes\"}}, raw_output='Content loaded! You can now search the information using read_google_search', is_error=False), return_direct=False), ToolCallResult(tool_name='read_google_search', tool_kwargs={'query': \"LLaMA 4 models and parameter counts list 'LLaMA 4' 'parameter' 'models' 'sizes'\"}, tool_id='call_XzdsoQJdOtO9kqzKJudv8nFS', tool_output=ToolOutput(blocks=[TextBlock(block_type='text', text='LLaMA 4 comes in two sizes: Llama 4 Scout (109B parameters) and Llama 4 Maverick (400B parameters).')], tool_name='read_google_search', raw_input={'args': (), 'kwargs': {'query': \"LLaMA 4 models and parameter counts list 'LLaMA 4' 'parameter' 'models' 'sizes'\"}}, raw_output='LLaMA 4 comes in two sizes: Llama 4 Scout (109B parameters) and Llama 4 Maverick (400B parameters).', is_error=False), return_direct=False)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "who-NM4pIhPn"
      },
      "source": [
        "# Using Tools w/ VectorStoreIndex\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9g9cTM9GI-19"
      },
      "source": [
        "A limitation of the current agent/tool in LlamaIndex is that it **relies solely on the page description from the retrieved pages** to answer questions. This approach will miss answers that are not visible in the page's description tag. To address this, a possible workaround is to fetch the page results, extract the page content using the newspaper3k library, and then create an index based on the downloaded content. Also, the previous method stacks all retrieved items from the search engine into a single document, making it **difficult to pinpoint the exact source** of the response. However, the following method will enable us to present the sources easily.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31G_fxxJIsbC"
      },
      "source": [
        "## Define Google Search Tool\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "lwRmj2odIHxt"
      },
      "outputs": [],
      "source": [
        "from llama_index.tools.google import GoogleSearchToolSpec\n",
        "\n",
        "tool_spec = GoogleSearchToolSpec(key=GOOGLE_SEARCH_KEY, engine=GOOGLE_SEARCH_ENGINE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "UVIxdj04Bsf2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "931f17ce-fe6e-4f14-8b4f-a64d2a8a84dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 10 results\n"
          ]
        }
      ],
      "source": [
        "search_results = tool_spec.google_search(\"LLaMA 4 model details\")\n",
        "\n",
        "print(f\"Found {len(search_results)} results\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(search_results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u0ealQh6re7t",
        "outputId": "a8026049-a7bc-45b8-d000-5e435d49ed57"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'title': 'The Llama 4 herd: The beginning of a new era of natively ...', 'link': 'https://ai.meta.com/blog/llama-4-multimodal-intelligence/', 'snippet': 'Apr 5, 2025 ... Llama 4 models are designed with native multimodality, incorporating early fusion to seamlessly integrate text and vision tokens into a unified\\xa0...'}, {'title': 'Unmatched Performance and Efficiency | Llama 4', 'link': 'https://www.llama.com/models/llama-4/', 'snippet': 'Latest models · Llama 4 Scout. Class-leading natively multimodal model that offers superior text and visual intelligence, single H100 GPU efficiency, and a 10M\\xa0...'}, {'title': 'Model tree for meta-llama/Llama-4-Scout-17B-16E', 'link': 'https://huggingface.co/meta-llama/Llama-4-Scout-17B-16E', 'snippet': 'Apr 5, 2025 ... \"Llama 4\" means the foundational large language models and software and algorithms, including machine-learning model code, trained model weights\\xa0...'}, {'title': 'Llama: Industry Leading, Open-Source AI', 'link': 'https://www.llama.com/', 'snippet': \"Discover Llama 4's class-leading AI models, Scout and Maverick. Experience top performance, multimodality, low costs, and unparalleled efficiency.\"}, {'title': 'Llama (language model) - Wikipedia', 'link': 'https://en.wikipedia.org/wiki/Llama_(language_model)', 'snippet': 'Llama (Large Language Model Meta AI) is a family of large language models (LLMs) released by Meta AI starting in February 2023. ... The latest version is Llama 4,\\xa0...'}, {'title': 'Model Cards and Prompt formats - Llama 4', 'link': 'https://www.llama.com/docs/model-cards-and-prompt-formats/llama4/', 'snippet': 'Technical details and prompt guidance for Llama 4 Maverick and Llama 4 Scout.'}, {'title': 'meta-llama (Meta Llama)', 'link': 'https://huggingface.co/meta-llama', 'snippet': '... Llama 4 Maverick, a 17 billion parameter model with 128 experts. History ... Llama 3.1 Evals: a collection that provides detailed information on how we\\xa0...'}, {'title': \"Inside Llama 4: How Meta's New Open-Source AI Crushes GPT-4o ...\", 'link': 'https://machine-learning-made-simple.medium.com/inside-llama-4-how-metas-new-open-source-ai-crushes-gpt-4o-and-gemini-e3265f914599', 'snippet': 'Apr 6, 2025 ... Traditional Deep Learning models are “dense.” When processing information, every single piece of input data flows through every single parameter\\xa0...'}, {'title': \"Meta's Llama 4 models now available on AWS\", 'link': 'https://www.aboutamazon.com/news/aws/aws-meta-llama-4-models-available', 'snippet': 'Meet the AI: What is Llama 4 Scout 17B and Llama 4 Maverick 17B? If Llama 4 models were people, Scout would be that detail-oriented research assistant with a\\xa0...'}, {'title': 'Llama 4 family of models from Meta are now available in SageMaker ...', 'link': 'https://aws.amazon.com/blogs/machine-learning/llama-4-family-of-models-from-meta-are-now-available-in-sagemaker-jumpstart/', 'snippet': 'Apr 7, 2025 ... Model description; License information; Technical specifications; Usage guidelines. Before you deploy the model, we recommended you review the\\xa0...'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import newspaper\n",
        "\n",
        "pages_content = []\n",
        "\n",
        "for item in search_results:\n",
        "    url = item.get(\"link\")\n",
        "    title = item.get(\"title\", \"\")\n",
        "    try:\n",
        "        article = newspaper.Article(url)\n",
        "        article.download()\n",
        "        article.parse()\n",
        "        if article.text:\n",
        "            pages_content.append({\n",
        "                \"url\": url,\n",
        "                \"title\": title,\n",
        "                \"text\": article.text\n",
        "            })\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to fetch {url}: {e}\")\n",
        "\n",
        "print(f\"Fetched content from {len(pages_content)} pages\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yg2SYgqltQ94",
        "outputId": "18836a69-9268-4b30-a47a-35fa4823080d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:newspaper.network:get_html_status(): bad status code 429 on URL: https://machine-learning-made-simple.medium.com/inside-llama-4-how-metas-new-open-source-ai-crushes-gpt-4o-and-gemini-e3265f914599, html: <!DOCTYPE html><html lang=\"en-US\"><head><title>Just a moment...</title><meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\"><meta http-equiv=\"X-UA-Compatible\" content=\"IE=Edge\"><meta nam\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to fetch https://machine-learning-made-simple.medium.com/inside-llama-4-how-metas-new-open-source-ai-crushes-gpt-4o-and-gemini-e3265f914599: Article `download()` failed with Status code 429 for url None on URL https://machine-learning-made-simple.medium.com/inside-llama-4-how-metas-new-open-source-ai-crushes-gpt-4o-and-gemini-e3265f914599\n",
            "Fetched content from 6 pages\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the index\n",
        "from llama_index.core import Document\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "from llama_index.core import VectorStoreIndex\n",
        "\n",
        "documents = [\n",
        "    Document(text=doc[\"text\"], metadata={\"title\": doc[\"title\"], \"url\": doc[\"url\"]})\n",
        "    for doc in pages_content\n",
        "]\n",
        "\n",
        "index = VectorStoreIndex.from_documents(\n",
        "    documents,\n",
        "    transformations=[SentenceSplitter(chunk_size=512, chunk_overlap=128)],\n",
        ")"
      ],
      "metadata": {
        "id": "CajsEP_ytXRW"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Query\n",
        "query_engine = index.as_query_engine()\n",
        "response = query_engine.query(\n",
        "    \"How many parameters does LLaMA 4 have? List exact sizes of each variant.\"\n",
        ")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OlRR3iEntAc_",
        "outputId": "07145995-045f-4c33-f525-e56ff8cc2d6c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Llama 4 variants and their parameter sizes (activated / total where provided):\n",
            "\n",
            "- Llama 4 Scout: 17 billion activated parameters (109 billion total parameters with 16 experts)  \n",
            "- Llama 4 Maverick: 17 billion activated parameters (400 billion total parameters with 128 experts)\n",
            "\n",
            "Additionally, earlier summary: Llama 4 series (2025) uses a mixture-of-experts architecture and the models are described as 17B parameter models with multiple experts.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Show sources\n",
        "for node in response.source_nodes:\n",
        "    print(f\"Title:  {node.metadata['title']}\")\n",
        "    print(f\"Source: {node.metadata['url']}\")\n",
        "    print(f\"Score:  {node.score:.4f}\")\n",
        "    print(\"-\" * 40)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZiJ042vDta-b",
        "outputId": "a1241dd1-43eb-4fb7-c8f5-4ed5bfff289d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Title:  Llama (language model) - Wikipedia\n",
            "Source: https://en.wikipedia.org/wiki/Llama_(language_model)\n",
            "Score:  0.5311\n",
            "----------------------------------------\n",
            "Title:  Model tree for meta-llama/Llama-4-Scout-17B-16E\n",
            "Source: https://huggingface.co/meta-llama/Llama-4-Scout-17B-16E\n",
            "Score:  0.5287\n",
            "----------------------------------------\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}