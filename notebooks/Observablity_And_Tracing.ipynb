{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/towardsai/ai-tutor-rag-system/blob/notebook%2Faman/notebooks/Observablity_And_Tracing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AItauLdsp7jj"
   },
   "source": [
    "## Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pmnledOoOjbn",
    "outputId": "74218379-8ce5-468b-d8c8-3ec768813cab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/75.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/447.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m447.5/447.5 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -q langchain-openai==0.3.33 langchain==0.3.27"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AgSc92W9qIeF"
   },
   "source": [
    "## Setting up env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "lXz2mUoyiEuJ"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"<OPENAI_API_KEY\"\n",
    "# os.environ[\"LANGCHAIN_API_KEY\"] = \"<LANGCHAIN_API_KEY>\"\n",
    "# os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "# os.environ[\"LANGCHAIN_PROJECT\"] = \"<LANGCHAIN_PROJECT>\"\n",
    "\n",
    "from google.colab import userdata\n",
    "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = userdata.get('LangChain_API_Key')\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"demo-project\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LAgKNo9kqOZc"
   },
   "source": [
    "## Tracing Langchain calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "okLDL4YQhljS",
    "outputId": "c9e80226-22c2-4978-8f84-ce2ab255257d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I’m missing one key detail: “Quantum Physics” is not a surgical procedure, so I can’t sensibly produce procedure-specific preoperative anesthetic guidance for it. Do you mean a particular operation (for example, “quadriceps tendon repair,” “quaternary iliofemoral arthroplasty,” or a different named procedure)? Please confirm the intended surgical procedure or section title.\n",
      "\n",
      "If you intended a general preoperative section template for WikiAnesthesia (useful until you tell me the exact procedure), here is a concise, WikiAnesthesia‑style preoperative section you can drop into any procedure page and then I can tailor it further to the exact surgery:\n",
      "\n",
      "Preoperative assessment\n",
      "- Focused history: baseline functional status, exercise tolerance, cardiopulmonary disease, bleeding history, prior anesthetic complications, airway problems, concurrent medications, and opioid/benzodiazepine tolerance.\n",
      "- Comorbidities: identify and document active cardiac, pulmonary, renal, hepatic, endocrine, neurologic, and hematologic conditions that affect perioperative risk.\n",
      "- Frailty and cognition: assess frailty, delirium risk, and capacity for consent in older adults.\n",
      "\n",
      "Investigations\n",
      "- Routine testing guided by age, comorbidity, and procedure invasiveness (ECG for >40–60 yrs or cardiac disease; chest radiograph for cardiopulmonary disease; basic metabolic panel and CBC as indicated).\n",
      "- Targeted studies for suspected problems (echocardiography for new heart failure/valvular disease, pulmonary function tests for severe COPD, coagulation studies for bleeding history).\n",
      "\n",
      "Optimization and risk modification\n",
      "- Cardiac: optimize ischemic heart disease, heart failure, and arrhythmias in consultation with cardiology when indicated.\n",
      "- Pulmonary: smoking cessation, bronchodilator optimization, treat infections, consider preop physiotherapy for high-risk patients.\n",
      "- Metabolic: control diabetes (target periop glucose range), correct significant electrolyte abnormalities.\n",
      "- Anemia: investigate and treat preoperative anemia when surgery is elective (iron, erythropoietin as indicated).\n",
      "\n",
      "Medication management\n",
      "- Continue chronic medications that reduce perioperative risk (eg, beta‑blockers if appropriate).\n",
      "- Antiplatelet/anticoagulant strategy individualized by thrombotic vs bleeding risk; coordinate with prescribing clinician and follow institutional protocols regarding timing of cessation and bridging.\n",
      "- Hold or adjust oral hypoglycemics and SGLT2 inhibitors per guidelines; continue most other maintenance meds with sips of water unless contraindicated.\n",
      "\n",
      "Fasting and aspiration risk\n",
      "- Standard fasting guidelines: clear fluids up to 2 hours, solids 6–8 hours (modify for infants, pregnancy, or gastroparesis).\n",
      "- Identify patients with increased aspiration risk (pregnancy, bowel obstruction, reflux/GERD) and plan rapid-sequence induction or aspiration prophylaxis as appropriate.\n",
      "\n",
      "Airway evaluation\n",
      "- Perform a focused airway exam (mouth opening, dentition, Mallampati, neck mobility, previous difficulty). Document predictors of difficult mask ventilation/intubation and prepare adjuncts and backup plans.\n",
      "\n",
      "Perioperative analgesia and opioid stewardship\n",
      "- Plan multimodal analgesia (acetaminophen, NSAIDs if not contraindicated, regional techniques where appropriate).\n",
      "- Review chronic opioid use and consider preop optimization and multimodal strategies to reduce postoperative opioid needs.\n",
      "\n",
      "Regional anesthesia and neuraxial considerations\n",
      "- Assess suitability for neuraxial/spinal/regional blocks (coagulation status, infection at site, patient cooperation).\n",
      "- Ensure appropriate timing of anticoagulants relative to neuraxial procedures per guidelines.\n",
      "\n",
      "Consent and patient information\n",
      "- Discuss anesthesia options, anticipated postoperative analgesia, nausea/vomiting risk, possibility of airway complications, and need for postoperative monitoring or ICU care.\n",
      "- Document risks specific to patient comorbidities and planned anesthetic technique.\n",
      "\n",
      "Logistics and planning\n",
      "- Anticipate need for invasive monitoring, blood products, temperature control, and postoperative high‑dependency care for high-risk procedures or patients.\n",
      "- Ensure cross‑discipline communication with surgical, nursing, and blood bank teams when complex support is predicted.\n",
      "\n",
      "Documentation and handover\n",
      "- Clearly document preop assessment, optimization steps taken, and planned anesthetic approach.\n",
      "- Communicate critical information (difficult airway, anticoagulation plan, allergies, implants, recent cardiac stents) during preop huddle and handover.\n",
      "\n",
      "If you tell me the exact surgical procedure, I will convert this template into a concise, procedure-specific preoperative section with evidence-based recommendations and any special considerations.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain import hub\n",
    "\n",
    "prompt = hub.pull(\"wikianes/section_writer\")\n",
    "llm = ChatOpenAI(model=\"gpt-5-mini\")\n",
    "\n",
    "chain = prompt | llm\n",
    "output = chain.invoke({\"section_description\": \"Quantum Physics\"})\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0-s3VBrSzkXZ"
   },
   "source": [
    "## Tracing OpenAI calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "1sM6lLW2sFT-"
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from langsmith.wrappers import wrap_openai\n",
    "\n",
    "# wrap_openai from langsmith provides wrapper to\n",
    "ai_client = wrap_openai(OpenAI())\n",
    "\n",
    "\n",
    "def retrieve_documents(inquiry: str):\n",
    "    docs = [\n",
    "        \"Vector databases enable efficient semantic search for LLMs.\",\n",
    "        \"Retrieval-Augmented Generation (RAG) improves LLM accuracy with relevant context.\",\n",
    "        \"Large Language Models (LLMs) have set new benchmarks in NLP tasks.\",\n",
    "        \"Transformer architectures have revolutionized many areas of machine learning.\",\n",
    "        \"Embedding models convert text into vector representations for similarity comparisons.\"\n",
    "    ]\n",
    "    return docs\n",
    "\n",
    "def ask_qa(query):\n",
    "    context = retrieve_documents(query)\n",
    "    system_prompt = f\"\"\"\n",
    "    You are an assistant for question-answering tasks.\n",
    "    Use the following pieces of retrieved context to answer the question.\n",
    "    Context: {context}\n",
    "    \"\"\"\n",
    "\n",
    "    return ai_client.responses.create(\n",
    "        instructions=system_prompt,\n",
    "        input=query,\n",
    "        model=\"gpt-5-mini\",\n",
    "        reasoning={'effort':'low'} # For Reasoning Models\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m_sGUKNusXcA",
    "outputId": "1bdfe8e3-470d-4c1b-8e36-d800b6cd55e5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Response(id='resp_68d138d45f6c81a1a9276e626f228da106ce867c79a9ec01', created_at=1758542036.0, error=None, incomplete_details=None, instructions=\"\\n    You are an assistant for question-answering tasks.\\n    Use the following pieces of retrieved context to answer the question.\\n    Context: ['Vector databases enable efficient semantic search for LLMs.', 'Retrieval-Augmented Generation (RAG) improves LLM accuracy with relevant context.', 'Large Language Models (LLMs) have set new benchmarks in NLP tasks.', 'Transformer architectures have revolutionized many areas of machine learning.', 'Embedding models convert text into vector representations for similarity comparisons.']\\n    \", metadata={}, model='gpt-5-mini-2025-08-07', object='response', output=[ResponseReasoningItem(id='rs_68d138d4d5b481a1ab51cdca73d5f77306ce867c79a9ec01', summary=[], type='reasoning', content=None, encrypted_content=None, status=None), ResponseOutputMessage(id='msg_68d138d5f84881a1b4feab828aa5dad006ce867c79a9ec01', content=[ResponseOutputText(annotations=[], text='A vector database (vector DB) is a specialized database designed to store and search vector representations (embeddings) of data—typically text, images, audio, or other items turned into numeric vectors by an embedding model. Instead of exact key/value lookups, a vector DB performs similarity searches to find items whose vectors are most \"close\" to a query vector.\\n\\nKey points\\n- Purpose: Enable fast semantic search and retrieval for applications like search, recommendation, and Retrieval-Augmented Generation (RAG) for LLMs.  \\n- How it works: Text or other content is converted into high-dimensional embeddings by an embedding model; those vectors are stored and indexed. A query is converted into a vector and the DB returns nearest neighbors based on a distance/similarity metric (cosine, dot product, Euclidean).  \\n- Performance: Uses specialized indexing and approximate nearest neighbor (ANN) algorithms to scale to millions or billions of vectors while keeping queries fast.  \\n- Benefits: Finds semantically related results (not just keyword matches), improves LLM outputs when used as retrieval context, and supports multimodal data.  \\n- Examples/implementations: FAISS, Milvus, Pinecone, Weaviate, and Annoy.\\n\\nIn short: a vector DB makes similarity-based, semantic retrieval efficient and scalable by storing embeddings and providing fast nearest-neighbor search.', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[], top_p=1.0, background=False, conversation=None, max_output_tokens=None, max_tool_calls=None, previous_response_id=None, prompt=None, prompt_cache_key=None, reasoning=Reasoning(effort='low', generate_summary=None, summary=None), safety_identifier=None, service_tier='default', status='completed', text=ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity='medium'), top_logprobs=0, truncation='disabled', usage=ResponseUsage(input_tokens=115, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=344, output_tokens_details=OutputTokensDetails(reasoning_tokens=64), total_tokens=459), user=None, billing={'payer': 'developer'}, store=True)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ask_qa(\"what is vector db?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 184
    },
    "id": "elIg_EATzbDr",
    "outputId": "fa4cd9a6-24e6-475d-d115-55d6316ab8c5"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'A vector database (vector DB) is a specialized database designed to store and search high-dimensional numeric vectors — typically embeddings produced by models that convert text, images, or other data into dense vectors. Instead of exact-match queries on keywords, a vector DB lets you find items that are semantically similar by comparing vector distances (e.g., cosine similarity, Euclidean distance).\\n\\nKey points:\\n- Purpose: Enable efficient semantic search and similarity matching for applications like semantic text search, recommendation, image retrieval, and Retrieval-Augmented Generation (RAG) for LLMs.\\n- How it works: You index embeddings (vector representations) of documents or items. At query time, you convert the query to an embedding and run a nearest-neighbor search to find the most similar stored vectors.\\n- Technology: Uses specialized indexes and algorithms (approximate nearest neighbor methods such as HNSW, IVF, PQ) to search quickly at scale.\\n- Benefits: Much better than keyword matching for finding relevant results by meaning; enables LLMs to access relevant context for more accurate responses.\\n- Limitations: Quality depends on the embedding model; must choose distance metric and indexing strategy carefully; storage and compute scale with dataset size.\\n\\nCommon vector DBs: Pinecone, Milvus, FAISS (library), Weaviate, and others.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ask_qa(\"what is vector db?\").output[1].content[0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-2Inyx_2FPuH"
   },
   "source": [
    "## Using Langsmith traceable decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "wzkrlI7Hsa54"
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from langsmith import traceable\n",
    "\n",
    "openai_client = OpenAI()\n",
    "\n",
    "@traceable(run_type=\"llm\", name=\"OpenAI call\")\n",
    "def call_llm(model, system_prompt,query):\n",
    "    result = openai_client.responses.create(\n",
    "            instructions=system_prompt,\n",
    "            input=query,\n",
    "            model=model,\n",
    "            reasoning={'effort':'low'} # For Reasoning Models\n",
    "        )\n",
    "    return result.output[1].content[0].text\n",
    "\n",
    "\n",
    "@traceable(run_type=\"retriever\")\n",
    "def retrieve_documents(inquiry: str):\n",
    "    docs = [\n",
    "        \"Vector databases enable efficient semantic search for LLMs.\",\n",
    "        \"Retrieval-Augmented Generation (RAG) improves LLM accuracy with relevant context.\",\n",
    "        \"Large Language Models (LLMs) have set new benchmarks in NLP tasks.\",\n",
    "        \"Transformer architectures have revolutionized many areas of machine learning.\",\n",
    "        \"Embedding models convert text into vector representations for similarity comparisons.\"\n",
    "    ]\n",
    "    return docs\n",
    "\n",
    "@traceable(run_type=\"chain\")\n",
    "def ask_qa(query):\n",
    "    context = retrieve_documents(query)\n",
    "    system_prompt = f\"\"\"\n",
    "    You are an assistant for question-answering tasks.\n",
    "    Use the following pieces of retrieved context to answer the question.\n",
    "    Context: {context}\n",
    "    \"\"\"\n",
    "\n",
    "    return call_llm(\"gpt-5-mini\", system_prompt,query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 184
    },
    "id": "IvUZtIjFFmyB",
    "outputId": "415208c5-80cb-45e4-f89d-cb28b6e9f3b1"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Large language models (LLMs) are machine learning models designed to understand and generate human language. Key points:\\n\\n- Architecture and training\\n  - Most modern LLMs use the transformer architecture, which enabled breakthroughs in language tasks through self-attention mechanisms.\\n  - They’re trained on very large text corpora (web pages, books, code, etc.) using unsupervised or self-supervised objectives (e.g., next-token prediction), which lets them learn grammar, facts, reasoning patterns, and linguistic structure.\\n\\n- What they can do\\n  - Generate fluent text, summarize, translate, answer questions, write code, extract information, and perform many other NLP tasks often at or above human benchmarks.\\n  - Produce vector embeddings: internal or external embedding models convert text into high-dimensional vectors useful for similarity comparisons.\\n\\n- How they’re used in systems\\n  - In many applications LLM outputs are combined with retrieval systems (Retrieval-Augmented Generation, or RAG) that fetch relevant documents and provide context to improve factual accuracy.\\n  - Vector databases are commonly used to store and search embeddings efficiently for semantic search and RAG pipelines.\\n\\n- Strengths and limitations\\n  - Strengths: versatile, strong few-shot and zero-shot performance, powerful language generation.\\n  - Limitations: can produce incorrect or misleading information (hallucinations), may reflect biases in training data, and can require large compute and data resources. Retrieval and grounding (e.g., RAG + vector DBs) help reduce factual errors.\\n\\nIn short, LLMs are powerful transformer-based models trained on massive text datasets to perform a wide range of language tasks; they’re often paired with embeddings, vector search, and retrieval techniques to improve accuracy and relevance.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ask_qa(\"What are large language models\", langsmith_extra={\"metadata\": {\"user\": \"test_user@gmail.com\"}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "sjEzBXy8aGdy"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
