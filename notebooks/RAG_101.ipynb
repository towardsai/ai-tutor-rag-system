{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## RAG 101"
      ],
      "metadata": {
        "id": "_PtvhdlwUmmb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_L_iOECPT-Ki",
        "outputId": "0075c6ca-8ee9-4fc7-bb31-2e2a1f959101"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/951.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m532.5/951.0 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m942.1/951.0 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m951.0/951.0 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q openai==1.107.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# Set the \"OPENAI_API_KEY\" in the Python environment. Will be used by OpenAI client later.\n",
        "# os.environ[\"OPENAI_API_KEY\"] = \"[OPENAI_API_KEY]\"\n",
        "\n",
        "from google.colab import userdata\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "nP6_Z0DJUerK"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "# Defining the \"client\" object that enables\n",
        "# us to connect to OpenAI API endpoints.\n",
        "client = OpenAI()"
      ],
      "metadata": {
        "id": "IPmDcnKlUiY5"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a user prompt with the user's question\n",
        "prompt = f\"How many parameters LLaMA 4 Models have?\"\n",
        "\n",
        "#Call the OpenAI API\n",
        "response = client.responses.create(\n",
        "  model=\"gpt-5-chat-latest\",\n",
        "  temperature=0.0,\n",
        "  input=prompt,\n",
        ")\n",
        "\n",
        "\n",
        "print(response.output[0].content[0].text.strip())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yplCUS2pUiU1",
        "outputId": "a3bb1c35-d8fb-4062-97f8-43875f663e5c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "As of now (knowledge cutoff: June 2024), **LLaMA 4 has not been released**. The latest publicly known version is **LLaMA 3**, which was released by Meta in April 2024.  \n",
            "\n",
            "The **LLaMA 3 models** come in the following parameter sizes:  \n",
            "- **8B (8 billion parameters)**  \n",
            "- **70B (70 billion parameters)**  \n",
            "\n",
            "Meta has also announced that **larger LLaMA 3.1 models (400B+ parameters)** are in development, but those are not yet released.  \n",
            "\n",
            "So to answer your question:  \n",
            "👉 **There are no official LLaMA 4 models yet, so we don’t know their parameter counts.**  \n",
            "\n",
            "Would you like me to give you a comparison of **LLaMA 2 vs LLaMA 3 parameter sizes** so you can see the progression and get an idea of what LLaMA 4 might look like when it arrives?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# An article about the release of LLaMA 4 model.\n",
        "# https://ai.meta.com/blog/llama-4-multimodal-intelligence/#. (Summarized Content)\n",
        "ARTICLE = \"\"\"\n",
        "Meta has unveiled the **Llama 4 herd**, a new generation of open-weight, natively multimodal AI models designed to push the boundaries of performance, efficiency, and accessibility. The release includes **Llama 4 Scout** and **Llama 4 Maverick**, alongside a preview of the massive **Llama 4 Behemoth** teacher model. These models introduce advanced **mixture-of-experts (MoE) architectures**, native text–image integration, and record-breaking context lengths, establishing Llama 4 as a major leap in multimodal AI innovation.\n",
        "**Llama 4 Scout** is a **17B active parameter model with 16 experts** (109B total parameters) that offers **10 million token context length**, far exceeding Llama 3’s 128K limit. Scout’s architecture leverages **interleaved attention layers (iRoPE)** and inference-time temperature scaling to achieve superior length generalization, enabling complex tasks like multi-document summarization, codebase reasoning, and long-context retrieval. Despite its smaller size, Scout surpasses prior Llama models and competitors such as Gemma 3 and Gemini 2.0 Flash-Lite in performance, all while being deployable on a **single NVIDIA H100 GPU**. Its multimodal training allows strong image grounding, multi-image reasoning, and visual question answering.\n",
        "**Llama 4 Maverick**, also with **17B active parameters**, uses **128 experts** and totals **400B parameters**, alternating dense and MoE layers for inference efficiency. It rivals or exceeds larger models like **GPT-4o, Gemini 2.0 Flash, and DeepSeek v3** on benchmarks for coding, reasoning, multilinguality, and multimodal tasks. Its efficient design makes it deployable on a single **NVIDIA H100 DGX host**, balancing performance with cost-effectiveness. Maverick was refined through a revamped **post-training pipeline**—lightweight supervised fine-tuning, continuous **online reinforcement learning (RL)** with adaptive difficulty filtering, and direct preference optimization—resulting in superior reasoning, conversational fluency, and multimodal understanding. Maverick’s **chat version achieved an ELO score of 1417 on LMArena**, reflecting best-in-class general assistant capabilities.\n",
        "At the top of the hierarchy, **Llama 4 Behemoth** serves as the **teacher model**, with **288B active parameters, 16 experts, and nearly 2 trillion total parameters**. It outperforms **GPT-4.5, Claude Sonnet 3.7, and Gemini 2.0 Pro** on STEM benchmarks like MATH-500 and GPQA Diamond. Behemoth’s scale demanded innovative training strategies, including pruning 95% of supervised fine-tuning data, advanced reinforcement learning with dynamically filtered prompts, and an optimized asynchronous MoE-parallelized RL infrastructure. These techniques boosted reasoning, coding, and multilingual capabilities while maintaining instruction-following reliability.\n",
        "All Llama 4 models are **natively multimodal**, trained with large-scale text, image, and video data, and leverage **Meta’s MetaP technique** for reliable hyperparameter scaling. They support **200 languages**, with 10x more multilingual tokens than Llama 3, and employ **FP8 precision** for efficient training across trillions of tokens. Safety remains a priority: Meta integrates **Llama Guard**, **Prompt Guard**, and **CyberSecEval** for content protection, while **Generative Offensive Agent Testing (GOAT)** automates adversarial red-teaming. Llama 4 also significantly reduces **political and social bias**, refusing fewer prompts and responding more neutrally than Llama 3.\n",
        "In sum, **Llama 4 Scout, Maverick, and Behemoth** represent a major leap in open AI research: compact yet powerful models with unmatched context length, multimodal fluency, reasoning power, and efficiency. By making Scout and Maverick openly available on **llama.com and Hugging Face**, Meta empowers developers, enterprises, and researchers worldwide to build the next generation of AI-driven experiences.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "a1XgODXdUiN3"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a user prompt with the user's question\n",
        "prompt = f\"Use the following article as the source and answer the question:\\n\\n{ARTICLE}\\n\\nHow many parameters LLaMA 4 Models have?\"\n",
        "\n",
        "# Call the OpenAI API\n",
        "response = client.responses.create(\n",
        "        model=\"gpt-5-chat-latest\",\n",
        "        temperature=0.0,\n",
        "        input=prompt,\n",
        "    )\n",
        "\n",
        "print(response.output[0].content[0].text.strip())"
      ],
      "metadata": {
        "id": "KnF52b8IUiEv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc3e395f-12aa-4126-d942-3c3a94e85d0d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Based on the article, here’s the breakdown of **LLaMA 4 model parameters**:\n",
            "\n",
            "- **Llama 4 Scout**  \n",
            "  - **17B active parameters**  \n",
            "  - **109B total parameters** (with 16 experts)  \n",
            "\n",
            "- **Llama 4 Maverick**  \n",
            "  - **17B active parameters**  \n",
            "  - **400B total parameters** (with 128 experts)  \n",
            "\n",
            "- **Llama 4 Behemoth (teacher model)**  \n",
            "  - **288B active parameters**  \n",
            "  - **~2 trillion total parameters** (with 16 experts)  \n",
            "\n",
            "✅ **Summary:**  \n",
            "- Scout → 17B active / 109B total  \n",
            "- Maverick → 17B active / 400B total  \n",
            "- Behemoth → 288B active / ~2T total  \n",
            "\n",
            "Would you like me to also create a **comparison table** so it’s easier to see the differences at a glance?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len( ARTICLE.split(\" \") )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vyUkCv0yVEZq",
        "outputId": "6fa2ad02-a1b2-4872-e0c1-3668f3337ae6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "506"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3ftx17EOS8hq"
      },
      "execution_count": 7,
      "outputs": []
    }
  ]
}