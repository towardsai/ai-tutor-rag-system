{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sZspRwqMNYfw"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/towardsai/ai-tutor-rag-system/blob/main/notebooks/RAG_101.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_PtvhdlwUmmb"
   },
   "source": [
    "## RAG 101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_L_iOECPT-Ki",
    "outputId": "0075c6ca-8ee9-4fc7-bb31-2e2a1f959101"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/951.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m532.5/951.0 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m942.1/951.0 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m951.0/951.0 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -q openai==1.107.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nP6_Z0DJUerK"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from google.colab import userdata\n",
    "\n",
    "# Set the \"OPENAI_API_KEY\" in the Python environment. Will be used by OpenAI client later.\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"[OPENAI_API_KEY]\"\n",
    "\n",
    "from google.colab import userdata\n",
    "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IPmDcnKlUiY5"
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Defining the \"client\" object that enables\n",
    "# us to connect to OpenAI API endpoints.\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yplCUS2pUiU1",
    "outputId": "a3bb1c35-d8fb-4062-97f8-43875f663e5c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As of now (knowledge cutoff: June 2024), **LLaMA 4 has not been released**. The latest publicly known version is **LLaMA 3**, which was released by Meta in April 2024.  \n",
      "\n",
      "The **LLaMA 3 models** come in the following parameter sizes:  \n",
      "- **8B (8 billion parameters)**  \n",
      "- **70B (70 billion parameters)**  \n",
      "\n",
      "Meta has also announced that **larger LLaMA 3.1 models (400B+ parameters)** are in development, but those are not yet released.  \n",
      "\n",
      "So to answer your question:  \n",
      "👉 **There are no official LLaMA 4 models yet, so we don’t know their parameter counts.**  \n",
      "\n",
      "Would you like me to give you a comparison of **LLaMA 2 vs LLaMA 3 parameter sizes** so you can see the progression and get an idea of what LLaMA 4 might look like when it arrives?\n"
     ]
    }
   ],
   "source": [
    "# Create a user prompt with the user's question\n",
    "prompt = f\"How many parameters LLaMA 4 Models have?\"\n",
    "\n",
    "#Call the OpenAI API\n",
    "response = client.responses.create(\n",
    "  model=\"gpt-5-chat-latest\",\n",
    "  temperature=0.0,\n",
    "  input=prompt,\n",
    ")\n",
    "\n",
    "\n",
    "print(response.output[0].content[0].text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a1XgODXdUiN3"
   },
   "outputs": [],
   "source": [
    "# An article about the release of LLaMA 4 model.\n",
    "# https://ai.meta.com/blog/llama-4-multimodal-intelligence/#. (Summarized Content)\n",
    "ARTICLE = \"\"\"\n",
    "Meta has unveiled the **Llama 4 herd**, a new generation of open-weight, natively multimodal AI models designed to push the boundaries of performance, efficiency, and accessibility. The release includes **Llama 4 Scout** and **Llama 4 Maverick**, alongside a preview of the massive **Llama 4 Behemoth** teacher model. These models introduce advanced **mixture-of-experts (MoE) architectures**, native text–image integration, and record-breaking context lengths, establishing Llama 4 as a major leap in multimodal AI innovation.\n",
    "**Llama 4 Scout** is a **17B active parameter model with 16 experts** (109B total parameters) that offers **10 million token context length**, far exceeding Llama 3’s 128K limit. Scout’s architecture leverages **interleaved attention layers (iRoPE)** and inference-time temperature scaling to achieve superior length generalization, enabling complex tasks like multi-document summarization, codebase reasoning, and long-context retrieval. Despite its smaller size, Scout surpasses prior Llama models and competitors such as Gemma 3 and Gemini 2.0 Flash-Lite in performance, all while being deployable on a **single NVIDIA H100 GPU**. Its multimodal training allows strong image grounding, multi-image reasoning, and visual question answering.\n",
    "**Llama 4 Maverick**, also with **17B active parameters**, uses **128 experts** and totals **400B parameters**, alternating dense and MoE layers for inference efficiency. It rivals or exceeds larger models like **GPT-4o, Gemini 2.0 Flash, and DeepSeek v3** on benchmarks for coding, reasoning, multilinguality, and multimodal tasks. Its efficient design makes it deployable on a single **NVIDIA H100 DGX host**, balancing performance with cost-effectiveness. Maverick was refined through a revamped **post-training pipeline**—lightweight supervised fine-tuning, continuous **online reinforcement learning (RL)** with adaptive difficulty filtering, and direct preference optimization—resulting in superior reasoning, conversational fluency, and multimodal understanding. Maverick’s **chat version achieved an ELO score of 1417 on LMArena**, reflecting best-in-class general assistant capabilities.\n",
    "At the top of the hierarchy, **Llama 4 Behemoth** serves as the **teacher model**, with **288B active parameters, 16 experts, and nearly 2 trillion total parameters**. It outperforms **GPT-4.5, Claude Sonnet 3.7, and Gemini 2.0 Pro** on STEM benchmarks like MATH-500 and GPQA Diamond. Behemoth’s scale demanded innovative training strategies, including pruning 95% of supervised fine-tuning data, advanced reinforcement learning with dynamically filtered prompts, and an optimized asynchronous MoE-parallelized RL infrastructure. These techniques boosted reasoning, coding, and multilingual capabilities while maintaining instruction-following reliability.\n",
    "All Llama 4 models are **natively multimodal**, trained with large-scale text, image, and video data, and leverage **Meta’s MetaP technique** for reliable hyperparameter scaling. They support **200 languages**, with 10x more multilingual tokens than Llama 3, and employ **FP8 precision** for efficient training across trillions of tokens. Safety remains a priority: Meta integrates **Llama Guard**, **Prompt Guard**, and **CyberSecEval** for content protection, while **Generative Offensive Agent Testing (GOAT)** automates adversarial red-teaming. Llama 4 also significantly reduces **political and social bias**, refusing fewer prompts and responding more neutrally than Llama 3.\n",
    "In sum, **Llama 4 Scout, Maverick, and Behemoth** represent a major leap in open AI research: compact yet powerful models with unmatched context length, multimodal fluency, reasoning power, and efficiency. By making Scout and Maverick openly available on **llama.com and Hugging Face**, Meta empowers developers, enterprises, and researchers worldwide to build the next generation of AI-driven experiences.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KnF52b8IUiEv",
    "outputId": "cc3e395f-12aa-4126-d942-3c3a94e85d0d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the article, here’s the breakdown of **LLaMA 4 model parameters**:\n",
      "\n",
      "- **Llama 4 Scout**  \n",
      "  - **17B active parameters**  \n",
      "  - **109B total parameters** (with 16 experts)  \n",
      "\n",
      "- **Llama 4 Maverick**  \n",
      "  - **17B active parameters**  \n",
      "  - **400B total parameters** (with 128 experts)  \n",
      "\n",
      "- **Llama 4 Behemoth (teacher model)**  \n",
      "  - **288B active parameters**  \n",
      "  - **~2 trillion total parameters** (with 16 experts)  \n",
      "\n",
      "✅ **Summary:**  \n",
      "- Scout → 17B active / 109B total  \n",
      "- Maverick → 17B active / 400B total  \n",
      "- Behemoth → 288B active / ~2T total  \n",
      "\n",
      "Would you like me to also create a **comparison table** so it’s easier to see the differences at a glance?\n"
     ]
    }
   ],
   "source": [
    "# Create a user prompt with the user's question\n",
    "prompt = f\"Use the following article as the source and answer the question:\\n\\n{ARTICLE}\\n\\nHow many parameters LLaMA 4 Models have?\"\n",
    "\n",
    "# Call the OpenAI API\n",
    "response = client.responses.create(\n",
    "        model=\"gpt-5-chat-latest\",\n",
    "        temperature=0.0,\n",
    "        input=prompt,\n",
    "    )\n",
    "\n",
    "print(response.output[0].content[0].text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vyUkCv0yVEZq",
    "outputId": "6fa2ad02-a1b2-4872-e0c1-3668f3337ae6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "506"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len( ARTICLE.split(\" \") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3ftx17EOS8hq"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
