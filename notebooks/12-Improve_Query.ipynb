{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zE1h0uQV7uT"
      },
      "source": [
        "# Install Packages and Setup Variables\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": true,
        "id": "QPJzr-I9XQ7l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e69de59d-41ea-4f67-fd3f-584d03f3f44d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/56.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.9/81.9 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m90.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.9/42.9 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m455.6/455.6 kB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.9/82.9 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m259.5/259.5 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m58.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.0/19.0 MB\u001b[0m \u001b[31m87.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.6/442.6 kB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.9/79.9 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m69.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m89.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m70.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m105.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m266.8/266.8 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/41.0 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.1/211.1 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m93.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.8/65.8 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.7/55.7 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.2/196.2 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.5/118.5 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m89.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.1/45.1 MB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.4/103.4 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m83.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m102.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m75.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m44.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m81.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m186.1/186.1 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.1/374.1 kB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m304.2/304.2 kB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.4/107.4 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m499.2/499.2 kB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m85.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m453.1/453.1 kB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.4/207.4 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.4/115.4 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.3/129.3 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for html2text (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for tinysegmenter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for spider-client (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for feedfinder2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for jieba3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q llama-index==0.12.37 openai==1.59.8 llama-index-finetuning==0.3.1 llama-index-embeddings-huggingface==0.5.1 llama-index-embeddings-cohere==0.4.0 \\\n",
        "                llama-index-readers-web==0.3.5 cohere==5.15.0 tiktoken==0.8.0 chromadb==1.0.11 html2text==2024.2.26 sentence-transformers==4.1.0 pydantic==2.11.0 \\\n",
        "                llama-index-vector-stores-chroma==0.4.2 kaleido==0.2.1 llama-index-llms-gemini==0.4.1 typing_extensions==4.14.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "riuXwpSPcvWC"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Set the following API Keys in the Python environment. Will be used later.\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"<YOUR-OPENAI-API-KEY>\"\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"<YOUR-GOOGLE-API-KEY>\"\n",
        "\n",
        "# from google.colab import userdata\n",
        "# os.environ[\"OPENAI_API_KEY\"] = userdata.get('openai_api_key')\n",
        "# os.environ[\"GOOGLE_API_KEY\"] = userdata.get('Google_api_key')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "jIEeZzqLbz0J"
      },
      "outputs": [],
      "source": [
        "# Allows running asyncio in environments with an existing event loop, like Jupyter notebooks.\n",
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bkgi2OrYzF7q"
      },
      "source": [
        "# Load a Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "9oGT6crooSSj"
      },
      "outputs": [],
      "source": [
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "from llama_index.llms.openai import OpenAI\n",
        "from llama_index.core import Settings\n",
        "\n",
        "Settings.llm = OpenAI(temperature=0, model=\"gpt-4o-mini\")\n",
        "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWaT6rL7ksp8"
      },
      "source": [
        "# Load Indexes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "16829ce67fd84a93a7c08940221152ff",
            "dcd8bd8429c44c60995e984b64c1ad11",
            "3a64b950b37b48e5ac62cab0a479bf25",
            "dc7f81bde2854c47a0554644c22e0b74",
            "a85e4a12564044e48ff32267f145b050",
            "e265399904284a0a97bbb548597398e7",
            "d8b45ee1cc2f41b0a46aa7d652a52731",
            "fd5b7d8f97784d19ba137dcf5fd27319",
            "04d4b9eadc0c4cea92643d644cdbe527",
            "16dd317689d14edfa1e34ba5645a9125",
            "b8418b4e6fca4be98c6289ef364692d4"
          ]
        },
        "id": "5sP-_zZso337",
        "outputId": "b90e7771-f843-48e6-ec73-83a4f956f9fc"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vectorstore.zip:   0%|          | 0.00/97.2M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "16829ce67fd84a93a7c08940221152ff"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "vectorstore = hf_hub_download(repo_id=\"jaiganesan/ai_tutor_knowledge\", filename=\"vectorstore.zip\", repo_type=\"dataset\", local_dir=\".\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SodY2Xpf_kxg",
        "outputId": "607ce46f-cf33-47fa-e942-6dc6509815ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  vectorstore.zip\n",
            "   creating: ai_tutor_knowledge/\n",
            "   creating: ai_tutor_knowledge/684af133-f877-4230-bde4-575cf53b6688/\n",
            "  inflating: ai_tutor_knowledge/684af133-f877-4230-bde4-575cf53b6688/length.bin  \n",
            "  inflating: ai_tutor_knowledge/684af133-f877-4230-bde4-575cf53b6688/index_metadata.pickle  \n",
            "  inflating: ai_tutor_knowledge/684af133-f877-4230-bde4-575cf53b6688/link_lists.bin  \n",
            "  inflating: ai_tutor_knowledge/684af133-f877-4230-bde4-575cf53b6688/header.bin  \n",
            "  inflating: ai_tutor_knowledge/684af133-f877-4230-bde4-575cf53b6688/data_level0.bin  \n",
            "  inflating: ai_tutor_knowledge/chroma.sqlite3  \n"
          ]
        }
      ],
      "source": [
        "!unzip -o vectorstore.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "mXi56KTXk2sp"
      },
      "outputs": [],
      "source": [
        "import chromadb\n",
        "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
        "from llama_index.core import VectorStoreIndex\n",
        "\n",
        "# Create the vector index\n",
        "db = chromadb.PersistentClient(path=\"./ai_tutor_knowledge\")\n",
        "chroma_collection = db.get_or_create_collection(\"ai_tutor_knowledge\")\n",
        "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
        "vector_index = VectorStoreIndex.from_vector_store(vector_store)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLrn8A3jckmW"
      },
      "source": [
        "# Multi-Step Query Engine\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UmpfpVCje8h3"
      },
      "source": [
        "## GPT-4o-mini\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "8y-Ya3GyfcAk"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.indices.query.query_transform.base import (\n",
        "    StepDecomposeQueryTransform,\n",
        ")\n",
        "\n",
        "step_decompose_transform_gpt4o = StepDecomposeQueryTransform(verbose=True, llm=Settings.llm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "zntXdSbGf_qF"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.query_engine.multistep_query_engine import MultiStepQueryEngine\n",
        "\n",
        "#Default query engine\n",
        "query_engine_gpt4o_mini = vector_index.as_query_engine()\n",
        "\n",
        "# Multi Step Query Engine\n",
        "multi_step_query_engine = MultiStepQueryEngine(\n",
        "    query_engine = query_engine_gpt4o_mini,\n",
        "    query_transform = step_decompose_transform_gpt4o,\n",
        "    index_summary = \"Used to answer the Questions about RAG, Machine Learning, Deep Learning, and Generative AI, Note: Don't repeat the Same quesion\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JPD8yAinVSq"
      },
      "source": [
        "# Query Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2IByQ5-ox9U"
      },
      "source": [
        "## Default"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0gue7cyctt1",
        "outputId": "b6726f3c-583f-4b40-81ed-b6659a653973"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The LLaMA model is a foundational model designed for various natural language processing tasks. It can be fine-tuned using the Parameter-Efficient Fine-Tuning (PEFT) library, which includes methods like LoRA (Low-Rank Adaptation) and Llama-Adapter. LoRA allows for efficient fine-tuning by introducing a small number of learnable parameters, making it suitable for scenarios with limited computational resources. Llama-Adapter specifically transforms the LLaMA model into an instruction-following model by integrating learnable adaptation prompts while preserving the model's pre-trained knowledge.\n",
            "\n",
            "BERT, another prominent model in the NLP landscape, is known for its bidirectional training approach, which allows it to understand context from both directions in a sentence. While BERT is not directly mentioned in the provided context, it is often used in conjunction with fine-tuning techniques similar to those offered by the PEFT library.\n",
            "\n",
            "Overall, the PEFT methods provide efficient ways to adapt large models like LLaMA and potentially BERT for specific tasks without the need for extensive computational resources or time-consuming training processes.\n"
          ]
        }
      ],
      "source": [
        "# Default query engine\n",
        "query_engine = vector_index.as_query_engine()\n",
        "res = query_engine.query(\"Write about Llama 3.1 Model, BERT and PEFT methods\")\n",
        "print(res.response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "465dH4yQc7Ct",
        "outputId": "7935c657-ce34-4b91-c8d6-71161b253a81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Node ID\t 781b7b12-eca2-47c0-a66e-9d6be670e951\n",
            "Title\t LLaMA\n",
            "Text\t on how to fine-tune LLaMA model using LoRA method via the 🤗 PEFT library with intuitive UI. 🌎 - A [notebook](https://github.com/aws/amazon-sagemaker-examples/blob/main/introduction_to_amazon_algorithms/jumpstart-foundation-models/text-generation-open-llama.ipynb) on how to deploy Open-LLaMA model for text generation on Amazon SageMaker. 🌎 ## LlamaConfig[[autodoc]] LlamaConfig## LlamaTokenizer[[autodoc]] LlamaTokenizer    - build_inputs_with_special_tokens    - get_special_tokens_mask    - create_token_type_ids_from_sequences    - save_vocabulary## LlamaTokenizerFast[[autodoc]] LlamaTokenizerFast    - build_inputs_with_special_tokens    - get_special_tokens_mask    - create_token_type_ids_from_sequences    - update_post_processor    - save_vocabulary## LlamaModel[[autodoc]] LlamaModel    - forward## LlamaForCausalLM[[autodoc]] LlamaForCausalLM    - forward## LlamaForSequenceClassification[[autodoc]] LlamaForSequenceClassification    - forward## LlamaForQuestionAnswering[[autodoc]] LlamaForQuestionAnswering    - forward## LlamaForTokenClassification[[autodoc]] LlamaForTokenClassification    - forward## FlaxLlamaModel[[autodoc]] FlaxLlamaModel    - __call__## FlaxLlamaForCausalLM[[autodoc]] FlaxLlamaForCausalLM    - __call__\n",
            "Score\t 0.43887592282330873\n",
            "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Node ID\t e1e3e842-7160-40c4-8e74-772fb8254f5e\n",
            "Title\t Llama-Adapter\n",
            "Text\t # Llama-Adapter[Llama-Adapter](https://hf.co/papers/2303.16199) is a PEFT method specifically designed for turning Llama into an instruction-following model. The Llama model is frozen and only a set of adaptation prompts prefixed to the input instruction tokens are learned. Since randomly initialized modules inserted into the model can cause the model to lose some of its existing knowledge, Llama-Adapter uses zero-initialized attention with zero gating to progressively add the instructional prompts to the model.The abstract from the paper is:*We present LLaMA-Adapter, a lightweight adaption method to efficiently fine-tune LLaMA into an instruction-following model. Using 52K self-instruct demonstrations, LLaMA-Adapter only introduces 1.2M learnable parameters upon the frozen LLaMA 7B model, and costs less than one hour for fine-tuning on 8 A100 GPUs. Specifically, we adopt a set of learnable adaption prompts, and prepend them to the input text tokens at higher transformer layers. Then, a zero-init attention mechanism with zero gating is proposed, which adaptively injects the new instructional cues into LLaMA, while effectively preserves its pre-trained knowledge. With efficient training, LLaMA-Adapter generates high-quality responses, comparable to Alpaca with fully fine-tuned 7B parameters. Furthermore, our approach can be simply extended to multi-modal input, e.g., images, for image-conditioned LLaMA, which achieves superior reasoning capacity on ScienceQA. We release our code at https://github.com/ZrrSkywalker/LLaMA-Adapter*.## AdaptionPromptConfig[[autodoc]] tuners.adaption_prompt.config.AdaptionPromptConfig## AdaptionPromptModel[[autodoc]] tuners.adaption_prompt.model.AdaptionPromptModel\n",
            "Score\t 0.4378040851975304\n",
            "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n"
          ]
        }
      ],
      "source": [
        "for src in res.source_nodes:\n",
        "    print(\"Node ID\\t\", src.node_id)\n",
        "    print(\"Title\\t\", src.metadata[\"title\"])\n",
        "    print(\"Text\\t\", src.text)\n",
        "    print(\"Score\\t\", src.score)\n",
        "    print(\"-_\" * 20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2y2AiInmpz7g"
      },
      "source": [
        "## GPT-4o-mini Multi-Step\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69kADAFilW1n",
        "outputId": "12c0c460-f7ef-4963-b5fc-2ec2bcbeda0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;3;33m> Current query: Write about Llama 3.1 Model, BERT and PEFT methods\n",
            "\u001b[0m\u001b[1;3;38;5;200m> New query: What are the key features of the Llama 3.1 Model?\n",
            "\u001b[0m\u001b[1;3;33m> Current query: Write about Llama 3.1 Model, BERT and PEFT methods\n",
            "\u001b[0m\u001b[1;3;38;5;200m> New query: What are the key features of the Llama 3.1 Model?\n",
            "\u001b[0m\u001b[1;3;33m> Current query: Write about Llama 3.1 Model, BERT and PEFT methods\n",
            "\u001b[0m\u001b[1;3;38;5;200m> New query: What are the key features of BERT?\n",
            "\u001b[0mThe Llama 3.1 model is a state-of-the-art AI language model known for its impressive scale and capabilities. It is the largest model developed by Meta, trained on over 15 trillion tokens using more than 16,000 H100 GPUs. One of its standout features is the extended context length of 128K, which allows it to process longer inputs effectively. The model exhibits enhanced reasoning and coding abilities, supports zero-shot tool use, and has a strong multilingual processing capability, with about 50% of its training data being multilingual. Additionally, Llama 3.1 excels in programming and reasoning skills, generating high-quality code and demonstrating strong analytical capabilities. It also outperforms other models in various benchmarks, particularly in mathematical reasoning and complex tasks.\n",
            "\n",
            "BERT, or Bidirectional Encoder Representations from Transformers, is another influential model in the field of natural language processing. It is distinguished by its bidirectional processing, which allows it to understand the full context of a sentence by analyzing text in both directions simultaneously. BERT employs an encoder-only transformer architecture to generate contextualized word embeddings. It is pre-trained on two main tasks: Masked Language Modeling (MLM) and Next Sentence Prediction (NSP), which help the model learn contextual relationships and sentence structures. BERT comes in two variants, BERT BASE and BERT LARGE, differing in the number of layers and parameters.\n",
            "\n",
            "PEFT, or Parameter-Efficient Fine-Tuning, refers to methods that allow for fine-tuning large pre-trained models like Llama 3.1 and BERT with fewer parameters. This approach is particularly useful for adapting models to specific tasks without the need for extensive computational resources. PEFT techniques can include methods like adapters, prompt tuning, and low-rank adaptation, which enable efficient training while maintaining the performance of the original model. These methods are increasingly important as they allow for the deployment of powerful models in resource-constrained environments.\n"
          ]
        }
      ],
      "source": [
        "response = multi_step_query_engine.query(\"Write about Llama 3.1 Model, BERT and PEFT methods\")\n",
        "print(response.response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wGmHP72Iu10s",
        "outputId": "9747c592-924b-4086-9583-ba1ae3fcb54e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**What are the key features of the Llama 3.1 Model?**\n",
            "The Llama 3.1 model boasts several key features, including:\n",
            "\n",
            "1. **Model Scale and Training**: It is the largest model from Meta, trained on over 15 trillion tokens using more than 16,000 H100 GPUs.\n",
            "\n",
            "2. **Extended Context Length**: The model supports a context length of 128K, enhancing its ability to handle longer inputs.\n",
            "\n",
            "3. **Enhanced Capabilities**: It demonstrates improved reasoning and coding abilities compared to its predecessors.\n",
            "\n",
            "4. **Tool Use and Agentic Behaviors**: The model supports zero-shot tool use, allowing it to perform tasks without prior training on specific tools.\n",
            "\n",
            "5. **Multilingual Processing**: Approximately 50% of its training data consists of multilingual tokens, enabling effective understanding and processing of multiple languages.\n",
            "\n",
            "6. **Programming and Reasoning Skills**: Llama 3.1 excels in generating high-quality code and exhibits strong logical reasoning, problem-solving, and analytical capabilities.\n",
            "\n",
            "7. **Benchmark Performance**: It outperforms competing models like GPT-4o and Claude 3.5 in areas such as mathematical reasoning, complex reasoning, multilingual support, and long text processing.\n",
            "\n",
            "These features collectively position Llama 3.1 as a powerful tool in the landscape of AI language models.\n",
            "\n",
            "**What are the key features of the Llama 3.1 Model?**\n",
            "The Llama 3.1 model boasts several key features, including:\n",
            "\n",
            "1. **Model Scale and Training**: It is the largest model from Meta, trained on over 15 trillion tokens using more than 16,000 H100 GPUs.\n",
            "\n",
            "2. **Extended Context Length**: The model supports a context length of 128K, enhancing its ability to handle longer inputs.\n",
            "\n",
            "3. **Enhanced Capabilities**: It demonstrates improved reasoning and coding abilities compared to its predecessors.\n",
            "\n",
            "4. **Tool Use and Agentic Behaviors**: The model supports zero-shot tool use, allowing it to perform tasks without prior training on specific tools.\n",
            "\n",
            "5. **Multilingual Processing**: Approximately 50% of its training data consists of multilingual tokens, enabling effective understanding and processing of multiple languages.\n",
            "\n",
            "6. **Programming and Reasoning Skills**: Llama 3.1 excels in generating high-quality code and exhibits strong logical reasoning, problem-solving, and analytical capabilities.\n",
            "\n",
            "7. **Benchmark Performance**: It outperforms competing models like GPT-4o and Claude 3.5 Sonnet in areas such as mathematical reasoning, complex reasoning, multilingual support, and long text processing.\n",
            "\n",
            "These features collectively position Llama 3.1 as a powerful tool in the landscape of AI language models.\n",
            "\n",
            "**What are the key features of BERT?**\n",
            "BERT, or Bidirectional Encoder Representations from Transformers, is characterized by several key features:\n",
            "\n",
            "1. **Bidirectional Processing**: BERT processes text in both directions simultaneously, allowing it to capture the full context of a sentence. This contrasts with traditional unidirectional models that read text sequentially, which can lead to ambiguity in understanding word meanings.\n",
            "\n",
            "2. **Transformer Architecture**: BERT utilizes an encoder-only transformer architecture, which is designed to generate contextualized word embeddings. This architecture enhances its ability to understand and generate human-like language.\n",
            "\n",
            "3. **Pre-training Tasks**: BERT is pre-trained on two main tasks:\n",
            "   - **Masked Language Modeling (MLM)**: This involves predicting masked tokens in sentences, helping the model learn contextualized representations based on surrounding words.\n",
            "   - **Next Sentence Prediction (NSP)**: This task involves predicting whether one sentence is likely to follow another, which aids in understanding sentence relationships.\n",
            "\n",
            "4. **Model Variants**: BERT comes in two versions: BERT BASE, which has 12 layers and 110 million parameters, and BERT LARGE, which has 24 layers and 340 million parameters.\n",
            "\n",
            "These features collectively contribute to BERT's effectiveness in various natural language processing tasks.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for query, response in response.metadata['sub_qa']:\n",
        "    print(f\"**{query}**\\n{response}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k5pJPBPRqjbG",
        "outputId": "21c7fe9e-f439-4594-efd7-c7811e8c7984"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Node ID\t a6ebe22b-5529-4e78-93a6-e7336d031600\n",
            "Text\t GPT-2  GPT-3  and GPT-4  which were decoder-only architectures. Another well-known example is BERT (Bidirectional Encoder Representations from Transformers)  an encoder-only transformer mode used as a component in sentence embedding models.   Lets talk about BERT!BERT stands for Bidirectional Encoder Representations from Transformers. It is a language model by Google that uses a transformer architecture to understand and generate human-like language. BERT is designed to simultaneously process text in both directions  allowing it to capture context more effectively than traditional unidirectional models  which read text sequentially from left to right or right to left.   Example of Bidirectional CapabilityConsider the sentence:    The bank is situated on the _______ of the river.   In a unidirectional model  understanding the blank would primarily rely on the words before it  potentially leading to ambiguity about whether bank refers to a financial institution or the side of a river.   However  BERTs bidirectional approach allows it to use the entire sentences context  including the words before and after the blank. Thus  the missing word is likely related to the river  resulting in a more accurate prediction  such as bank referring to the riverbank rather than a financial institution.   BERT has two versions:   BERT BASE with    Layers: 12Parameters: 110MAttention Heads: 12Hidden Units: 768BERT LARGE with    Layers: 24Parameters: 340MAttention Heads: 12Hidden Units: 1024DYK?BERT was pre-trained on 3.3 Billion words!   What was it pre-trained on? For what?BERT was pre-trained on two tasks:   Masked Language Modeling (MLM):The inputs are sentences that start with a special token called CLS (Classify Token) and end with a SEP (separator token).   Words  tokens (consider)   Around 15% of the input tokens are masked  and the model is trained to predict those masked tokens.   The model learns to produce contextualized vectors based on the surrounding words at this stage. Read the example above and reread this sentence.   Next Sentence Prediction (NSP):In this one  the model predicts if one sentence is likely to follow another.\n",
            "Score\t 0.47732872464031784\n",
            "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Node ID\t 5c948017-64fe-4457-9d22-548f03306e1e\n",
            "Text\t # DeBERTa-v2## OverviewThe DeBERTa model was proposed in [DeBERTa: Decoding-enhanced BERT with Disentangled Attention](https://arxiv.org/abs/2006.03654) by Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen It is based on Google'sBERT model released in 2018 and Facebook's RoBERTa model released in 2019.It builds on RoBERTa with disentangled attention and enhanced mask decoder training with half of the data used inRoBERTa.The abstract from the paper is the following:*Recent progress in pre-trained neural language models has significantly improved the performance of many naturallanguage processing (NLP) tasks. In this paper we propose a new model architecture DeBERTa (Decoding-enhanced BERT withdisentangled attention) that improves the BERT and RoBERTa models using two novel techniques. The first is thedisentangled attention mechanism, where each word is represented using two vectors that encode its content andposition, respectively, and the attention weights among words are computed using disentangled matrices on theircontents and relative positions. Second, an enhanced mask decoder is used to replace the output softmax layer topredict the masked tokens for model pretraining. We show that these two techniques significantly improve the efficiencyof model pretraining and performance of downstream tasks. Compared to RoBERTa-Large, a DeBERTa model trained on half ofthe training data performs consistently better on a wide range of NLP tasks, achieving improvements on MNLI by +0.9%(90.2% vs. 91.1%), on SQuAD v2.0 by +2.3% (88.4% vs. 90.7%) and RACE by +3.6% (83.2% vs. 86.8%). The DeBERTa code andpre-trained models will be made publicly available at https://github.com/microsoft/DeBERTa.*The following information is visible directly on the [original implementationrepository](https://github.com/microsoft/DeBERTa). DeBERTa v2 is the second version of the DeBERTa model. It includesthe 1.5B model used for the SuperGLUE single-model submission and achieving 89.9, versus human baseline 89.8. You canfind more details about this\n",
            "Score\t 0.44709040168047093\n",
            "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n"
          ]
        }
      ],
      "source": [
        "for src in response.source_nodes:\n",
        "    print(\"Node ID\\t\", src.node_id)\n",
        "    print(\"Text\\t\", src.text)\n",
        "    print(\"Score\\t\", src.score)\n",
        "    print(\"-_\" * 20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwcSCiMhp4Uh"
      },
      "source": [
        "# Test gemini-1.5-flash Multi-Step\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uH9gNfZuslHK"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.indices.query.query_transform.base import StepDecomposeQueryTransform\n",
        "from llama_index.core.query_engine.multistep_query_engine import MultiStepQueryEngine\n",
        "from llama_index.llms.gemini import Gemini\n",
        "\n",
        "llm_gemini = Gemini(model=\"models/gemini-1.5-flash\")\n",
        "\n",
        "step_decompose_transform = StepDecomposeQueryTransform(llm=llm_gemini, verbose=True)\n",
        "\n",
        "query_engine_gemini = vector_index.as_query_engine(llm=llm_gemini)\n",
        "\n",
        "query_engine_gemini = MultiStepQueryEngine(\n",
        "    query_engine=query_engine_gemini,\n",
        "    query_transform=step_decompose_transform,\n",
        "    index_summary=\"Used to answer the Questions about RAG, Machine Learning, Deep Learning,LLMs and Generative AI\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "id": "9s6SkHI0p6VZ",
        "outputId": "a1375ae4-9320-401f-fd56-4da87d5ca748"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;3;33m> Current query: Write about Llama 3.1 Model, BERT and PEFT\n",
            "\u001b[0m\u001b[1;3;38;5;200m> New query: What are Llama 3.1, BERT, and PEFT, and how do they relate to LLMs and generative AI?\n",
            "\n",
            "\u001b[0m\u001b[1;3;33m> Current query: Write about Llama 3.1 Model, BERT and PEFT\n",
            "\u001b[0m\u001b[1;3;38;5;200m> New query: What are the key characteristics and applications of Llama 3.1 and PEFT in the context of LLMs and generative AI?\n",
            "\n",
            "\u001b[0m\u001b[1;3;33m> Current query: Write about Llama 3.1 Model, BERT and PEFT\n",
            "\u001b[0m\u001b[1;3;38;5;200m> New query: What are the key characteristics and applications of BERT in the context of LLMs and generative AI?\n",
            "\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "response_gemini = query_engine_gemini.query(\"Write about Llama 3.1 Model, BERT and PEFT\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "FlgMkAhQsTIY",
        "outputId": "74513198-dc0e-490a-9631-1d07d0777a34"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Llama 3.1 is an open-source large language model known for its impressive performance, often matching or surpassing that of GPT-4 in various benchmarks while maintaining a lower inference cost. Its architecture allows for scalability, with versions ranging from smaller models that require fewer resources to a substantial 405 billion parameter version that demands significant GPU capabilities. The model can be deployed on-premises or through cloud services, making it versatile for different applications.\\n\\nPEFT, or Parameter-Efficient Fine-Tuning, is a library designed to enhance the fine-tuning process of large models like Llama 3.1. It employs techniques such as Low-Rank Adaptation (LoRA) to focus on a subset of parameters, allowing for efficient adjustments without the need for extensive computational resources. This method is particularly useful for customizing models for specific tasks, improving their performance while minimizing costs.\\n\\nBERT, on the other hand, is a discriminative language model that specializes in tasks such as text classification and sentiment analysis. It operates by estimating conditional probabilities, making it effective for understanding and interpreting text. However, it is not designed as a generative model, which distinguishes it from models like Llama 3.1 that can generate text.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "response_gemini.response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxOF2qth1gUC"
      },
      "source": [
        "## Test Retriever on Multistep\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "In9BZbU10KAz"
      },
      "outputs": [],
      "source": [
        "# import llama_index\n",
        "# from llama_index.core.indices.query.schema import QueryBundle\n",
        "\n",
        "# t = QueryBundle(\"How Retrieval Augmented Generation (RAG) work?\")\n",
        "# query_engine_gemini.retrieve(t)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2xI2YPowYpd"
      },
      "source": [
        "## Subquestion Query Engine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a4JSpCzFwWG2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66010ea8-4ea1-4758-ac68-3afdd29d33dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated 5 sub questions.\n",
            "\u001b[1;3;38;2;237;90;200m[LlamaIndex] Q: What are the key features and improvements of the Llama 3.1 model compared to its predecessors?\n",
            "\u001b[0m\u001b[1;3;38;2;90;149;237m[LlamaIndex] Q: How does BERT work and what are its main applications in natural language processing?\n",
            "\u001b[0m\u001b[1;3;38;2;11;159;203m[LlamaIndex] Q: What is PEFT (Parameter-Efficient Fine-Tuning) and how does it enhance the performance of models like BERT?\n",
            "\u001b[0m\u001b[1;3;38;2;155;135;227m[LlamaIndex] Q: What are the differences in architecture between Llama 3.1 and BERT?\n",
            "\u001b[0m\u001b[1;3;38;2;237;90;200m[LlamaIndex] Q: In what scenarios is PEFT particularly beneficial for fine-tuning language models?\n",
            "\u001b[0m\u001b[1;3;38;2;155;135;227m[LlamaIndex] A: The information provided does not include specific details about the architectural differences between Llama 3.1 and BERT. However, it highlights Llama 3.1's advancements, such as its large scale, extensive training on diverse data, and capabilities in reasoning and multilingual processing. For a comprehensive comparison, one would typically consider factors like model size, training methodology, context length, and specific architectural features, which are not detailed in the available information.\n",
            "\u001b[0m\u001b[1;3;38;2;237;90;200m[LlamaIndex] A: PEFT is particularly beneficial for fine-tuning language models in scenarios where computational and storage costs are a concern. It allows for the adaptation of large pretrained models by fine-tuning only a small number of additional parameters, which significantly reduces the resources required compared to traditional fine-tuning methods. This makes it more accessible for training and storing large language models on consumer hardware. Additionally, PEFT is advantageous when aiming for performance that is comparable to fully fine-tuned models without the associated high costs, making it suitable for various downstream applications.\n",
            "\u001b[0m\u001b[1;3;38;2;90;149;237m[LlamaIndex] A: BERT, which stands for Bidirectional Encoder Representations from Transformers, operates using a transformer architecture that processes text bidirectionally. This means it can consider the entire context of a sentence, allowing it to capture nuances and meanings more effectively than traditional unidirectional models that read text sequentially. \n",
            "\n",
            "The model is pre-trained on two main tasks: Masked Language Modeling (MLM) and Next Sentence Prediction (NSP). In MLM, BERT predicts masked tokens in a sentence, learning to generate contextualized word representations based on surrounding words. NSP involves predicting whether one sentence is likely to follow another, enhancing the model's understanding of sentence relationships.\n",
            "\n",
            "BERT's architecture and training enable it to excel in various natural language processing applications, including sentiment analysis, question answering, and language translation, making it a foundational model in the development of advanced language understanding systems.\n",
            "\u001b[0m\u001b[1;3;38;2;11;159;203m[LlamaIndex] A: PEFT (Parameter-Efficient Fine-Tuning) is a library designed to adapt large pretrained models to various downstream applications efficiently. It achieves this by fine-tuning only a small number of additional model parameters, which significantly reduces both computational and storage costs. This approach allows for performance that is comparable to fully fine-tuned models while making it more feasible to train and store large language models on consumer hardware.\n",
            "\n",
            "By integrating with popular libraries such as Transformers, Diffusers, and Accelerate, PEFT enhances the training and inference processes of large models, streamlining the workflow for users. This efficiency is particularly beneficial for models like BERT, as it allows for effective adaptation to specific tasks without the need for extensive resource investment.\n",
            "\u001b[0m\u001b[1;3;38;2;237;90;200m[LlamaIndex] A: The Llama 3.1 model introduces several key features and improvements over its predecessors. Notably, it is the largest model developed by Meta, trained on over 15 trillion tokens using more than 16,000 H100 GPUs. This significant training scale enhances its performance across various tasks.\n",
            "\n",
            "Key features include:\n",
            "\n",
            "1. **Extended Context Length**: Llama 3.1 supports a context length of 128K, allowing for better handling of long text inputs.\n",
            "\n",
            "2. **Enhanced Reasoning and Coding Capabilities**: The model demonstrates improved abilities in logical reasoning, problem-solving, and generating high-quality code, surpassing earlier versions.\n",
            "\n",
            "3. **Multilingual Processing**: With approximately 50% of its training data consisting of multilingual tokens, Llama 3.1 effectively understands and processes multiple languages.\n",
            "\n",
            "4. **Tool Use and Agentic Behaviors**: The model supports zero-shot tool use and can develop agentic behaviors, enhancing its versatility in various applications.\n",
            "\n",
            "5. **Benchmark Performance**: Llama 3.1 outperforms previous models in areas such as mathematical reasoning, complex reasoning, and long text processing, achieving high scores in benchmark tests.\n",
            "\n",
            "Overall, these advancements position Llama 3.1 as a leading open-source AI model, offering significant improvements in performance and capabilities compared to Llama 1 and Llama 2.\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
        "from llama_index.core.query_engine import SubQuestionQueryEngine\n",
        "\n",
        "query_engine = vector_index.as_query_engine()\n",
        "\n",
        "query_engine_tools = [\n",
        "    QueryEngineTool(\n",
        "        query_engine=query_engine,\n",
        "        metadata=ToolMetadata(\n",
        "            name=\"LlamaIndex\",\n",
        "            description=\"Used to answer the Questions about RAG, Machine Learning, Deep Learning, and Generative AI. Note: Don't repeat the Same question\",\n",
        "        ),\n",
        "    ),\n",
        "]\n",
        "\n",
        "sub_question_engine = SubQuestionQueryEngine.from_defaults(\n",
        "    query_engine_tools=query_engine_tools,\n",
        "    use_async=True,\n",
        ")\n",
        "\n",
        "response = sub_question_engine.query(\"Write about Llama 3.1 Model, BERT and PEFT\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IKxgCfJawjcj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "2ce04682-beac-4572-dd74-7a14aa5aab5a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Llama 3.1 is a state-of-the-art model developed by Meta, notable for its extensive training on over 15 trillion tokens using more than 16,000 H100 GPUs. This model features an extended context length of 128K, enhancing its ability to process long text inputs. It demonstrates improved reasoning and coding capabilities, excels in multilingual processing, and supports zero-shot tool use, making it versatile for various applications. Llama 3.1 outperforms its predecessors in benchmark tests, particularly in mathematical and complex reasoning tasks.\\n\\nBERT, or Bidirectional Encoder Representations from Transformers, utilizes a transformer architecture that processes text bidirectionally, allowing it to capture the full context of sentences. It is pre-trained on tasks like Masked Language Modeling and Next Sentence Prediction, which help it generate contextualized word representations and understand sentence relationships. BERT is widely used in natural language processing applications such as sentiment analysis, question answering, and language translation.\\n\\nParameter-Efficient Fine-Tuning (PEFT) is a library that enhances the performance of large pretrained models like BERT by allowing for efficient adaptation to specific tasks. It fine-tunes only a small number of additional parameters, significantly reducing computational and storage costs while maintaining performance comparable to fully fine-tuned models. PEFT is particularly beneficial in scenarios where resources are limited, making it easier to train and store large language models on consumer hardware.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "response.response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCdPwVAQ6ixg"
      },
      "source": [
        "# HyDE Transform\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1x6He0T961Kg"
      },
      "outputs": [],
      "source": [
        "query_engine = vector_index.as_query_engine()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0GgtfeBC6m0H"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.indices.query.query_transform import HyDEQueryTransform\n",
        "from llama_index.core.query_engine.transform_query_engine import TransformQueryEngine\n",
        "\n",
        "hyde = HyDEQueryTransform(include_original=True)\n",
        "hyde_query_engine = TransformQueryEngine(query_engine, hyde)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mm3nYnIE6mwl"
      },
      "outputs": [],
      "source": [
        "response = hyde_query_engine.query(\"Write about Llama 3.1 Model, BERT and PEFT\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PjTJ2poc6mt5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "df5926d7-9f02-41f5-a310-f1419a241c2b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Llama 3.1 405B is a significant advancement in the field of AI, developed by Meta. It stands out as the largest open-source model to date, trained on over 15 trillion tokens using more than 16,000 H100 GPUs. This extensive training has enabled it to achieve a 128K context length, which enhances its capabilities in reasoning, coding, and multilingual processing. The model excels in generating high-quality code and demonstrates strong logical reasoning and problem-solving skills. It has been shown to outperform proprietary models like GPT-4o and Claude 3.5 Sonnet in various benchmark tests, particularly in areas such as mathematical reasoning and multilingual support.\\n\\nBERT, or Bidirectional Encoder Representations from Transformers, is another influential model in the AI landscape, primarily designed for natural language understanding tasks. Unlike Llama 3.1, which focuses on a broader range of capabilities including coding and multilingual processing, BERT is specifically optimized for tasks like sentiment analysis, question answering, and language inference. Its architecture allows it to consider the context of words in both directions, which enhances its understanding of language nuances.\\n\\nPEFT, or Parameter-Efficient Fine-Tuning, is a technique that allows models like Llama 3.1 and BERT to be fine-tuned on specific tasks with fewer parameters, making the process more efficient. This approach is particularly beneficial for adapting large models to specialized applications without the need for extensive computational resources. By leveraging PEFT, users can achieve high performance on targeted tasks while maintaining the model's general capabilities.\\n\\nIn summary, Llama 3.1 represents a leap forward in open-source AI with its extensive training and versatile features, while BERT remains a cornerstone for natural language understanding. PEFT serves as a valuable method for optimizing these models for specific applications.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "response.response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "StgikqWZ6mrl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a48d1f4f-8039-44b4-98c2-ebebfc2d02a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Node ID\t 5624cdc8-2997-4e4d-82d1-c7383d389215\n",
            "Text\t 3.1 405B is Metas largest model  trained with over 15 trillion tokens. For this  Meta optimized the entire training stack and trained it on more than 16 000 H100 GPUs  making it the first Llama model trained at this scale.   According to Meta  this version of the original model (Llama 1 and Llama 2) has 128K context length  improved reasoning and coding capabilities. Meta has also upgraded both multilingual 8B and 70B models.   Key Features of Llama 3.1 40 5B:Llama 3.1 comes with a host of features and capabilities that appeal to The users  such as:   RAG & tool use  Meta states that you can use Llama system components to extend the model using zero-shot tool use and build agentic behaviors with RAG.   Multi-lingual  Llama 3 naturally supports multilingual processing. The pre-training data includes about 50% multilingual tokens and can process and understand multiple languages.   Programming and Reasoning  Llama 3 has powerful programming capabilities  generating high-quality code with a strong understanding of syntax and logic. It can create complex code structures and perform well in various programming tasks. Llama 3 excels in logical reasoning  problem-solving  analysis  and inference. It handles complex logical tasks and solves intricate problems effectively.   Multimodal Models  Multimodal models have been developed that support image recognition  video recognition  and speech understanding capabilities  but these models are still under development and have not yet been widely released.   Benchmark ResultsMeta compared the Llama 3.1 405B model with models such as GPT-4  GPT-4o  and Claude 3.5 sonnet. The results showed that Llama 3.1 performed better than GPT-4o and Claude 3.5 sonnet on test data sets such as mathematical reasoning  complex reasoning  and Multilingual support and its long text processing ability is also excellent  receiving 95.2 points in zero scrolls quality.   it falls short compared to Claude 3.5 sonnet in tool utilization ability\n",
            "Score\t 0.5479605684655463\n",
            "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Node ID\t 309bb3c8-d52c-4571-b42d-7a2cde27fac4\n",
            "Text\t the AI news in the past 7 days has been insane  with so much happening in the world of AI   in this video  were diving into some of the latest AI developments from major players like Llama 3.1 405B  GPT-4o and Claude 3.5 Sonnet   Llama 3.1 405B is the first open-source model that performs on par with leading proprietary AI models in general knowledge  steerability  math  tool use  and multilingual translation  among other capabilities.   Meta announced the launch of Llama 3.1  which is the largest open-source AI model to date  and has surpassed OpenAIs GPT-4o and Anthropics Claude 3.5 Sonnet in multiple benchmark tests!   In this step-by-step guide  we will cover what Llama 3.1 405B is  how to use Llama 3.1 405B locally  and why Llama 3.1 405B is so much better than GPT-4o and Claude 3.5 Sonnet.   I highly recommend you watch this video to the end is a game changer in your chatbot that will realize the power of Llama 3.1 405B!   If you like this topic and you want to support me:   Clap my article 50 times; that will really help me out.U+1F44FFollow me on Medium and subscribe to get my latest articleU+1FAF6Follow me on my YouTube channelMore info on my discordLlama 3.1 405B is Metas largest model  trained with over 15 trillion tokens. For this  Meta optimized the entire training stack and trained it on more than 16 000 H100 GPUs  making it the first Llama model trained at this scale.   According to Meta  this version of the original model (Llama 1 and Llama 2) has 128K context length  improved reasoning and coding capabilities. Meta has also upgraded both multilingual 8B and 70B models.   Key Features of Llama 3.1 40 5B:Llama\n",
            "Score\t 0.5404638765911781\n",
            "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n"
          ]
        }
      ],
      "source": [
        "for src in response.source_nodes:\n",
        "    print(\"Node ID\\t\", src.node_id)\n",
        "    print(\"Text\\t\", src.text)\n",
        "    print(\"Score\\t\", src.score)\n",
        "    print(\"-_\" * 20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "17Jbo1FH6mjH"
      },
      "outputs": [],
      "source": [
        "query_bundle = hyde(\"Write about Llama 3.1 Model, BERT and PEFT\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UZEK63K77W7X"
      },
      "outputs": [],
      "source": [
        "hyde_doc = query_bundle.embedding_strs[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wyzwkpSn7Yi1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "outputId": "b75df1cb-d7e3-4a38-c111-ae24c16786e8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"The Llama 3.1 model, developed by Meta, represents a significant advancement in the field of natural language processing (NLP). Building on the foundation laid by its predecessors, Llama 3.1 is designed to enhance the understanding and generation of human-like text. It incorporates a larger dataset and improved training techniques, which contribute to its ability to perform a wide range of language tasks, from text completion to question answering. The model's architecture is based on transformer technology, which allows it to capture complex patterns in language and generate coherent and contextually relevant responses.\\n\\nIn contrast, BERT (Bidirectional Encoder Representations from Transformers), developed by Google, is another influential model in the NLP landscape. BERT introduced the concept of bidirectional training, allowing the model to consider the context of a word based on all of its surrounding words, rather than just the words that precede it. This bidirectional approach significantly improves the model's understanding of context and nuance in language, making it particularly effective for tasks such as sentiment analysis, named entity recognition, and question answering. BERT's architecture is also based on transformers, but it focuses on understanding the input text rather than generating new text.\\n\\nPEFT (Parameter-Efficient Fine-Tuning) is a technique that has gained traction in the NLP community, particularly in the context of large pre-trained models like Llama 3.1 and BERT. PEFT allows for the fine-tuning of these models on specific tasks with a minimal number of parameters, making the process more efficient and less resource-intensive. This approach is particularly beneficial for organizations with limited computational resources, as it enables them to leverage the power of large models without the need for extensive retraining. By focusing on a smaller subset of parameters, PEFT can achieve competitive performance on various NLP tasks while reducing the time and cost associated with model training.\\n\\nIn summary, the Llama 3.1 model and BERT are both pivotal in advancing NLP capabilities, each with its unique strengths. Llama 3.1 excels in text generation and understanding, while BERT's bidirectional approach enhances contextual comprehension. Meanwhile, PEFT offers a practical solution for fine-tuning these models efficiently, making advanced NLP accessible to a broader range of users and applications.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "hyde_doc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rSHSMLQHmvh0"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "llamaindexkernel",
      "language": "python",
      "name": "llamaindexkernel"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "16829ce67fd84a93a7c08940221152ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_dcd8bd8429c44c60995e984b64c1ad11",
              "IPY_MODEL_3a64b950b37b48e5ac62cab0a479bf25",
              "IPY_MODEL_dc7f81bde2854c47a0554644c22e0b74"
            ],
            "layout": "IPY_MODEL_a85e4a12564044e48ff32267f145b050"
          }
        },
        "dcd8bd8429c44c60995e984b64c1ad11": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e265399904284a0a97bbb548597398e7",
            "placeholder": "​",
            "style": "IPY_MODEL_d8b45ee1cc2f41b0a46aa7d652a52731",
            "value": "vectorstore.zip: 100%"
          }
        },
        "3a64b950b37b48e5ac62cab0a479bf25": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fd5b7d8f97784d19ba137dcf5fd27319",
            "max": 97198458,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_04d4b9eadc0c4cea92643d644cdbe527",
            "value": 97198458
          }
        },
        "dc7f81bde2854c47a0554644c22e0b74": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_16dd317689d14edfa1e34ba5645a9125",
            "placeholder": "​",
            "style": "IPY_MODEL_b8418b4e6fca4be98c6289ef364692d4",
            "value": " 97.2M/97.2M [00:00&lt;00:00, 144MB/s]"
          }
        },
        "a85e4a12564044e48ff32267f145b050": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e265399904284a0a97bbb548597398e7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d8b45ee1cc2f41b0a46aa7d652a52731": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fd5b7d8f97784d19ba137dcf5fd27319": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "04d4b9eadc0c4cea92643d644cdbe527": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "16dd317689d14edfa1e34ba5645a9125": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b8418b4e6fca4be98c6289ef364692d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}