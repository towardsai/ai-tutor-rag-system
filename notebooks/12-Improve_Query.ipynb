{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zE1h0uQV7uT"
      },
      "source": [
        "# Install Packages and Setup Variables\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": true,
        "id": "QPJzr-I9XQ7l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c463e4ad-24b5-4cc1-afc3-269529fc4565"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.0/88.0 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m57.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.5/46.5 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.4/295.4 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.6/111.6 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.8/19.8 MB\u001b[0m \u001b[31m70.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m62.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.6/245.6 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m83.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m64.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.9/11.9 MB\u001b[0m \u001b[31m88.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m303.3/303.3 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.5/440.5 kB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.3/103.3 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.1/211.1 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m80.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.9/131.9 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.7/65.7 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m208.0/208.0 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.9/45.9 MB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m109.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.9/43.9 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.9/190.9 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.5/81.5 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.8/510.8 kB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m160.3/160.3 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.1/56.1 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.9/63.9 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.5/310.5 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.4/107.4 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m499.2/499.2 kB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m74.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m452.2/452.2 kB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.8/211.8 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.9/116.9 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.4/144.4 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for html2text (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for tinysegmenter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for spider-client (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for feedfinder2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for jieba3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q llama-index==0.14.0 openai==1.107.0 llama-index-finetuning==0.4.1 llama-index-embeddings-huggingface==0.6.1 \\\n",
        "                llama-index-embeddings-cohere==0.6.1 cohere==5.18.0 llama-index-readers-web==0.5.3 chromadb==1.0.21 jedi==0.19.2 \\\n",
        "                llama-index-vector-stores-chroma==0.5.3 llama-index-llms-google-genai==0.5.0 google-genai==1.38.0 llama-index-llms-openai==0.5.6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "riuXwpSPcvWC"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Set the following API Keys in the Python environment. Will be used later.\n",
        "# os.environ[\"OPENAI_API_KEY\"] = \"<YOUR-OPENAI-API-KEY>\"\n",
        "# os.environ[\"GOOGLE_API_KEY\"] = \"<YOUR-GOOGLE-API-KEY>\"\n",
        "\n",
        "from google.colab import userdata\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
        "os.environ[\"GOOGLE_API_KEY\"] = userdata.get('Google_api_key')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "jIEeZzqLbz0J"
      },
      "outputs": [],
      "source": [
        "# Allows running asyncio in environments with an existing event loop, like Jupyter notebooks.\n",
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bkgi2OrYzF7q"
      },
      "source": [
        "# Load a Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "9oGT6crooSSj"
      },
      "outputs": [],
      "source": [
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "from llama_index.llms.openai import OpenAI\n",
        "from llama_index.core import Settings\n",
        "\n",
        "Settings.llm = OpenAI(model=\"gpt-5-mini\", additional_kwargs={'reasoning_effort':'minimal'})\n",
        "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWaT6rL7ksp8"
      },
      "source": [
        "# Load Indexes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "accb4f3f09c14b878f47a3c1ed9e4377",
            "110bfb27846a4834ba4337d53d533907",
            "06fe752a8ccf4468976bf06cb20c5f0d",
            "ce0441d29cc341e18d58ad7408d4604d",
            "1b229f5c86214f5fb74f7d9089073939",
            "d8ffac78365447788cf5cbb5bf22637c",
            "6653c88dfc7c4bfca1ded0c50d5f5503",
            "debdadab590a4a9aa0de5b31fa88c5c7",
            "b9b5c5e401ed482b817a3d9c2e690d2e",
            "c66d9fec492a4a20a4ce14ceadee0c8d",
            "1201c501de2f4a6ab8050c93c70e95ab"
          ]
        },
        "id": "5sP-_zZso337",
        "outputId": "1f2be4b7-6a0a-4255-f4ac-38073a04486b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vectorstore.zip:   0%|          | 0.00/97.2M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "accb4f3f09c14b878f47a3c1ed9e4377"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "vectorstore = hf_hub_download(repo_id=\"jaiganesan/ai_tutor_knowledge\", filename=\"vectorstore.zip\", repo_type=\"dataset\", local_dir=\".\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SodY2Xpf_kxg",
        "outputId": "281b7dc3-d28a-42e9-8a04-2d36ad053edf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  vectorstore.zip\n",
            "   creating: ai_tutor_knowledge/\n",
            "   creating: ai_tutor_knowledge/684af133-f877-4230-bde4-575cf53b6688/\n",
            "  inflating: ai_tutor_knowledge/684af133-f877-4230-bde4-575cf53b6688/length.bin  \n",
            "  inflating: ai_tutor_knowledge/684af133-f877-4230-bde4-575cf53b6688/index_metadata.pickle  \n",
            "  inflating: ai_tutor_knowledge/684af133-f877-4230-bde4-575cf53b6688/link_lists.bin  \n",
            "  inflating: ai_tutor_knowledge/684af133-f877-4230-bde4-575cf53b6688/header.bin  \n",
            "  inflating: ai_tutor_knowledge/684af133-f877-4230-bde4-575cf53b6688/data_level0.bin  \n",
            "  inflating: ai_tutor_knowledge/chroma.sqlite3  \n"
          ]
        }
      ],
      "source": [
        "!unzip -o vectorstore.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "mXi56KTXk2sp"
      },
      "outputs": [],
      "source": [
        "import chromadb\n",
        "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
        "from llama_index.core import VectorStoreIndex\n",
        "\n",
        "# Create the vector index\n",
        "db = chromadb.PersistentClient(path=\"./ai_tutor_knowledge\")\n",
        "chroma_collection = db.get_or_create_collection(\"ai_tutor_knowledge\")\n",
        "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
        "vector_index = VectorStoreIndex.from_vector_store(vector_store)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLrn8A3jckmW"
      },
      "source": [
        "# Multi-Step Query Engine\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UmpfpVCje8h3"
      },
      "source": [
        "## GPT-5-mini\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "8y-Ya3GyfcAk"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.indices.query.query_transform.base import (\n",
        "    StepDecomposeQueryTransform,\n",
        ")\n",
        "\n",
        "step_decompose_transform_gpt5 = StepDecomposeQueryTransform(verbose=True, llm=Settings.llm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "zntXdSbGf_qF"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.query_engine.multistep_query_engine import MultiStepQueryEngine\n",
        "\n",
        "#Default query engine\n",
        "query_engine_gpt5_mini = vector_index.as_query_engine()\n",
        "\n",
        "# Multi Step Query Engine\n",
        "multi_step_query_engine = MultiStepQueryEngine(\n",
        "    query_engine = query_engine_gpt5_mini,\n",
        "    query_transform = step_decompose_transform_gpt5,\n",
        "    index_summary = \"Used to answer the Questions about RAG, Machine Learning, Deep Learning, and Generative AI, Note: Don't repeat the Same quesion\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JPD8yAinVSq"
      },
      "source": [
        "# Query Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2IByQ5-ox9U"
      },
      "source": [
        "## Default"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0gue7cyctt1",
        "outputId": "4ee65a7c-383e-4e6c-8b3f-f742dbe092b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I can only answer using the provided excerpts. The available information covers LLaMA (not Llama 3.1 specifically), PEFT methods (including LoRA and Llama-Adapter), and mentions tokenizer/model classes. Based on that information:\n",
            "\n",
            "- LLaMA model family:\n",
            "  - Configuration and classes: A configuration class is provided (LlamaConfig). Tokenization is supported via LlamaTokenizer and LlamaTokenizerFast, which include utilities like build_inputs_with_special_tokens, get_special_tokens_mask, create_token_type_ids_from_sequences, and save_vocabulary; the fast tokenizer also supports update_post_processor. Model classes include LlamaModel (forward), and task-specific variants such as LlamaForCausalLM, LlamaForSequenceClassification, LlamaForQuestionAnswering, and LlamaForTokenClassification (each exposing forward). Flax implementations include FlaxLlamaModel and FlaxLlamaForCausalLM (with __call__).\n",
            "\n",
            "- PEFT methods for LLaMA:\n",
            "  - LoRA (Low-Rank Adaptation): The documentation references fine-tuning LLaMA using the LoRA method via the 🤗 PEFT library and points to notebooks demonstrating this approach, including one with an intuitive UI. The PEFT tooling integrates with Hugging Face model classes (e.g., using PeftModel wrappers) to enable parameter-efficient fine-tuning.\n",
            "  - Llama-Adapter: A PEFT approach that keeps the base LLaMA model frozen and learns a small set of adaptation prompts which are prefixed to input tokens. It uses zero-initialized attention with zero gating so the learned prompts are progressively injected without overwriting pre-trained knowledge. For a frozen 7B LLaMA model, Llama-Adapter adds about 1.2M learnable parameters and can fine-tune very quickly (reported <1 hour on 8 A100 GPUs for 52K self-instruct demonstrations), achieving performance comparable to fully fine-tuned baselines. The method can be extended to multimodal inputs and the code is available on GitHub. The PEFT package exposes specific components for Llama-Adapter such as AdaptionPromptConfig and AdaptionPromptModel.\n",
            "\n",
            "- BERT:\n",
            "  - The provided excerpts do not contain information about BERT, its classes, or methods. No BERT-specific details are available in the given context.\n",
            "\n",
            "- Deployment:\n",
            "  - For deploying LLaMA-family models, a practical notebook is provided demonstrating how to deploy an Open-LLaMA model for text generation on Amazon SageMaker (a link to an example notebook is included in the materials).\n",
            "\n",
            "If you want details about Llama 3.1 or BERT-specific PEFT workflows, or concrete code examples for applying LoRA or Llama-Adapter to a specific model, provide sources covering those or I can fetch and summarize targeted documentation or notebooks if you allow linking to external resources.\n"
          ]
        }
      ],
      "source": [
        "# Default query engine\n",
        "query_engine = vector_index.as_query_engine()\n",
        "res = query_engine.query(\"Write about Llama 3.1 Model, BERT and PEFT methods\")\n",
        "print(res.response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "465dH4yQc7Ct",
        "outputId": "b9838dd8-6aa9-4ff3-e8de-0b225a8eceba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Node ID\t 781b7b12-eca2-47c0-a66e-9d6be670e951\n",
            "Title\t LLaMA\n",
            "Text\t on how to fine-tune LLaMA model using LoRA method via the 🤗 PEFT library with intuitive UI. 🌎 - A [notebook](https://github.com/aws/amazon-sagemaker-examples/blob/main/introduction_to_amazon_algorithms/jumpstart-foundation-models/text-generation-open-llama.ipynb) on how to deploy Open-LLaMA model for text generation on Amazon SageMaker. 🌎 ## LlamaConfig[[autodoc]] LlamaConfig## LlamaTokenizer[[autodoc]] LlamaTokenizer    - build_inputs_with_special_tokens    - get_special_tokens_mask    - create_token_type_ids_from_sequences    - save_vocabulary## LlamaTokenizerFast[[autodoc]] LlamaTokenizerFast    - build_inputs_with_special_tokens    - get_special_tokens_mask    - create_token_type_ids_from_sequences    - update_post_processor    - save_vocabulary## LlamaModel[[autodoc]] LlamaModel    - forward## LlamaForCausalLM[[autodoc]] LlamaForCausalLM    - forward## LlamaForSequenceClassification[[autodoc]] LlamaForSequenceClassification    - forward## LlamaForQuestionAnswering[[autodoc]] LlamaForQuestionAnswering    - forward## LlamaForTokenClassification[[autodoc]] LlamaForTokenClassification    - forward## FlaxLlamaModel[[autodoc]] FlaxLlamaModel    - __call__## FlaxLlamaForCausalLM[[autodoc]] FlaxLlamaForCausalLM    - __call__\n",
            "Score\t 0.43886127400343966\n",
            "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Node ID\t e1e3e842-7160-40c4-8e74-772fb8254f5e\n",
            "Title\t Llama-Adapter\n",
            "Text\t # Llama-Adapter[Llama-Adapter](https://hf.co/papers/2303.16199) is a PEFT method specifically designed for turning Llama into an instruction-following model. The Llama model is frozen and only a set of adaptation prompts prefixed to the input instruction tokens are learned. Since randomly initialized modules inserted into the model can cause the model to lose some of its existing knowledge, Llama-Adapter uses zero-initialized attention with zero gating to progressively add the instructional prompts to the model.The abstract from the paper is:*We present LLaMA-Adapter, a lightweight adaption method to efficiently fine-tune LLaMA into an instruction-following model. Using 52K self-instruct demonstrations, LLaMA-Adapter only introduces 1.2M learnable parameters upon the frozen LLaMA 7B model, and costs less than one hour for fine-tuning on 8 A100 GPUs. Specifically, we adopt a set of learnable adaption prompts, and prepend them to the input text tokens at higher transformer layers. Then, a zero-init attention mechanism with zero gating is proposed, which adaptively injects the new instructional cues into LLaMA, while effectively preserves its pre-trained knowledge. With efficient training, LLaMA-Adapter generates high-quality responses, comparable to Alpaca with fully fine-tuned 7B parameters. Furthermore, our approach can be simply extended to multi-modal input, e.g., images, for image-conditioned LLaMA, which achieves superior reasoning capacity on ScienceQA. We release our code at https://github.com/ZrrSkywalker/LLaMA-Adapter*.## AdaptionPromptConfig[[autodoc]] tuners.adaption_prompt.config.AdaptionPromptConfig## AdaptionPromptModel[[autodoc]] tuners.adaption_prompt.model.AdaptionPromptModel\n",
            "Score\t 0.4377525242037941\n",
            "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n"
          ]
        }
      ],
      "source": [
        "for src in res.source_nodes:\n",
        "    print(\"Node ID\\t\", src.node_id)\n",
        "    print(\"Title\\t\", src.metadata[\"title\"])\n",
        "    print(\"Text\\t\", src.text)\n",
        "    print(\"Score\\t\", src.score)\n",
        "    print(\"-_\" * 20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2y2AiInmpz7g"
      },
      "source": [
        "## GPT-5-mini Multi-Step\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69kADAFilW1n",
        "outputId": "6fad1c96-f561-4905-8dda-ac0dc38a8390"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;3;33m> Current query: Write about Llama 3.1 Model, BERT and PEFT methods\n",
            "\u001b[0m\u001b[1;3;38;5;200m> New query: What are the key features, differences, and typical use cases of Llama 3.1, BERT, and PEFT methods?\n",
            "\u001b[0m\u001b[1;3;33m> Current query: Write about Llama 3.1 Model, BERT and PEFT methods\n",
            "\u001b[0m\u001b[1;3;38;5;200m> New query: Which specific classes, methods, and example notebooks are provided for fine-tuning LLaMA 3.1 with PEFT (especially LoRA), and how do they demonstrate training and deployment (e.g., SageMaker, DeepSpeed, FSDP) for tasks like causal language modeling, sequence classification, and question answering?\n",
            "\u001b[0m\u001b[1;3;33m> Current query: Write about Llama 3.1 Model, BERT and PEFT methods\n",
            "\u001b[0m\u001b[1;3;38;5;200m> New query: Which specific PEFT/LoRA classes, methods, and example notebooks are available for fine-tuning and deploying Llama 3.1 (LLaMA) for tasks like causal language modeling, sequence classification, and question answering, and how do those examples demonstrate training and deployment (e.g., SageMaker, DeepSpeed, FSDP)?\n",
            "\u001b[0mLlama 3.1 Model, BERT, and PEFT methods — concise comparison and descriptions\n",
            "\n",
            "Llama 3.1 (LLaMA family)\n",
            "- What it is:\n",
            "  - A family of model implementations with accompanying configuration and tokenizer utilities and a range of task-specific model classes.\n",
            "- Key classes and APIs:\n",
            "  - Configuration: LlamaConfig.\n",
            "  - Tokenizers: LlamaTokenizer and LlamaTokenizerFast with methods such as build_inputs_with_special_tokens, get_special_tokens_mask, create_token_type_ids_from_sequences, save_vocabulary (and update_post_processor on the fast tokenizer).\n",
            "  - Model classes: LlamaModel (base), LlamaForCausalLM (causal language modeling / text generation), LlamaForSequenceClassification, LlamaForQuestionAnswering, LlamaForTokenClassification. Flax variants are available (FlaxLlamaModel, FlaxLlamaForCausalLM).\n",
            "  - Models expose standard forward/call entry points so they can be wrapped or adapted for training and inference.\n",
            "- Typical use cases:\n",
            "  - Causal language modeling and text generation (LlamaForCausalLM).\n",
            "  - Downstream supervised tasks such as sequence classification, question answering, and token classification via the dedicated model classes.\n",
            "  - Deployment workflows for generation (example notebooks show deploying Open‑LLaMA to cloud platforms for text generation).\n",
            "\n",
            "BERT\n",
            "- What it is (as reflected in the provided materials):\n",
            "  - The provided excerpts do not include detailed BERT classes, methods, or examples. BERT is not described in the listed materials.\n",
            "- Typical use cases (inferred role from context):\n",
            "  - Although not detailed in the excerpts, encoder‑style models like BERT typically serve similar downstream roles (classification, QA, token classification). The context highlights that LLaMA has dedicated classes for those tasks, indicating comparable downstream applicability for encoder models.\n",
            "\n",
            "PEFT methods (Parameter‑Efficient Fine‑Tuning)\n",
            "- What it is:\n",
            "  - A set of techniques and a library for adapting large pretrained models by training a small number of additional parameters rather than full model fine‑tuning.\n",
            "- Key techniques and resources:\n",
            "  - LoRA (Low‑Rank Adaptation) is a primary PEFT technique highlighted; soft prompting and other parameter‑efficient strategies are also covered in conceptual materials.\n",
            "  - Practical how‑to guides and reference documentation cover image classification, causal language modeling, and automatic speech recognition, and detail PEFT classes and usage patterns.\n",
            "  - PeftModel is used in examples to load and run models with PEFT adapters.\n",
            "- Typical use cases:\n",
            "  - Efficiently fine‑tuning large models when compute, memory, or storage is limited.\n",
            "  - Adapting base models (including LLaMA) for classification, QA, and generative tasks by attaching small adapters rather than updating all weights.\n",
            "- Example notebooks and demonstrations (workflows and deployments):\n",
            "  - LoRA fine‑tuning notebooks showing how to attach and train LoRA adapters on LLaMA (including an intuitive UI fine‑tuner notebook).\n",
            "  - Inference/evaluation notebooks demonstrating loading and running LLaMA with PeftModel adapters.\n",
            "  - Integration examples showing how to load a PEFT adapter for LLaMA within LangChain pipelines.\n",
            "  - Memory‑constrained fine‑tuning notebook (xturing) demonstrating techniques to fine‑tune on GPUs with limited memory.\n",
            "  - A StackLLaMA blog post demonstrating training LLaMA with RLHF (relevant training pipeline example for QA).\n",
            "  - A notebook demonstrating deployment of Open‑LLaMA for text generation on Amazon SageMaker (end‑to‑end deployment example).\n",
            "- How the examples demonstrate training and deployment:\n",
            "  - Fine‑tuning workflows illustrate adding low‑rank adapters (LoRA) so only adapter parameters are trained, handling checkpoints, and running evaluation/inference with PeftModel.\n",
            "  - Memory‑efficient guides show strategies to enable training on constrained hardware, complementing PEFT’s parameter savings.\n",
            "  - Inference and integration notebooks show loading PEFT adapters and using them in application pipelines (e.g., LangChain).\n",
            "  - Deployment examples present a reproducible path to serve generative LLaMA models in cloud environments (SageMaker example).\n",
            "- Notes on distributed training backends:\n",
            "  - The excerpts emphasize PEFT/LoRA and memory‑efficient notebooks; specific step‑by‑step examples for distributed backends such as DeepSpeed or Fully Sharded Data Parallel are not enumerated in the provided materials.\n",
            "\n",
            "Relationship and typical workflow\n",
            "- Base model versus adaptation:\n",
            "  - LLaMA provides the base models, tokenizer utilities, and task‑specific model classes that perform the core modeling functions (generation, classification, QA).\n",
            "  - PEFT provides methods (e.g., LoRA) and tooling to adapt those base models efficiently by training small adapter modules rather than full model weights.\n",
            "- Common workflow:\n",
            "  - Choose the appropriate LLaMA model class for the task (e.g., LlamaForCausalLM for generation, LlamaForSequenceClassification for classification, LlamaForQuestionAnswering for QA).\n",
            "  - Attach PEFT adapters (LoRA) and fine‑tune only adapter parameters using the provided notebooks and utilities.\n",
            "  - Load the adapted model with PeftModel for inference, integrate into pipelines (e.g., LangChain), and deploy to serving platforms (e.g., Amazon SageMaker) following the example notebooks.\n",
            "- When to use each approach:\n",
            "  - Use base LLaMA classes directly when you need full-model training or immediate inference with pretrained weights.\n",
            "  - Use PEFT/LoRA when compute, memory, or storage constraints make full fine‑tuning impractical, or when rapid experimentation with many task‑specific adapters is desired.\n",
            "\n",
            "If you’d like, I can summarize the specific LoRA fine‑tuning notebook steps into a short recipe (prepare data, configure LoRA adapter, run training with minimal trainable params, evaluate with PeftModel, and deploy), or list the example notebooks and what each one contains.\n"
          ]
        }
      ],
      "source": [
        "response = multi_step_query_engine.query(\"Write about Llama 3.1 Model, BERT and PEFT methods\")\n",
        "print(response.response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "wGmHP72Iu10s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4476e21-b8ce-422f-928a-7f004a83e46e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**What are the key features, differences, and typical use cases of Llama 3.1, BERT, and PEFT methods?**\n",
            "Key features, differences, and typical use cases based only on the provided information:\n",
            "\n",
            "Llama 3.1\n",
            "- Key features:\n",
            "  - Provided as a family of LLaMA model classes and utilities (configuration and tokenizers).\n",
            "  - Relevant classes include LlamaConfig, LlamaTokenizer / LlamaTokenizerFast (with methods for special tokens, token type ids, post-processing, and saving vocabulary), and model classes for various tasks (LlamaModel, LlamaForCausalLM, LlamaForSequenceClassification, LlamaForQuestionAnswering, LlamaForTokenClassification).\n",
            "  - Flax implementations are available (FlaxLlamaModel, FlaxLlamaForCausalLM).\n",
            "  - Typical forward/serve APIs are exposed (forward or __call__).\n",
            "- Typical use cases:\n",
            "  - Causal language modeling / text generation (LlamaForCausalLM).\n",
            "  - Sequence classification, question answering, and token classification via dedicated model classes.\n",
            "  - Deployments for text generation (example: a notebook showing deployment of Open‑LLaMA on Amazon SageMaker).\n",
            "\n",
            "BERT\n",
            "- Key features:\n",
            "  - Not described in detail in the provided excerpts (no explicit classes, methods, or properties for BERT are given in the context).\n",
            "- Typical use cases:\n",
            "  - The context does not include explicit BERT use cases; however, comparable model roles (sequence classification, QA, token classification) are highlighted for LLaMA via dedicated classes, implying similar downstream tasks are relevant for encoder-style models like BERT. The excerpt does not provide further specifics.\n",
            "\n",
            "PEFT methods\n",
            "- Key features:\n",
            "  - PEFT = Parameter-Efficient Fine-Tuning, a library and set of methods to adapt large pretrained models by tuning a small number of extra parameters.\n",
            "  - Includes practical how‑to guides (image classification, causal language modeling, automatic speech recognition) and conceptual guides (theory of LoRA and soft prompting).\n",
            "  - Reference documentation describes PEFT classes and methods.\n",
            "  - LoRA (Low-Rank Adaptation) is a highlighted PEFT technique; notebooks demonstrate fine-tuning LLaMA with LoRA and using PeftModel wrappers, including an intuitive UI for LoRA fine-tuning.\n",
            "  - Integration with training/inference tooling (examples mention DeepSpeed and Fully Sharded Data Parallel scripts).\n",
            "- Typical use cases:\n",
            "  - Efficiently fine-tuning large models when compute or storage is limited.\n",
            "  - Tasks shown in guides: image classification, causal language modeling, automatic speech recognition, and adapting LLaMA for classification/QA via LoRA adapters.\n",
            "  - Practical workflows: applying LoRA adapters to LLaMA and loading PEFT adapters with libraries like LangChain; deploying PEFT-adapted models (e.g., notebooks for SageMaker deployment).\n",
            "\n",
            "Differences and how they relate\n",
            "- Model vs. fine-tuning framework:\n",
            "  - LLaMA (Llama 3.1 family in context) refers to model architectures and implementations (tokenizers, configs, task-specific model classes) used for a range of NLP tasks including text generation.\n",
            "  - PEFT refers to methods and tooling for parameter-efficient adaptation of large models (including LLaMA) using techniques like LoRA and soft prompting.\n",
            "- Scope:\n",
            "  - LLaMA provides the base model implementations and task-specific variants.\n",
            "  - PEFT provides efficient fine-tuning adapters/methods and integration guides to adapt such base models with fewer trainable parameters.\n",
            "- Typical workflow complementarity:\n",
            "  - Use LLaMA model classes to perform or serve a task (e.g., causal LM for generation).\n",
            "  - Use PEFT (e.g., LoRA) to fine-tune a LLaMA instance efficiently for downstream tasks, then deploy (notebooks demonstrate deployment, including on Amazon SageMaker).\n",
            "\n",
            "If you want, I can summarize the specific PEFT/LoRA classes or point to the example notebooks (e.g., the SageMaker Open‑LLaMA deployment notebook) mentioned in the materials.\n",
            "\n",
            "**Which specific classes, methods, and example notebooks are provided for fine-tuning LLaMA 3.1 with PEFT (especially LoRA), and how do they demonstrate training and deployment (e.g., SageMaker, DeepSpeed, FSDP) for tasks like causal language modeling, sequence classification, and question answering?**\n",
            "Classes and methods available for working with and fine-tuning LLaMA (including via PEFT/LoRA) and the example notebooks referenced:\n",
            "\n",
            "1) Model & config classes (for the Hugging Face LLaMA implementations)\n",
            "- LlamaConfig — configuration class for LLaMA.\n",
            "- LlamaModel — base model class (forward).\n",
            "- LlamaForCausalLM — causal language modeling class (forward).\n",
            "- LlamaForSequenceClassification — sequence classification class (forward).\n",
            "- LlamaForQuestionAnswering — question answering class (forward).\n",
            "- LlamaForTokenClassification — token classification class (forward).\n",
            "- FlaxLlamaModel — Flax implementation (__call__).\n",
            "- FlaxLlamaForCausalLM — Flax causal LM implementation (__call__).\n",
            "\n",
            "2) Tokenizer classes and key methods\n",
            "- LlamaTokenizer and LlamaTokenizerFast\n",
            "  - build_inputs_with_special_tokens\n",
            "  - get_special_tokens_mask\n",
            "  - create_token_type_ids_from_sequences\n",
            "  - save_vocabulary\n",
            "  - (LlamaTokenizerFast additionally: update_post_processor)\n",
            "\n",
            "3) PEFT / LoRA-specific resources and example notebooks\n",
            "- A notebook demonstrating fine-tuning LLaMA using the LoRA method via the 🤗 PEFT library with an intuitive UI (Simple_LLaMA_FineTuner.ipynb on GitHub). This shows how to apply LoRA-style adapters for interactive fine-tuning.\n",
            "- A notebook showing how to run a LLaMA model using PeftModel from the 🤗 PEFT library (evaluation/inference notebook). This demonstrates loading and running PEFT adapters.\n",
            "- A notebook demonstrating loading a PEFT adapter LLaMA model with LangChain (integration / inference example).\n",
            "- A notebook on fine-tuning LLaMA using the xturing library for memory-constrained GPUs (optimization-focused fine-tuning).\n",
            "- The StackLLaMA blog post demonstrating training LLaMA with RLHF (practical training guidance for question-answering).\n",
            "- A notebook for deploying Open-LLaMA for text generation on Amazon SageMaker (text-generation SageMaker deployment example).\n",
            "\n",
            "4) Demonstrated training and deployment scenarios shown by the examples\n",
            "- Causal language modeling: LlamaForCausalLM and FlaxLlamaForCausalLM classes are available for causal LM tasks; the SageMaker Open-LLaMA text-generation notebook demonstrates deploying models for text generation (causal LM inference) on SageMaker.\n",
            "- Sequence classification: LlamaForSequenceClassification is provided for sequence classification fine-tuning (prompt tuning / fine-tuning notebooks referenced for adapting LLaMA to classification).\n",
            "- Question answering: LlamaForQuestionAnswering is provided; the StackLLaMA RLHF guide shows training LLaMA to answer questions (QA training).\n",
            "- PEFT/LoRA fine-tuning workflow: the LoRA PEFT fine-tuning notebook demonstrates adding and training LoRA adapters and using PeftModel for inference.\n",
            "- Inference & integration: PeftModel notebook and LangChain notebook show loading and running PEFT adapters in inference pipelines and chains.\n",
            "- Deployment on SageMaker: the Open-LLaMA SageMaker notebook provides a practical example of deploying a generative LLaMA model for text generation on Amazon SageMaker.\n",
            "- Optimization for limited hardware: the xturing notebook shows techniques for fine-tuning on GPUs with limited memory.\n",
            "\n",
            "Notes on distributed training backends (DeepSpeed, FSDP):\n",
            "- The provided excerpts reference optimization and memory-constrained fine-tuning notebooks (xturing) and broad resources for fine-tuning and deployment, and they include examples for deployment on SageMaker. Specific how-to notebooks for DeepSpeed or FSDP were not listed in the excerpt; the referenced materials focus on PEFT/LoRA fine-tuning, xturing for memory-limited GPUs, RLHF training, PEFT inference, LangChain integration, and SageMaker deployment.\n",
            "\n",
            "If you want, I can:\n",
            "- Provide direct links to each referenced notebook and the StackLLaMA blog post.\n",
            "- Summarize a step-by-step LoRA + PEFT fine-tuning recipe using the classes and notebooks above.\n",
            "\n",
            "**Which specific PEFT/LoRA classes, methods, and example notebooks are available for fine-tuning and deploying Llama 3.1 (LLaMA) for tasks like causal language modeling, sequence classification, and question answering, and how do those examples demonstrate training and deployment (e.g., SageMaker, DeepSpeed, FSDP)?**\n",
            "Available PEFT/LoRA-related resources, model classes, tokenizer classes, and example notebooks for fine‑tuning and deploying the LLaMA family (LLaMA / Open‑LLaMA) as described in the excerpt:\n",
            "\n",
            "1) Model and tokenizer classes (usable when adapting / fine‑tuning with PEFT / LoRA)\n",
            "- Configuration\n",
            "  - LlamaConfig\n",
            "- Tokenizers\n",
            "  - LlamaTokenizer\n",
            "    - build_inputs_with_special_tokens\n",
            "    - get_special_tokens_mask\n",
            "    - create_token_type_ids_from_sequences\n",
            "    - save_vocabulary\n",
            "  - LlamaTokenizerFast\n",
            "    - build_inputs_with_special_tokens\n",
            "    - get_special_tokens_mask\n",
            "    - create_token_type_ids_from_sequences\n",
            "    - update_post_processor\n",
            "    - save_vocabulary\n",
            "- Model classes (forward / call entry points are exposed so they can be wrapped by PEFT adapters)\n",
            "  - LlamaModel — forward\n",
            "  - LlamaForCausalLM — forward (for causal language modeling)\n",
            "  - LlamaForSequenceClassification — forward (for sequence classification)\n",
            "  - LlamaForQuestionAnswering — forward (for QA)\n",
            "  - LlamaForTokenClassification — forward\n",
            "- Flax variants\n",
            "  - FlaxLlamaModel — __call__\n",
            "  - FlaxLlamaForCausalLM — __call__\n",
            "\n",
            "2) PEFT / LoRA example notebooks and what they demonstrate\n",
            "- LoRA fine‑tuning with an intuitive UI\n",
            "  - Notebook: Simple LLaMA FineTuner (link provided in excerpt)\n",
            "  - Demonstration: fine‑tuning LLaMA using the LoRA method via the 🤗 PEFT library with an intuitive UI (shows using LoRA adapters to adapt the base model efficiently).\n",
            "- Running a PEFT adapter at inference\n",
            "  - Notebook: Evaluate / run LLaMA Model using PeftModel (link provided)\n",
            "  - Demonstration: how to load/run a LLaMA model with a PEFT adapter (PeftModel) for inference.\n",
            "- Loading a PEFT adapter with LangChain\n",
            "  - Notebook: Load PEFT adapter LLaMA model with LangChain (link provided)\n",
            "  - Demonstration: integration of a PEFT‑adapter LLaMA model into a LangChain workflow for downstream tasks.\n",
            "- Fine‑tuning on memory‑constrained GPUs\n",
            "  - Notebook: fine‑tune LLaMA using xturing on limited‑memory GPU (link provided)\n",
            "  - Demonstration: techniques to fine‑tune with constrained GPU memory (supports efficient fine‑tuning workflows that pair well with PEFT methods).\n",
            "- RLHF training example (relevant to training workflows)\n",
            "  - Blog: StackLLaMA (train LLaMA with RLHF)\n",
            "  - Demonstration: a hands‑on guide to train LLaMA for QA with RLHF (shows broader training pipeline practices).\n",
            "- Deployment on Amazon SageMaker (text generation)\n",
            "  - Notebook: text‑generation‑open‑llama.ipynb (Amazon SageMaker example, link provided)\n",
            "  - Demonstration: how to deploy Open‑LLaMA for text generation on Amazon SageMaker (practical end‑to‑end deployment notebook).\n",
            "\n",
            "3) How the examples demonstrate training and deployment workflows\n",
            "- LoRA / PEFT fine‑tuning notebooks:\n",
            "  - Show attaching low‑rank adapters to the base LLaMA model so only adapter parameters need training (LoRA via the 🤗 PEFT library).\n",
            "  - Provide UI or Colab notebook steps to launch training, manage checkpoints, and run evaluation/inference with the PEFT adapter (PeftModel) applied.\n",
            "- Memory‑efficient training notebooks:\n",
            "  - Show strategies and tooling (xturing notebook) to fine‑tune on GPUs with limited memory — complements PEFT/LoRA to reduce resource needs.\n",
            "- Inference and integration:\n",
            "  - PeftModel notebook shows how to load a model with a PEFT adapter and run inference, and the LangChain notebook shows how to integrate that into application pipelines.\n",
            "- SageMaker deployment notebook:\n",
            "  - Provides a concrete, reproducible example of packaging and deploying Open‑LLaMA for text generation on Amazon SageMaker (serving the model for inference in the cloud).\n",
            "\n",
            "Notes:\n",
            "- The excerpt lists the available model/tokenizer classes and the notebooks referenced above, but it does not enumerate explicit PEFT class names (e.g., PeftModel is referenced in the inference notebook title) beyond showing where PEFT/LoRA is used in those notebooks.\n",
            "- Specific distributed training frameworks (DeepSpeed, FSDP) are not detailed in the excerpt; the provided notebooks instead emphasize LoRA/PEFT for parameter‑efficient tuning, memory‑constrained training techniques (xturing), and SageMaker deployment.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for query, response in response.metadata['sub_qa']:\n",
        "    print(f\"**{query}**\\n{response}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "k5pJPBPRqjbG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d784e2de-1277-4b4c-8127-d06571ea3155"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Node ID\t 781b7b12-eca2-47c0-a66e-9d6be670e951\n",
            "Text\t on how to fine-tune LLaMA model using LoRA method via the 🤗 PEFT library with intuitive UI. 🌎 - A [notebook](https://github.com/aws/amazon-sagemaker-examples/blob/main/introduction_to_amazon_algorithms/jumpstart-foundation-models/text-generation-open-llama.ipynb) on how to deploy Open-LLaMA model for text generation on Amazon SageMaker. 🌎 ## LlamaConfig[[autodoc]] LlamaConfig## LlamaTokenizer[[autodoc]] LlamaTokenizer    - build_inputs_with_special_tokens    - get_special_tokens_mask    - create_token_type_ids_from_sequences    - save_vocabulary## LlamaTokenizerFast[[autodoc]] LlamaTokenizerFast    - build_inputs_with_special_tokens    - get_special_tokens_mask    - create_token_type_ids_from_sequences    - update_post_processor    - save_vocabulary## LlamaModel[[autodoc]] LlamaModel    - forward## LlamaForCausalLM[[autodoc]] LlamaForCausalLM    - forward## LlamaForSequenceClassification[[autodoc]] LlamaForSequenceClassification    - forward## LlamaForQuestionAnswering[[autodoc]] LlamaForQuestionAnswering    - forward## LlamaForTokenClassification[[autodoc]] LlamaForTokenClassification    - forward## FlaxLlamaModel[[autodoc]] FlaxLlamaModel    - __call__## FlaxLlamaForCausalLM[[autodoc]] FlaxLlamaForCausalLM    - __call__\n",
            "Score\t 0.5992694833307108\n",
            "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Node ID\t 255bc174-376a-4389-9da8-6665786a715e\n",
            "Text\t on how to use prompt tuning to adapt the LLaMA model for text classification task. 🌎<PipelineTag pipeline=\"question-answering\"/>- [StackLLaMA: A hands-on guide to train LLaMA with RLHF](https://huggingface.co/blog/stackllama#stackllama-a-hands-on-guide-to-train-llama-with-rlhf), a blog post about how to train LLaMA to answer questions on [Stack Exchange](https://stackexchange.com/) with RLHF.⚗️ Optimization- A [notebook](https://colab.research.google.com/drive/1SQUXq1AMZPSLD4mk3A3swUIc6Y2dclme?usp=sharing) on how to fine-tune LLaMA model using xturing library on GPU which has limited memory. 🌎 ⚡️ Inference- A [notebook](https://colab.research.google.com/github/DominguesM/alpaca-lora-ptbr-7b/blob/main/notebooks/02%20-%20Evaluate.ipynb) on how to run the LLaMA Model using PeftModel from the 🤗 PEFT library. 🌎 - A [notebook](https://colab.research.google.com/drive/1l2GiSSPbajVyp2Nk3CFT4t3uH6-5TiBe?usp=sharing) on how to load a PEFT adapter LLaMA model with LangChain. 🌎🚀 Deploy- A [notebook](https://colab.research.google.com/github/lxe/simple-llama-finetuner/blob/master/Simple_LLaMA_FineTuner.ipynb#scrollTo=3PM_DilAZD8T) on how to fine-tune LLaMA model using LoRA method via the 🤗 PEFT library with intuitive UI. 🌎 - A [notebook](https://github.com/aws/amazon-sagemaker-examples/blob/main/introduction_to_amazon_algorithms/jumpstart-foundation-models/text-generation-open-llama.ipynb) on how to deploy Open-LLaMA model for text generation on Amazon SageMaker. 🌎 ## LlamaConfig[[autodoc]] LlamaConfig##\n",
            "Score\t 0.5600086830315225\n",
            "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n"
          ]
        }
      ],
      "source": [
        "for src in response.source_nodes:\n",
        "    print(\"Node ID\\t\", src.node_id)\n",
        "    print(\"Text\\t\", src.text)\n",
        "    print(\"Score\\t\", src.score)\n",
        "    print(\"-_\" * 20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwcSCiMhp4Uh"
      },
      "source": [
        "# Test gemini-2.5-flash Multi-Step\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "uH9gNfZuslHK"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.indices.query.query_transform.base import StepDecomposeQueryTransform\n",
        "from llama_index.core.query_engine.multistep_query_engine import MultiStepQueryEngine\n",
        "from llama_index.llms.google_genai import GoogleGenAI\n",
        "\n",
        "llm_gemini = GoogleGenAI(model=\"gemini-2.5-flash\")\n",
        "\n",
        "step_decompose_transform = StepDecomposeQueryTransform(llm=llm_gemini, verbose=True)\n",
        "\n",
        "query_engine_gemini = vector_index.as_query_engine(llm=llm_gemini)\n",
        "\n",
        "query_engine_gemini = MultiStepQueryEngine(\n",
        "    query_engine=query_engine_gemini,\n",
        "    query_transform=step_decompose_transform,\n",
        "    index_summary=\"Used to answer the Questions about RAG, Machine Learning, Deep Learning,LLMs and Generative AI\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9s6SkHI0p6VZ",
        "outputId": "1b40c50d-209f-41ab-c259-7d355f607f09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;3;33m> Current query: Write about Llama 3.1 Model, BERT and PEFT\n",
            "\u001b[0m\u001b[1;3;38;5;200m> New query: What is Llama 3.1 Model?\n",
            "\u001b[0m\u001b[1;3;33m> Current query: Write about Llama 3.1 Model, BERT and PEFT\n",
            "\u001b[0m\u001b[1;3;38;5;200m> New query: What is BERT?\n",
            "\u001b[0m\u001b[1;3;33m> Current query: Write about Llama 3.1 Model, BERT and PEFT\n",
            "\u001b[0m\u001b[1;3;38;5;200m> New query: What is PEFT?\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "response_gemini = query_engine_gemini.query(\"Write about Llama 3.1 Model, BERT and PEFT\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 184
        },
        "id": "FlgMkAhQsTIY",
        "outputId": "1344de2a-85a8-485e-cab4-6fc48f20f272"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Llama 3.1 Model\\n- Llama 3.1 405B is Meta’s largest model, trained on over 15 trillion tokens using more than 16,000 H100 GPUs — the first Llama model trained at that scale.\\n- It supports a 128K context length, improving its ability to handle very long inputs.\\n- Capabilities: enhanced reasoning and coding compared with earlier Llama versions; strong programming ability with high-quality code generation and solid understanding of syntax and logic; excels in logical reasoning, problem solving, analysis, and inference.\\n- Multilingual: roughly 50% of its pretraining tokens are multilingual, enabling effective processing and understanding of multiple languages.\\n- RAG & tool use: supports zero-shot tool use and can be used to build agentic behaviors with retrieval-augmented generation (RAG). In benchmarks it outperformed some contemporaries (e.g., GPT-4o and Claude 3.5 Sonnet) on mathematical reasoning, complex reasoning, multilingual support, and long-text processing (scoring 95.2 points in zero-scrolls quality), though it was observed to be less effective at tool utilization than Claude 3.5 Sonnet.\\n- Multimodal extensions for image, video, and speech recognition are under development but not yet widely released.\\n\\nBERT\\n- BERT stands for Bidirectional Encoder Representations from Transformers, a language model developed by Google.\\n- Architecture: encoder-only transformer that processes text bidirectionally (simultaneously considering words from both directions) to capture context more effectively than models that read text sequentially.\\n- Pretraining tasks: Masked Language Modeling and Next Sentence Prediction.\\n\\nPEFT\\n- PEFT stands for Parameter-Efficient Fine-Tuning, a library for adapting large pretrained models to downstream tasks by fine-tuning only a small number of additional parameters.\\n- Benefits: substantially reduces computational and storage costs while achieving performance comparable to full fine-tuning, making it more feasible to train and store large models on consumer hardware.\\n- Integrations: works with libraries such as Transformers, Diffusers, and Accelerate to simplify loading, training, and inference of large models.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "response_gemini.response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxOF2qth1gUC"
      },
      "source": [
        "## Test Retriever on Multistep\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "collapsed": true,
        "id": "In9BZbU10KAz"
      },
      "outputs": [],
      "source": [
        "# import llama_index\n",
        "# from llama_index.core.indices.query.schema import QueryBundle\n",
        "\n",
        "# t = QueryBundle(\"How Retrieval Augmented Generation (RAG) work?\")\n",
        "# query_engine_gemini.retrieve(t)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2xI2YPowYpd"
      },
      "source": [
        "## Subquestion Query Engine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "a4JSpCzFwWG2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6180905-6386-4bb5-e31f-4d38397ddf15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated 8 sub questions.\n",
            "\u001b[1;3;38;2;237;90;200m[LlamaIndex] Q: What are the key features, architecture, and capabilities of Llama 3.1?\n",
            "\u001b[0m\u001b[1;3;38;2;90;149;237m[LlamaIndex] Q: What are common use cases and performance characteristics of Llama 3.1 compared to other large language models?\n",
            "\u001b[0m\u001b[1;3;38;2;11;159;203m[LlamaIndex] Q: What is BERT's architecture, training objective, and main contributions to NLP?\n",
            "\u001b[0m\u001b[1;3;38;2;155;135;227m[LlamaIndex] Q: What are typical downstream use cases and performance strengths/limitations of BERT?\n",
            "\u001b[0m\u001b[1;3;38;2;237;90;200m[LlamaIndex] Q: What is PEFT (Parameter-Efficient Fine-Tuning), its main methods (e.g., LoRA, adapters, prompt tuning), and how it works?\n",
            "\u001b[0m\u001b[1;3;38;2;90;149;237m[LlamaIndex] Q: How is PEFT applied to large models like Llama 3.1 and BERT in practice, including trade-offs and example workflows?\n",
            "\u001b[0m\u001b[1;3;38;2;11;159;203m[LlamaIndex] Q: What are recommended best practices for choosing between full fine-tuning and PEFT for different resource constraints and tasks?\n",
            "\u001b[0m\u001b[1;3;38;2;155;135;227m[LlamaIndex] Q: Can you provide illustrative examples or code snippets showing PEFT methods applied to Llama 3.1 and BERT?\n",
            "\u001b[0m\u001b[1;3;38;2;11;159;203m[LlamaIndex] A: BERT is an encoder-only transformer model (Bidirectional Encoder Representations from Transformers). Its architecture uses stacked Transformer encoder layers (examples: BERT BASE has 12 layers, 12 attention heads, 768 hidden units; BERT LARGE has 24 layers, 12 attention heads, 1024 hidden units). Unlike unidirectional models, the encoder processes text bidirectionally, allowing each token’s representation to attend to both left and right context.\n",
            "\n",
            "Pre-training objectives:\n",
            "- Masked Language Modeling (MLM): about 15% of input tokens are masked and the model is trained to predict those masked tokens, producing contextualized token representations.\n",
            "- Next Sentence Prediction (NSP): the model is trained to predict whether one sentence follows another, improving understanding of sentence relationships.\n",
            "\n",
            "Main contributions to NLP:\n",
            "- Strongly contextualized word embeddings by using bidirectional attention, which resolves ambiguities that unidirectional models struggle with.\n",
            "- A pretraining + fine-tuning paradigm that produces versatile representations useful across many downstream tasks (classification, QA, embeddings for sentence-level tasks).\n",
            "- Provided practical model sizes and checkpoints (BASE and LARGE) that established performance baselines and influenced many subsequent models and improvements in transformer-based NLP.\n",
            "\u001b[0m\u001b[1;3;38;2;90;149;237m[LlamaIndex] A: Common use cases:\n",
            "- Complex reasoning and mathematical problem solving: suited for tasks that require multi-step logical or numerical reasoning.\n",
            "- Programming and code generation: strong coding capabilities and high-quality code generation.\n",
            "- Long-text processing: able to handle very long contexts (notably models in this family support very large context lengths), so useful for document summarization, long-form QA, and tasks requiring context over many tokens.\n",
            "- Multilingual processing: effective across multiple languages due to substantial multilingual training data.\n",
            "- Retrieval-augmented generation and tool-backed workflows: supports RAG and zero-shot tool use, enabling agentic behaviors and integrations with external tools (though tool use may lag some specialized competitors).\n",
            "\n",
            "Performance characteristics compared to other large models:\n",
            "- Strong in mathematical and complex reasoning, often outperforming certain competitors on those benchmarks.\n",
            "- Excellent long-text handling, reflected in high scores on long-context quality metrics.\n",
            "- Competitive multilingual performance; generally strong but may have some shortfalls versus top models on specific tool-utilization metrics.\n",
            "- On coding benchmarks, smaller specialized variants can outperform much larger general models (example: a specialized 7B variant outperformed a 70B model on common code benchmarks).\n",
            "- Overall output quality in manual evaluations is comparable to other leading models, with minor differences depending on the specific task (e.g., slightly behind some closed-source models on certain benchmarks).\n",
            "- Offers configurations that trade off size and capability (e.g., 8B, 70B, 405B family examples), with smaller models recommended for efficient local use while larger variants deliver top-end performance.\n",
            "\u001b[0m\u001b[1;3;38;2;11;159;203m[LlamaIndex] A: Recommended best practices based on the provided information:\n",
            "\n",
            "- Prefer PEFT when compute, memory, or storage are constrained\n",
            "  - PEFT fine-tunes only a small number of additional parameters, greatly reducing computational and storage costs compared to full fine-tuning.\n",
            "  - This makes PEFT a good choice when training on consumer hardware or when you need to store many fine-tuned checkpoints (smaller parameter delta vs storing whole models).\n",
            "\n",
            "- Use full fine-tuning when you can afford resources or need maximum task-specific capacity\n",
            "  - If you have abundant GPU memory, compute, and storage and the downstream task demands full-model adaptation for best possible performance, full fine-tuning may be appropriate.\n",
            "\n",
            "- Match method to task and practical guides\n",
            "  - For common downstream tasks (image classification, causal language modeling, automatic speech recognition), consult practical how‑to guides to see PEFT workflows and integrations (e.g., with DeepSpeed or Fully Sharded Data Parallel) that make PEFT practical and efficient.\n",
            "  - If a task is covered in the how‑to guides with PEFT examples, that’s an indication PEFT is a suitable approach.\n",
            "\n",
            "- Consider theoretical trade-offs of specific PEFT methods\n",
            "  - When reducing trainable parameters is a priority, methods like LoRA and soft prompting are specifically intended to reduce the number of trainable parameters and improve training efficiency. Use conceptual guides to choose and tune these methods for your needs.\n",
            "\n",
            "- Leverage integrations and tooling for scalability\n",
            "  - If you need distributed or large-scale runs but still want parameter efficiency, use the documented integrations (DeepSpeed, Fully Sharded Data Parallel) and follow the how‑to guides for best results.\n",
            "\n",
            "- Experiment and benchmark for your setup\n",
            "  - If uncertain, run small comparative experiments (PEFT vs full fine-tuning) on your hardware and dataset to confirm the performance/resource trade-offs for your specific task. Use available examples and reference material to configure training and distributed settings.\n",
            "\n",
            "Summary decision rule:\n",
            "- Limited resources or many target tasks/models → use PEFT (LoRA/soft prompting).\n",
            "- Ample resources and need for maximum adaptation → consider full fine-tuning, but consult guides/integrations if you still want to optimize training.\n",
            "\u001b[0m\u001b[1;3;38;2;237;90;200m[LlamaIndex] A: PEFT (Parameter-Efficient Fine-Tuning) is an approach and supporting library for adapting large pretrained models to downstream tasks while changing only a small fraction of the model’s parameters. The goal is to reduce storage, compute, and memory costs compared with full fine-tuning so training and storing adapted models becomes practical on consumer or low-cost GPUs.\n",
            "\n",
            "Main methods and how they work (high-level):\n",
            "\n",
            "- Adapters\n",
            "  - What they are: Small modules inserted into a pretrained model’s architecture.\n",
            "  - How they work: Instead of updating all model weights, you train the parameters of these lightweight adapter modules while keeping the base model frozen. This reduces the number of trainable parameters and so lowers memory and compute needs.\n",
            "\n",
            "- LoRA (Low-Rank Adaptation)\n",
            "  - What it is: A matrix-decomposition-style method for modifying weight updates.\n",
            "  - How it works: LoRA models parameter updates as low-rank matrices (factorized updates) added to existing weight matrices. Because the update is represented by low-rank factors, the number of parameters to train is much smaller than full-rank updates. Configurations (e.g., rank, alpha, which layers to transform) control how LoRA is applied.\n",
            "\n",
            "- Soft prompting / Prompt tuning (including prompt encoders / p-tuning)\n",
            "  - What it is: Methods that optimize continuous prompt vectors or small encoder modules instead of model weights.\n",
            "  - How they work: A small set of trainable vectors (continuous prompts) or a prompt encoder is learned and prepended or injected into the model’s input representation. The base model remains frozen and the prompt parameters are what get updated, again greatly reducing trainable parameter count.\n",
            "\n",
            "Common properties and benefits\n",
            "- All these methods focus on reducing how many parameters are trained, enabling efficient adaptation of billion-parameter models without full fine-tuning.\n",
            "- They make training more accessible on limited hardware, reduce storage for each downstream task (only small adapter/prompt checkpoints need to be stored), and lower computational and environmental costs.\n",
            "- PEFT is designed to integrate with common training tools and frameworks, and uses configuration files to specify method-specific settings (for example, LoRA configurations). When applying or loading an adapter, it’s important to have the associated adapter configuration (adapter_config.json) that records these parameters.\n",
            "\u001b[0m\u001b[1;3;38;2;155;135;227m[LlamaIndex] A: Typical downstream use cases\n",
            "- Sentence- and token-level classification tasks (e.g., sentiment analysis, intent detection, named-entity recognition) — BERT is an encoder-only model that produces contextualized token representations useful for both token- and sentence-level classifiers.\n",
            "- Sentence pair tasks (e.g., natural language inference, next-sentence / sentence-similarity tasks) — BERT’s pretraining includes Next Sentence Prediction, which helps with modeling relationships between sentences.\n",
            "- Question answering and reading-comprehension (extractive QA) — BERT’s bidirectional contextual embeddings improve prediction of masked or missing text spans by using both left and right context.\n",
            "- Sentence embedding / semantic similarity and retrieval components — BERT’s contextual vectors can be used as features for embedding or downstream ranking.\n",
            "- Feature extractor for downstream models — BERT representations are commonly fine-tuned or used as inputs to task-specific heads (classification, QA span predictors, etc.).\n",
            "\n",
            "Performance strengths\n",
            "- Strong context understanding because of bidirectional processing: it uses tokens from both left and right contexts simultaneously, reducing ambiguity (example: disambiguating “bank” using words before and after the blank).\n",
            "- Effective transfer learning via pretraining: Masked Language Modeling (MLM) enables learning of rich, contextualized token vectors that transfer well to many NLP tasks.\n",
            "- Helpful for sentence-pair relations due to the Next Sentence Prediction (NSP) pretraining objective, which improves modeling of sentence relationships.\n",
            "- Competitive performance with moderate model sizes (BERT BASE: 12 layers, 110M params; BERT LARGE: 24 layers, 340M params), making it practical to fine-tune for many tasks.\n",
            "\n",
            "Limitations\n",
            "- Masked Language Modeling pretraining is not directly optimized for generative, left-to-right text generation — BERT is encoder-only rather than a decoder or causal LM, so it’s less suited as a stand-alone text generator.\n",
            "- NSP and MLM objectives can leave weaknesses on tasks that require long-range document-level reasoning not captured by sentence-pair pretraining.\n",
            "- Model size and compute: larger variants (e.g., BERT LARGE) require substantial compute and memory for fine-tuning and inference compared with lightweight alternatives.\n",
            "- Architecture constraints: because it’s encoder-only, some downstream use cases (causal generation, autoregressive decoding) require different architectures or additional components.\n",
            "\u001b[0m\u001b[1;3;38;2;237;90;200m[LlamaIndex] A: Key features, architecture, and capabilities of Llama 3.1 (from the provided information)\n",
            "\n",
            "Key features\n",
            "- Large-scale model family with multiple parameter sizes (notably 405B, 70B, and 8B).\n",
            "- Open-source release enabling vendor independence, on‑prem or preferred-cloud hosting, and community-driven customization and fine‑tuning.\n",
            "- Very long context support (up to 128K tokens reported for the largest model).\n",
            "- Strong benchmarks for reasoning and coding, including mathematical and complex reasoning.\n",
            "- Multilingual training mix (around half multilingual tokens), giving good multilingual understanding and processing.\n",
            "- Support for retrieval-augmented generation (RAG) and zero‑shot tool use; can exhibit agent-like behaviors.\n",
            "- Competitive performance vs. top closed models: matches or surpasses GPT-4 on many benchmarks and scores strongly on long-text quality (example: 95.2 zero-scrolls quality).\n",
            "- More economical inference costs reported (roughly ~50% lower than comparable closed models like GPT-4o).\n",
            "- Community and provider ecosystem: available through cloud/LLM hosts (AWS, Azure, Google Cloud, Oracle, Together AI, FireworksAI, Groq, etc.) and runnable locally via tools like Ollama for smaller sizes.\n",
            "\n",
            "Architecture and scale\n",
            "- Transformer-based large language model family (derived from the Llama lineage).\n",
            "- Multiple model scales:\n",
            "  - 405 billion parameters: top-tier, trained at very large scale (trillions of tokens, large GPU fleets).\n",
            "  - 70B and 8B parameter variants: much smaller and more accessible for most organizations.\n",
            "- Training infrastructure characteristics for the largest model:\n",
            "  - Massive GPU fleets used (training scale described as involving up to ~16,000 H100 GPUs with 80GB HBM3).\n",
            "  - NVLink interconnects within servers (eight GPUs + two CPUs per server) for highest-scale runs.\n",
            "  - Large distributed storage needs (examples cited: up to ~240 PB total with peak throughput on the order of multiple TB/s).\n",
            "- Smaller variants use high-speed InfiniBand fabrics (Nvidia Quantum2, 400 Gbps interconnects) making distributed training/serving more feasible.\n",
            "\n",
            "Capabilities\n",
            "- Strong mathematical reasoning and complex multi-step logical reasoning.\n",
            "- High-quality code generation and programming assistance.\n",
            "- Excellent long-context understanding and long-text processing.\n",
            "- Robust multilingual capabilities owing to multilingual token mix in training data.\n",
            "- Comparable manual-evaluation quality to other top models (GPT-4, Claude 3.5 Sonnet), with occasional weaknesses noted:\n",
            "  - Slightly inferior in some multi-task language understanding, HumanEval, and MATH benchmarks versus some closed models (but by small margins).\n",
            "  - Noted weaker tool-utilization ability compared to Claude 3.5 Sonnet in the specific benchmark cited.\n",
            "- Flexible deployment/use options:\n",
            "  - Smaller models (8B, 70B) can be run locally or via hosted providers; 8B is recommended for local testing (example: run via Ollama).\n",
            "  - Fine-tuning can be done on much smaller hardware in practice (an example claim: fine-tuning the 405B with 8 H100 GPUs was reported).\n",
            "\n",
            "Operational and team considerations\n",
            "- To train/fine-tune at scale requires expertise in ML/NLP, transformer architectures, data preprocessing, distributed training, model optimization, and tools like PyTorch; ML engineers, ML Ops, and developers are needed.\n",
            "- Deploying or using out‑of‑the‑box models emphasizes software development and cloud operations skills.\n",
            "\n",
            "In short: Llama 3.1 is a large, transformer‑based, open‑source model family that scales from accessible (8B) to very large (405B), offers long context and strong reasoning/coding/multilingual abilities, is designed for flexible deployment (on‑premises or cloud), and aims to deliver high performance at lower inferred cost with strong community-driven customization potential.\n",
            "\u001b[0m\u001b[1;3;38;2;90;149;237m[LlamaIndex] A: PEFT is applied to large models (e.g., Llama 3.1, BERT) by adding and training a small set of additional parameters (adapters, low-rank matrices, or prompts) instead of fine-tuning all model weights. This reduces computation, memory, and storage requirements and allows training and serving on more modest GPUs. Below are practical aspects, trade-offs, and example workflows based on the PEFT approach.\n",
            "\n",
            "How it’s applied in practice\n",
            "- Choose a PEFT method that suits your task:\n",
            "  - Adapters: insert small modules into layers and train only those.\n",
            "  - LoRA (low-rank adaptation): add low-rank update matrices to attention/projection layers and train those parameters.\n",
            "  - Prompt/tuning variants: train small prompt encoders or soft prompts.\n",
            "- Create a PEFT configuration that specifies where and how to apply the method (for LoRA, use a LoraConfig with parameters like base model name/path, lora_alpha, fan_in_fan_out, inference_mode, init_lora_weights, layers_to_transform, etc.).\n",
            "- Combine the PEFT adapter with the pretrained base model (e.g., Llama 3.1, BERT) and set the active adapter for inference/training.\n",
            "- Train using your preferred framework (Transformers Trainer, Accelerate, a custom PyTorch loop). Only the adapter/PEFT parameters are updated.\n",
            "- Save and load adapters; ensure the adapter bundle includes an adapter_config.json because it contains essential configuration for correct application of the PEFT method.\n",
            "\n",
            "Typical example workflow\n",
            "1. Select base model and PEFT method:\n",
            "   - Example: base_model_name_or_path = \"facebook/opt-350m\" (analogous to selecting Llama 3.1 or BERT)\n",
            "   - Choose LoRA, adapters, or prompt tuning depending on task and latency/size goals.\n",
            "2. Create/configure adapter:\n",
            "   - Prepare a JSON config (adapter_config.json) or programmatic LoraConfig specifying target layers, rank (implicit via lora_alpha and structure), bias handling, whether inference_mode is true, init flags, etc.\n",
            "3. Load base model and apply PEFT config:\n",
            "   - Load the pretrained model, wrap or inject adapter modules per configuration, and set the adapter active.\n",
            "4. Train only the adapter:\n",
            "   - Use low-cost GPUs or Accelerate to train just the small adapter parameters. Use standard Trainer or custom loop.\n",
            "5. Save adapter artifacts:\n",
            "   - Save adapter weights plus adapter_config.json so it can be reloaded and applied to the same base model later.\n",
            "6. Inference:\n",
            "   - Load base model, load adapter (ensuring adapter_config.json present), set active adapter, and run inference. Optionally disable/enable adapters as needed.\n",
            "\n",
            "Trade-offs\n",
            "- Pros:\n",
            "  - Large reduction in trainable parameters → lower GPU memory use, faster training, and smaller storage for model checkpoints.\n",
            "  - Makes training large models accessible on consumer or low-cost hardware.\n",
            "  - Comparable performance to full fine-tuning for many tasks.\n",
            "  - Easy integration with existing training stacks (Transformers, Accelerate, Diffusers).\n",
            "- Cons / limitations:\n",
            "  - Some tasks may still benefit from full fine-tuning; PEFT can be slightly less optimal in a few cases.\n",
            "  - You must manage matching base model and adapter configurations; adapter_config.json is required to correctly reapply an adapter.\n",
            "  - Complexity in choosing which layers to adapt and tuning PEFT-specific hyperparameters (e.g., LoRA ranks/alpha, which layers to transform).\n",
            "  - Potential for slightly different inference trade-offs (e.g., added adapter modules can affect latency/throughput depending on deployment).\n",
            "\n",
            "In short: PEFT workflows inject and train small adapter/prompt/low-rank modules (configured via files like adapter_config.json or LoraConfig) on top of large pretrained models, giving large savings in compute and storage at the cost of some configuration choices and occasional small performance trade-offs compared with full fine-tuning.\n",
            "\u001b[0m\u001b[1;3;38;2;155;135;227m[LlamaIndex] A: I can only provide examples drawn from the provided material. The excerpts describe general PEFT workflows and specific tasks (image classification, causal language modeling, automatic speech recognition), and outline the steps to prepare a PeftConfig, create a PeftModel with get_peft_model, and load models for inference with AutoPeftModel / AutoPeftModelFor* classes. They also highlight conceptual guides for LoRA and soft prompting.\n",
            "\n",
            "Based on that guidance, here are two illustrative, high-level code patterns (not model-specific ready-to-run code) that follow the documented PEFT workflow and can be adapted to Llama 3.1 (causal LM style) and BERT (encoder/classification style). Replace model names, configurations, and task-specific pieces with the appropriate Llama 3.1 or BERT checkpoints and PEFT method details when implementing.\n",
            "\n",
            "1) High-level pattern for a causal LM (e.g., Llama 3.1) using a PEFT method\n",
            "- Steps to follow:\n",
            "  1. Prepare a PeftConfig for the chosen PEFT method (e.g., LoRA or prompt tuning).\n",
            "  2. Load the base pretrained causal LM.\n",
            "  3. Wrap the base model with get_peft_model and the PeftConfig to produce a PeftModel.\n",
            "  4. Train the PeftModel as usual.\n",
            "  5. Save and load for inference with an AutoPeftModelForCausalLM (or AutoPeftModel if no task-specific class exists).\n",
            "\n",
            "- Illustrative (framework-level) snippet:\n",
            "from peft import PeftConfig, get_peft_model, AutoPeftModel, AutoPeftModelForCausalLM\n",
            "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
            "\n",
            "# 1) Create a PeftConfig (fill in method-specific fields)\n",
            "peft_config = PeftConfig(\n",
            "    # method name and hyperparameters here (e.g., rank for LoRA, prompt length for prompt tuning)\n",
            ")\n",
            "\n",
            "# 2) Load base Llama causal LM\n",
            "base_model = AutoModelForCausalLM.from_pretrained(\"llama-3.1-base\")  # replace with actual checkpoint\n",
            "tokenizer = AutoTokenizer.from_pretrained(\"llama-3.1-base\")\n",
            "\n",
            "# 3) Create a PeftModel\n",
            "peft_model = get_peft_model(base_model, peft_config)\n",
            "\n",
            "# 4) Train peft_model with your training loop / Trainer\n",
            "\n",
            "# 5) Save and later load for inference\n",
            "peft_model.save_pretrained(\"my-llama3.1-peft\")\n",
            "inference_model = AutoPeftModelForCausalLM.from_pretrained(\"my-llama3.1-peft\")\n",
            "\n",
            "# Use tokenizer + inference_model for generation as usual\n",
            "\n",
            "2) High-level pattern for an encoder/classification model (e.g., BERT) using a PEFT method\n",
            "- Steps to follow:\n",
            "  1. Prepare a PeftConfig for the chosen PEFT method.\n",
            "  2. Load the base pretrained encoder (e.g., BERT).\n",
            "  3. Wrap it with get_peft_model to get a PeftModel.\n",
            "  4. Train for the classification task.\n",
            "  5. Load for inference (use AutoPeftModel if no task-specific AutoPeftModelFor* exists).\n",
            "\n",
            "- Illustrative (framework-level) snippet:\n",
            "from peft import PeftConfig, get_peft_model, AutoPeftModel\n",
            "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
            "\n",
            "# 1) Create a PeftConfig (method-specific)\n",
            "peft_config = PeftConfig(\n",
            "    # fill method name and parameters\n",
            ")\n",
            "\n",
            "# 2) Load base BERT for sequence classification\n",
            "base_model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
            "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
            "\n",
            "# 3) Create PeftModel\n",
            "peft_model = get_peft_model(base_model, peft_config)\n",
            "\n",
            "# 4) Train peft_model on your classification dataset\n",
            "\n",
            "# 5) Save and load for inference (if no AutoPeftModelForSequenceClassification, use AutoPeftModel)\n",
            "peft_model.save_pretrained(\"my-bert-peft\")\n",
            "inference_model = AutoPeftModel.from_pretrained(\"my-bert-peft\")\n",
            "\n",
            "# Tokenize inputs and run inference with inference_model\n",
            "\n",
            "Notes and pointers from the guides\n",
            "- The how-to guides show practical applications across tasks like image classification, causal language modeling, and automatic speech recognition and include using PEFT with DeepSpeed and Fully Sharded Data Parallel scripts.\n",
            "- The quicktour pattern is: (1) prepare a PeftConfig, (2) use get_peft_model to create a PeftModel, then train and save. For inference, use AutoPeftModel or a task-specific AutoPeftModelFor* class when available.\n",
            "- Conceptual guides cover why methods such as LoRA and soft prompting reduce trainable parameters and improve training efficiency; choose method and hyperparameters accordingly.\n",
            "\n",
            "If you want, I can:\n",
            "- Produce concrete, runnable code for Llama 3.1 or BERT by filling in a specific PEFT method (e.g., LoRA with rank and alpha) and exact checkpoint names you plan to use.\n",
            "- Show example Trainer or training loop code and how to integrate DeepSpeed/FSDP. Which model and PEFT method should I use for a concrete snippet?\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
        "from llama_index.core.query_engine import SubQuestionQueryEngine\n",
        "from llama_index.core.question_gen.llm_generators import LLMQuestionGenerator\n",
        "from llama_index.llms.openai import OpenAI\n",
        "\n",
        "llm = OpenAI(model=\"gpt-5-mini\", additional_kwargs={'reasoning_effort':'minimal'})\n",
        "\n",
        "question_gen = LLMQuestionGenerator.from_defaults(llm=llm)\n",
        "\n",
        "query_engine = vector_index.as_query_engine()\n",
        "\n",
        "query_engine_tools = [\n",
        "    QueryEngineTool(\n",
        "        query_engine=query_engine,\n",
        "        metadata=ToolMetadata(\n",
        "            name=\"LlamaIndex\",\n",
        "            description=\"Used to answer the Questions about RAG, Machine Learning, Deep Learning, and Generative AI. Note: Don't repeat the Same question\",\n",
        "        ),\n",
        "    ),\n",
        "]\n",
        "\n",
        "sub_question_engine = SubQuestionQueryEngine.from_defaults(\n",
        "    query_engine_tools=query_engine_tools,\n",
        "    question_gen=question_gen,\n",
        "    use_async=True,\n",
        ")\n",
        "\n",
        "response = sub_question_engine.query(\"Write about Llama 3.1 Model, BERT and PEFT\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "IKxgCfJawjcj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 184
        },
        "outputId": "8a6045d6-1fce-4416-b512-b210c6552cb6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Llama 3.1, BERT, and PEFT — concise overview and how they relate\\n\\nLlama 3.1 — key features, architecture, and capabilities\\n- Model family and release\\n  - A transformer-based large language model family that scales across multiple sizes (notably 405B, 70B, and 8B parameters).\\n  - Open-source release enabling vendor independence, on‑prem or preferred-cloud hosting, and community-driven customization and fine‑tuning.\\n- Architecture and scale\\n  - Transformer encoder-decoder lineage derived from the Llama family but used as a causal/LM-style model family.\\n  - The largest model (405B) was trained at extreme scale (trillions of tokens) using very large GPU fleets (examples up to ~16,000 H100 GPUs with 80GB HBM3), NVLink server topologies for the highest-scale runs, and huge distributed storage/throughput requirements. Smaller variants (70B, 8B) are much more accessible and use high-speed fabrics (InfiniBand / NVidia Quantum2) for distributed training/serving.\\n- Context and capabilities\\n  - Very long context support (up to 128K tokens for the largest model).\\n  - Strong performance on reasoning, math, and coding; competitive with top closed models on many benchmarks and highly rated on long-text quality (example metric: 95.2 zero-scrolls quality).\\n  - Multilingual training mix (~50% multilingual tokens), yielding robust multilingual understanding.\\n  - Supports retrieval-augmented generation (RAG) and zero-shot tool use and can act in agent-like workflows; tool-use performance may lag some specialized competitors on some benchmarks.\\n- Deployment and ecosystem\\n  - Runs on cloud hosts and can be served locally for smaller variants; an ecosystem of providers and tools supports deployment and custom fine-tuning.\\n  - Reported inference cost efficiency (roughly ~50% lower than some comparable closed models in reported examples).\\n- Typical usage patterns\\n  - Use 405B for top-end performance when resources permit; use 70B/8B for more practical on-prem or local use (8B recommended for local testing in examples).\\n\\nBERT — architecture, training objective, contributions, and typical use cases\\n- Architecture\\n  - Encoder-only transformer model (bidirectional encoder representations).\\n  - Example sizes: BASE (12 layers, 12 heads, 768 hidden units), LARGE (24 layers, larger hidden dims). The encoder processes tokens bidirectionally so each representation attends to both left and right context.\\n- Pretraining objectives\\n  - Masked Language Modeling (MLM): randomly mask ~15% of tokens and train to predict them, producing contextualized token embeddings.\\n  - Next Sentence Prediction (NSP): train to predict whether one sentence follows another to help model sentence relationships.\\n- Main contributions to NLP\\n  - Strong contextualized embeddings via bidirectional attention, enabling much better disambiguation than unidirectional approaches for many tasks.\\n  - A pretrain-then-finetune paradigm that generalizes to many downstream tasks (classification, QA, embedding extraction).\\n  - Practical BASE and LARGE checkpoints that set baselines and influenced subsequent transformer work.\\n- Downstream use cases and strengths\\n  - Sentence- and token-level classification (sentiment, intent, NER), sentence-pair tasks (NLI, similarity), extractive QA, and producing embeddings for retrieval/semantic similarity.\\n  - Strengths: excellent contextual understanding from bidirectionality, effective transfer learning via MLM, good performance at moderate model sizes.\\n- Limitations\\n  - Not designed for generative, left-to-right text generation (encoder-only).\\n  - NSP and MLM pretraining may not capture long-range document-level reasoning as well as some other pretraining regimes.\\n  - Larger variants require substantial compute and memory for fine-tuning/inference.\\n\\nPEFT (Parameter-Efficient Fine-Tuning) — what it is and main methods\\n- Purpose\\n  - Adapt large pretrained models to downstream tasks while training only a small fraction of parameters, reducing compute, memory, and storage compared with full fine-tuning.\\n- Main methods\\n  - Adapters: small modules inserted into model layers; only adapter parameters are trained while the base model stays frozen.\\n  - LoRA (Low-Rank Adaptation): represent parameter updates as low-rank factorized matrices added to existing weights; only the low-rank factors are trained.\\n  - Soft prompting / Prompt tuning: train continuous prompt vectors or small prompt-encoder modules prepended/injected into the input representation while keeping the base model frozen.\\n- Common benefits\\n  - Much lower trainable-parameter count → lower GPU memory use, faster training, smaller checkpoints per task.\\n  - Makes adapting very large models feasible on modest hardware and reduces storage overhead for many task-specific variants.\\n- Typical trade-offs\\n  - PEFT often matches full fine-tuning on many tasks but can be slightly less optimal for some tasks.\\n  - Requires managing adapter configurations (e.g., adapter_config.json) and ensuring matching base model + adapter combos during load/inference.\\n  - Choosing which layers to adapt and tuning method-specific hyperparameters (LoRA rank/alpha, adapter size, prompt length) adds configuration complexity.\\n\\nApplying PEFT to Llama 3.1 and BERT — workflows, trade-offs, and best practices\\n- How PEFT is applied in practice (general workflow)\\n  1. Choose base model (Llama 3.1 variant for causal/generation tasks; BERT for encoder/classification tasks) and select a PEFT method (LoRA, adapters, or prompt tuning).\\n  2. Prepare a PEFT configuration (e.g., LoraConfig or adapter_config) specifying which layers to target and method hyperparameters (rank, alpha, prompt length, bias handling, etc.).\\n  3. Load the pretrained base model and wrap it with the PEFT adapter (get_peft_model) to produce a PeftModel.\\n  4. Train only the PEFT parameters using your chosen training loop or Trainer/Accelerate. Use smaller GPUs or fewer resources because only a small subset of parameters are updated.\\n  5. Save the adapter weights and configuration (adapter_config.json); for inference, load the base model and the adapter (AutoPeftModel or task-specific AutoPeftModelFor* when available), then set the adapter active.\\n- Example patterns (high-level)\\n  - Causal LM (Llama 3.1): create a PeftConfig, load AutoModelForCausalLM for the Llama checkpoint, call get_peft_model to inject LoRA/adapters/prompts, train the PeftModel, save and later load with AutoPeftModelForCausalLM for generation.\\n  - Encoder/classification (BERT): create a PeftConfig, load AutoModelForSequenceClassification for BERT, wrap with get_peft_model, train the PeftModel, save and later load with AutoPeftModel for inference.\\n- Trade-offs and when to choose PEFT vs full fine-tuning\\n  - Prefer PEFT when compute, memory, or storage are constrained, when you need many task-specific variants stored cheaply, or when you want to adapt very large models on limited hardware.\\n  - Prefer full fine-tuning when you have ample resources and need maximum possible task-specific performance, or when experiments show PEFT underperforms materially on your task.\\n  - If uncertain, run small comparative experiments on your dataset/hardware to measure performance vs resource use.\\n- Operational considerations\\n  - Using PEFT at scale still requires ML/NLP expertise (model architecture, data preprocessing, distributed training, tuning).\\n  - Integrations with tools (DeepSpeed, FSDP) can help scale PEFT training and deployment while maintaining parameter efficiency.\\n\\nPractical notes and recommendations\\n- For Llama 3.1:\\n  - Use smaller variants (8B or 70B) with PEFT for local or low-cost GPU workflows; reserve the 405B variant for environments with substantial GPU/infra resources when top-end performance is needed.\\n  - Leverage long-context capabilities and RAG workflows for document-heavy tasks; apply LoRA or adapters if you need to specialize behavior without full fine-tuning.\\n- For BERT:\\n  - Use adapters or LoRA for classification, NER, or sentence-pair tasks when you want to save compute/storage and keep strong performance.\\n  - Remember BERT is encoder-only and not ideal for autoregressive generation tasks; PEFT helps adapt BERT to many encoder-style downstream tasks efficiently.\\n- Implementation pointers\\n  - Ensure the adapter bundle includes its configuration file (adapter_config.json) so the adapter can be correctly re-applied to the same base model later.\\n  - Tune PEFT hyperparameters (e.g., LoRA rank and alpha, adapter bottleneck size, prompt length) for your task; defaults may be a good starting point but task-specific tuning helps.\\n  - Save and version adapters separately from base model checkpoints to minimize storage and make reuse simpler.\\n\\nIf you’d like runnable, concrete examples, I can produce a ready-to-run PEFT code snippet for a specific model (pick a Llama 3.1 checkpoint name and a PEFT method like LoRA, or pick a BERT checkpoint and method). Which model and method would you prefer for a detailed code example?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "response.response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCdPwVAQ6ixg"
      },
      "source": [
        "# HyDE Transform\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "1x6He0T961Kg"
      },
      "outputs": [],
      "source": [
        "query_engine = vector_index.as_query_engine()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "0GgtfeBC6m0H"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.indices.query.query_transform import HyDEQueryTransform\n",
        "from llama_index.core.query_engine.transform_query_engine import TransformQueryEngine\n",
        "\n",
        "hyde = HyDEQueryTransform(include_original=True)\n",
        "hyde_query_engine = TransformQueryEngine(query_engine, hyde)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "mm3nYnIE6mwl"
      },
      "outputs": [],
      "source": [
        "response = hyde_query_engine.query(\"Write about Llama 3.1 Model, BERT and PEFT\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "PjTJ2poc6mt5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 184
        },
        "outputId": "bb281154-f204-4190-bb9b-9479d7e3b39b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I can only answer using the provided information. The excerpts you gave discuss Llama-Adapter (a PEFT method for LLaMA), LoRA usage for LLaMA via the 🤗 PEFT library, and related tooling. There is no information in the provided excerpts about \"Llama 3.1\" specifically or about BERT. Below is a summary of what the provided excerpts do cover and how PEFT is presented there.\\n\\nLLaMA-related PEFT (from the excerpts)\\n- LLaMA-Adapter:\\n  - Purpose: A parameter-efficient fine-tuning method to turn a frozen LLaMA model into an instruction-following model.\\n  - Mechanism: Learns a set of adaptation prompts that are prefixed to input tokens and injected via a zero-initialized attention mechanism with zero gating so the frozen model’s pre-trained knowledge is preserved while adding instructional cues.\\n  - Efficiency: Adds only 1.2M learnable parameters to a frozen LLaMA 7B model and fine-tunes in under one hour on 8 A100 GPUs (using 52K self-instruct examples).\\n  - Performance and extensions: Produces high-quality responses comparable to a fully fine-tuned 7B model like Alpaca and can be extended to multi-modal inputs (e.g., images for tasks such as ScienceQA). Code is available on GitHub.\\n\\n- LoRA and PEFT tooling:\\n  - The 🤗 PEFT library supports LoRA for LLaMA fine-tuning; documentation and notebooks are available (including a practical SageMaker notebook for deploying Open-LLaMA for text generation).\\n  - Relevant classes and utilities in the LLaMA docs include configuration, tokenizers, model classes (LlamaModel, LlamaForCausalLM, and task-specific variants), and Flax implementations.\\n\\nWhat I cannot provide from the given excerpts\\n- Any specifics about a \"Llama 3.1\" model (architecture, size, capabilities, release notes) are not present.\\n- Any information about BERT (architecture, variants, pretraining, or fine-tuning) is not included in the provided excerpts.\\n\\nIf you want, I can:\\n- Summarize more details about LLaMA-Adapter or PEFT methods present in the excerpts.\\n- Explain how PEFT approaches like prompt-based adapters and LoRA differ in general terms (if you allow me to use general knowledge).\\n- Search the provided documentation sources for mentions of \"Llama 3.1\" or BERT if you can supply additional excerpts.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "response.response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "StgikqWZ6mrl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fafa77ab-8029-4b9c-909d-937cf9a94d38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Node ID\t e1e3e842-7160-40c4-8e74-772fb8254f5e\n",
            "Text\t # Llama-Adapter[Llama-Adapter](https://hf.co/papers/2303.16199) is a PEFT method specifically designed for turning Llama into an instruction-following model. The Llama model is frozen and only a set of adaptation prompts prefixed to the input instruction tokens are learned. Since randomly initialized modules inserted into the model can cause the model to lose some of its existing knowledge, Llama-Adapter uses zero-initialized attention with zero gating to progressively add the instructional prompts to the model.The abstract from the paper is:*We present LLaMA-Adapter, a lightweight adaption method to efficiently fine-tune LLaMA into an instruction-following model. Using 52K self-instruct demonstrations, LLaMA-Adapter only introduces 1.2M learnable parameters upon the frozen LLaMA 7B model, and costs less than one hour for fine-tuning on 8 A100 GPUs. Specifically, we adopt a set of learnable adaption prompts, and prepend them to the input text tokens at higher transformer layers. Then, a zero-init attention mechanism with zero gating is proposed, which adaptively injects the new instructional cues into LLaMA, while effectively preserves its pre-trained knowledge. With efficient training, LLaMA-Adapter generates high-quality responses, comparable to Alpaca with fully fine-tuned 7B parameters. Furthermore, our approach can be simply extended to multi-modal input, e.g., images, for image-conditioned LLaMA, which achieves superior reasoning capacity on ScienceQA. We release our code at https://github.com/ZrrSkywalker/LLaMA-Adapter*.## AdaptionPromptConfig[[autodoc]] tuners.adaption_prompt.config.AdaptionPromptConfig## AdaptionPromptModel[[autodoc]] tuners.adaption_prompt.model.AdaptionPromptModel\n",
            "Score\t 0.6015401245981568\n",
            "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Node ID\t 781b7b12-eca2-47c0-a66e-9d6be670e951\n",
            "Text\t on how to fine-tune LLaMA model using LoRA method via the 🤗 PEFT library with intuitive UI. 🌎 - A [notebook](https://github.com/aws/amazon-sagemaker-examples/blob/main/introduction_to_amazon_algorithms/jumpstart-foundation-models/text-generation-open-llama.ipynb) on how to deploy Open-LLaMA model for text generation on Amazon SageMaker. 🌎 ## LlamaConfig[[autodoc]] LlamaConfig## LlamaTokenizer[[autodoc]] LlamaTokenizer    - build_inputs_with_special_tokens    - get_special_tokens_mask    - create_token_type_ids_from_sequences    - save_vocabulary## LlamaTokenizerFast[[autodoc]] LlamaTokenizerFast    - build_inputs_with_special_tokens    - get_special_tokens_mask    - create_token_type_ids_from_sequences    - update_post_processor    - save_vocabulary## LlamaModel[[autodoc]] LlamaModel    - forward## LlamaForCausalLM[[autodoc]] LlamaForCausalLM    - forward## LlamaForSequenceClassification[[autodoc]] LlamaForSequenceClassification    - forward## LlamaForQuestionAnswering[[autodoc]] LlamaForQuestionAnswering    - forward## LlamaForTokenClassification[[autodoc]] LlamaForTokenClassification    - forward## FlaxLlamaModel[[autodoc]] FlaxLlamaModel    - __call__## FlaxLlamaForCausalLM[[autodoc]] FlaxLlamaForCausalLM    - __call__\n",
            "Score\t 0.5616809653424492\n",
            "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n"
          ]
        }
      ],
      "source": [
        "for src in response.source_nodes:\n",
        "    print(\"Node ID\\t\", src.node_id)\n",
        "    print(\"Text\\t\", src.text)\n",
        "    print(\"Score\\t\", src.score)\n",
        "    print(\"-_\" * 20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "17Jbo1FH6mjH"
      },
      "outputs": [],
      "source": [
        "query_bundle = hyde(\"Write about Llama 3.1 Model, BERT and PEFT\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "UZEK63K77W7X"
      },
      "outputs": [],
      "source": [
        "hyde_doc = query_bundle.embedding_strs[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "wyzwkpSn7Yi1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 184
        },
        "outputId": "adae6590-a1f9-431a-febb-aedf86b83a3e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Llama 3.1, BERT, and PEFT each occupy important places in modern natural language processing; together they illustrate how foundational architectures, transfer learning, and parameter-efficient tuning interact to build capable, practical language models.\\n\\nLlama 3.1\\nLlama 3.1 is a recent generation in Meta’s Llama family of large language models (LLMs). Like other Llama variants, it is a transformer-based autoregressive model that predicts the next token given prior context. Llama 3.1 advances prior releases in several dimensions: scale, training data curation, architectural refinements, and instruction-following ability. Key features and considerations include:\\n- Model sizes and scaling: Llama 3.1 is available in multiple parameter scales (ranging from billions to hundreds of billions of parameters), enabling trade-offs between latency, memory footprint, and performance. Larger sizes generally yield better few-shot and zero-shot capabilities.\\n- Training data and pretraining: It is pretrained on a mixture of web, code, dialog, and curated high-quality corpora. Improvements in data filtering and diversity are intended to reduce toxic or low-quality outputs and to improve factuality.\\n- Architecture and optimizations: Llama 3.1 retains the transformer decoder architecture (multi-head self-attention, feed-forward networks) but typically includes engineering improvements: better normalization, optimized rotary or ALiBi-style positional encodings, and efficiency improvements for longer context windows.\\n- Instruction tuning and alignment: Llama 3.1 is often fine-tuned with supervised instruction datasets and reinforcement learning from human feedback (RLHF) or other preference-learning methods to make it more helpful, safe, and aligned to user intent. This yields stronger performance on conversational tasks, step-by-step reasoning, and following complex instructions.\\n- Deployment and runtime: Practical deployment considerations include quantization (8-bit/4-bit or other compressed formats), memory-efficient attention kernels, and model parallelism strategies that allow use in both cloud and on-device scenarios.\\n- Capabilities and limitations: Llama 3.1 shows strong language generation, code writing, summarization, and reasoning abilities, but it still faces limitations like hallucinations, sensitivity to prompt phrasing, and possible generation of unsafe or biased content if not properly aligned and filtered.\\n\\nBERT\\nBERT (Bidirectional Encoder Representations from Transformers) is a foundational transformer model introduced by Google in 2018 that transformed many NLP tasks by providing deep, pre-trained contextualized word representations. Key details:\\n- Architecture: BERT uses the transformer encoder stack (multi-headed self-attention and position-wise feed-forward layers) and is bidirectional: during pretraining it masks tokens and learns to predict them from both left and right context simultaneously (Masked Language Modeling, MLM).\\n- Pretraining objectives: The main objectives are MLM and Next Sentence Prediction (NSP) in the original paper (though many later variants drop or replace NSP). MLM encourages contextualized, bidirectional representations that capture nuanced semantics.\\n- Model sizes: Common variants include BERT-Base (12 layers, 768 hidden size, 12 heads, ~110M parameters) and BERT-Large (24 layers, 1024 hidden size, 16 heads, ~340M parameters).\\n- Transfer learning: BERT is pretrained once on large corpora (BooksCorpus, English Wikipedia) and then fine-tuned on downstream tasks (classification, QA, NER, etc.) by adding small task-specific heads. Fine-tuning often requires small labeled datasets and yields state-of-the-art results across many benchmarks when it was introduced.\\n- Strengths and limitations: BERT excels at encoding input text for discriminative tasks, producing rich contextual embeddings. However, because it is an encoder-only model, it is not naturally suited to autoregressive generation; for generative tasks other architectures (decoder-only or encoder-decoder) are more appropriate. BERT also has fixed-size input contexts (typical 512 tokens) and can be computationally expensive at larger scales.\\n- Legacy and influence: BERT popularized pretraining + fine-tuning and inspired many subsequent models (RoBERTa, DistilBERT, ALBERT, etc.) and techniques (span masking, larger-scale pretraining). Its embeddings and checkpoints remain widely used in industry and research.\\n\\nPEFT (Parameter-Efficient Fine-Tuning)\\nPEFT denotes a family of methods designed to adapt large pretrained models to downstream tasks while changing only a small fraction of parameters, reducing storage, compute, and data needs. PEFT methods are especially valuable when fine-tuning very large LLMs (like Llama 3.1-scale models). Key methods and properties:\\n- Motivation: Fine-tuning all parameters of a huge model is costly in GPU memory, requires storing full-size checkpoints per task, and risks catastrophic forgetting. PEFT reduces the number of trainable parameters, enabling efficient adaptation and easier model sharing.\\n- Popular techniques:\\n  - Adapters: Small bottleneck feed-forward layers are inserted between existing transformer layers. Only adapter weights are trained; the pre-trained model weights are frozen. Adapters can be stacked or merged.\\n  - LoRA (Low-Rank Adaptation): Decomposes weight updates into low-rank matrices added to existing weight matrices; only the low-rank factors are trained, drastically lowering parameter count and memory usage.\\n  - Prefix-tuning and Prompt tuning: Learnable continuous prompts or prefix tokens prepended to the input or attention keys/values; only the prompt vectors are trained, leaving model weights fixed.\\n  - BitFit: Only bias terms are fine-tuned, an extremely lightweight approach that can work surprisingly well for certain tasks.\\n  - QLoRA: Combines low-bit quantization with LoRA-style updates to enable fine-tuning of very large quantized models on modest hardware.\\n- Trade-offs: PEFT methods often achieve competitive or near–full fine-tuning performance while using a tiny fraction of parameters, but the exact effectiveness depends on task, base model size, and hyperparameters (rank for LoRA, adapter size, prompt length, etc.). Some PEFTs are better for generative tasks, others for discriminative tasks.\\n- Practical workflow: Typical usage freezes the base model, applies a PEFT module (e.g., LoRA layers), trains only the PEFT parameters with task-specific data, and stores only the small PEFT parameter set. At inference, the PEFT modifications can be applied virtually or merged into weights if desired for deployment.\\n- Ecosystem and tools: Libraries such as Hugging Face’s PEFT, bitsandbytes (for quantization), and adapters frameworks provide implementations that integrate with major model families and accelerate adoption.\\n\\nHow they relate\\n- BERT and Llama 3.1 represent different modeling paradigms: BERT is encoder-based and optimized for producing contextual representations for discriminative tasks, while Llama 3.1 is a decoder-only autoregressive LLM focused on generation and instruction-following. Techniques and lessons from BERT-era transfer learning informed later LLM pretraining and fine-tuning practices.\\n- PEFT methods are highly relevant to both families: for very large models like Llama 3.1, PEFT (LoRA, adapters, prefix tuning, QLoRA) enables practical fine-tuning and deployment across many downstream tasks without retraining or storing full model copies. BERT and its descendants also benefit from adapters and other PEFT approaches to enable multi-task adapters or domain adaptation with minimal parameter overhead.\\n- Combined workflow example: Start from a pretrained Llama 3.1 or BERT checkpoint, apply a PEFT method (e.g., LoRA or adapters), fine-tune on a labeled instruction or task dataset, and optionally perform alignment tuning (e.g., RLHF) or evaluation. This yields task-specialized models that are cheaper to train and maintain.\\n\\nSummary\\nBERT introduced the paradigm of deep bidirectional contextual encoders and transformed transfer learning in NLP. Llama 3.1 builds on decades of transformer research to provide a large autoregressive model trained and tuned for high-quality generation and instruction following. PEFT provides the practical set of methods that make adapting such large models efficient and practical, enabling fine-tuning and deployment across many tasks with minimal additional parameters and compute. Together they illustrate the evolution from foundational encoders to massive generative models and the engineering techniques that make large-scale adaptation feasible.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "hyde_doc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "rSHSMLQHmvh0"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "llamaindexkernel",
      "language": "python",
      "name": "llamaindexkernel"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "accb4f3f09c14b878f47a3c1ed9e4377": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_110bfb27846a4834ba4337d53d533907",
              "IPY_MODEL_06fe752a8ccf4468976bf06cb20c5f0d",
              "IPY_MODEL_ce0441d29cc341e18d58ad7408d4604d"
            ],
            "layout": "IPY_MODEL_1b229f5c86214f5fb74f7d9089073939"
          }
        },
        "110bfb27846a4834ba4337d53d533907": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d8ffac78365447788cf5cbb5bf22637c",
            "placeholder": "​",
            "style": "IPY_MODEL_6653c88dfc7c4bfca1ded0c50d5f5503",
            "value": "vectorstore.zip: 100%"
          }
        },
        "06fe752a8ccf4468976bf06cb20c5f0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_debdadab590a4a9aa0de5b31fa88c5c7",
            "max": 97198458,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b9b5c5e401ed482b817a3d9c2e690d2e",
            "value": 97198458
          }
        },
        "ce0441d29cc341e18d58ad7408d4604d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c66d9fec492a4a20a4ce14ceadee0c8d",
            "placeholder": "​",
            "style": "IPY_MODEL_1201c501de2f4a6ab8050c93c70e95ab",
            "value": " 97.2M/97.2M [00:01&lt;00:00, 73.3MB/s]"
          }
        },
        "1b229f5c86214f5fb74f7d9089073939": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d8ffac78365447788cf5cbb5bf22637c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6653c88dfc7c4bfca1ded0c50d5f5503": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "debdadab590a4a9aa0de5b31fa88c5c7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b9b5c5e401ed482b817a3d9c2e690d2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c66d9fec492a4a20a4ce14ceadee0c8d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1201c501de2f4a6ab8050c93c70e95ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}