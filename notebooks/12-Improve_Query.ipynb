{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-zE1h0uQV7uT"
   },
   "source": [
    "# Install Packages and Setup Variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "QPJzr-I9XQ7l",
    "outputId": "883b9e93-144f-4f09-9362-f5f76cbd95bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/56.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m51.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m455.6/455.6 kB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.4/177.4 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m44.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m584.3/584.3 kB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m171.5/171.5 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.9/79.9 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m68.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.5/15.5 MB\u001b[0m \u001b[31m92.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.6/278.6 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.2/139.2 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m79.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.7/150.7 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m679.1/679.1 kB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m60.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m50.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.1/211.1 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m81.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.5/52.5 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.7/149.7 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.0/64.0 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m82.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.2/44.2 MB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.8/70.8 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m85.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m63.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.7/410.7 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.0/241.0 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m262.5/262.5 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.8/295.8 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.9/104.9 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m486.3/486.3 kB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m59.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m452.6/452.6 kB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.6/137.6 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.6/49.6 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for html2text (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for tinysegmenter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for spider-client (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for feedfinder2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for jieba3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "!pip install -q llama-index==0.10.57 openai==1.59.8 llama-index-finetuning==0.1.12 llama-index-embeddings-huggingface==0.2.3 llama-index-embeddings-cohere==0.1.9 llama-index-readers-web==0.1.23 cohere==5.6.2 tiktoken==0.8.0 chromadb==0.5.5 html2text==2024.2.26 sentence-transformers==2.7.0 pydantic==2.10.5 llama-index-vector-stores-chroma==0.1.10 kaleido==0.2.1 llama-index-llms-gemini==0.1.11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "riuXwpSPcvWC"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set the following API Keys in the Python environment. Will be used later.\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"<YOUR-OPENAI-API-KEY>\"\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"<YOUR-GOOGLE-API-KEY>\"\n",
    "\n",
    "# from google.colab import userdata\n",
    "# os.environ[\"OPENAI_API_KEY\"] = userdata.get('openai_api_key')\n",
    "# os.environ[\"GOOGLE_API_KEY\"] = userdata.get('Google_api_key')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "jIEeZzqLbz0J"
   },
   "outputs": [],
   "source": [
    "# Allows running asyncio in environments with an existing event loop, like Jupyter notebooks.\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bkgi2OrYzF7q"
   },
   "source": [
    "# Load a Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "9oGT6crooSSj"
   },
   "outputs": [],
   "source": [
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core import Settings\n",
    "\n",
    "Settings.llm = OpenAI(temperature=0, model=\"gpt-4o-mini\")\n",
    "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OWaT6rL7ksp8"
   },
   "source": [
    "# Load Indexes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "bb35006ab7a54331acd68b405e28181a",
      "69dcde5d56c74471877aa8cf238297a5",
      "b23090935201420b83a2a7e625fd7975",
      "06d84f1a1e284b37a8416f43f7aec7cc",
      "c34a5a2cf12a434080394da3370b4a7b",
      "b4572be914ea4760b23e25a697dfdab0",
      "20da933399044214921a78da04870368",
      "7a65728464034191986d0c36331a9cc6",
      "8ae06d18f5f8496a873268e6d654462a",
      "3c7345bf99fb485cbd4b927cd0628c9b",
      "e96602b698204adf884619ed375303a6"
     ]
    },
    "id": "5sP-_zZso337",
    "outputId": "9d59ba62-0b59-4b7c-a2b8-9a87c7366e2b"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb35006ab7a54331acd68b405e28181a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vectorstore.zip:   0%|          | 0.00/97.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "vectorstore = hf_hub_download(repo_id=\"jaiganesan/ai_tutor_knowledge\", filename=\"vectorstore.zip\", repo_type=\"dataset\", local_dir=\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SodY2Xpf_kxg",
    "outputId": "8a71fbef-1aa7-4566-d607-de756ad75bca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  vectorstore.zip\n",
      "   creating: ai_tutor_knowledge/\n",
      "   creating: ai_tutor_knowledge/684af133-f877-4230-bde4-575cf53b6688/\n",
      "  inflating: ai_tutor_knowledge/684af133-f877-4230-bde4-575cf53b6688/length.bin  \n",
      "  inflating: ai_tutor_knowledge/684af133-f877-4230-bde4-575cf53b6688/index_metadata.pickle  \n",
      "  inflating: ai_tutor_knowledge/684af133-f877-4230-bde4-575cf53b6688/link_lists.bin  \n",
      "  inflating: ai_tutor_knowledge/684af133-f877-4230-bde4-575cf53b6688/header.bin  \n",
      "  inflating: ai_tutor_knowledge/684af133-f877-4230-bde4-575cf53b6688/data_level0.bin  \n",
      "  inflating: ai_tutor_knowledge/chroma.sqlite3  \n"
     ]
    }
   ],
   "source": [
    "!unzip -o vectorstore.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "mXi56KTXk2sp"
   },
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "# Create the vector index\n",
    "db = chromadb.PersistentClient(path=\"./ai_tutor_knowledge\")\n",
    "chroma_collection = db.get_or_create_collection(\"ai_tutor_knowledge\")\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "vector_index = VectorStoreIndex.from_vector_store(vector_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SLrn8A3jckmW"
   },
   "source": [
    "# Multi-Step Query Engine\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UmpfpVCje8h3"
   },
   "source": [
    "## GPT-4o-mini\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "8y-Ya3GyfcAk"
   },
   "outputs": [],
   "source": [
    "from llama_index.core.indices.query.query_transform.base import (\n",
    "    StepDecomposeQueryTransform,\n",
    ")\n",
    "\n",
    "step_decompose_transform_gpt4o = StepDecomposeQueryTransform(verbose=True, llm=Settings.llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "zntXdSbGf_qF"
   },
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine.multistep_query_engine import MultiStepQueryEngine\n",
    "\n",
    "#Default query engine\n",
    "query_engine_gpt4o_mini = vector_index.as_query_engine()\n",
    "\n",
    "# Multi Step Query Engine\n",
    "multi_step_query_engine = MultiStepQueryEngine(\n",
    "    query_engine = query_engine_gpt4o_mini,\n",
    "    query_transform = step_decompose_transform_gpt4o,\n",
    "    index_summary = \"Used to answer the Questions about RAG, Machine Learning, Deep Learning, and Generative AI, Note: Don't repeat the Same quesion\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8JPD8yAinVSq"
   },
   "source": [
    "# Query Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D2IByQ5-ox9U"
   },
   "source": [
    "## Default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b0gue7cyctt1",
    "outputId": "7270fb24-9dfe-4d72-e849-578941fd962e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Llama model, particularly in its adaptations like Llama-Adapter, focuses on efficient fine-tuning techniques to enhance its capabilities, such as transforming it into an instruction-following model. This is achieved through the use of adaptation prompts that are integrated into the model while keeping its pre-trained knowledge intact. The Llama-Adapter method introduces a minimal number of learnable parameters, making it a lightweight solution for fine-tuning.\n",
      "\n",
      "BERT, another foundational model in natural language processing, employs a different architecture and training approach, primarily focusing on masked language modeling and next sentence prediction. While BERT is designed for a variety of NLP tasks, it typically requires more extensive fine-tuning compared to methods like Llama-Adapter, which emphasizes parameter efficiency.\n",
      "\n",
      "The PEFT (Parameter-Efficient Fine-Tuning) methods, including LoRA and Llama-Adapter, are designed to adapt large models like Llama and BERT to specific tasks with fewer resources. These methods allow for effective model adaptation without the need for full retraining, thus saving time and computational power while maintaining performance.\n"
     ]
    }
   ],
   "source": [
    "# Default query engine\n",
    "query_engine = vector_index.as_query_engine()\n",
    "res = query_engine.query(\"Write about Llama 3.1 Model, BERT and PEFT methods\")\n",
    "print(res.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "465dH4yQc7Ct",
    "outputId": "401d5785-073c-48b0-d6e0-06268ed88024"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node ID\t 781b7b12-eca2-47c0-a66e-9d6be670e951\n",
      "Title\t LLaMA\n",
      "Text\t on how to fine-tune LLaMA model using LoRA method via the 🤗 PEFT library with intuitive UI. 🌎 - A [notebook](https://github.com/aws/amazon-sagemaker-examples/blob/main/introduction_to_amazon_algorithms/jumpstart-foundation-models/text-generation-open-llama.ipynb) on how to deploy Open-LLaMA model for text generation on Amazon SageMaker. 🌎 ## LlamaConfig[[autodoc]] LlamaConfig## LlamaTokenizer[[autodoc]] LlamaTokenizer    - build_inputs_with_special_tokens    - get_special_tokens_mask    - create_token_type_ids_from_sequences    - save_vocabulary## LlamaTokenizerFast[[autodoc]] LlamaTokenizerFast    - build_inputs_with_special_tokens    - get_special_tokens_mask    - create_token_type_ids_from_sequences    - update_post_processor    - save_vocabulary## LlamaModel[[autodoc]] LlamaModel    - forward## LlamaForCausalLM[[autodoc]] LlamaForCausalLM    - forward## LlamaForSequenceClassification[[autodoc]] LlamaForSequenceClassification    - forward## LlamaForQuestionAnswering[[autodoc]] LlamaForQuestionAnswering    - forward## LlamaForTokenClassification[[autodoc]] LlamaForTokenClassification    - forward## FlaxLlamaModel[[autodoc]] FlaxLlamaModel    - __call__## FlaxLlamaForCausalLM[[autodoc]] FlaxLlamaForCausalLM    - __call__\n",
      "Score\t 0.43887592282330873\n",
      "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
      "Node ID\t e1e3e842-7160-40c4-8e74-772fb8254f5e\n",
      "Title\t Llama-Adapter\n",
      "Text\t # Llama-Adapter[Llama-Adapter](https://hf.co/papers/2303.16199) is a PEFT method specifically designed for turning Llama into an instruction-following model. The Llama model is frozen and only a set of adaptation prompts prefixed to the input instruction tokens are learned. Since randomly initialized modules inserted into the model can cause the model to lose some of its existing knowledge, Llama-Adapter uses zero-initialized attention with zero gating to progressively add the instructional prompts to the model.The abstract from the paper is:*We present LLaMA-Adapter, a lightweight adaption method to efficiently fine-tune LLaMA into an instruction-following model. Using 52K self-instruct demonstrations, LLaMA-Adapter only introduces 1.2M learnable parameters upon the frozen LLaMA 7B model, and costs less than one hour for fine-tuning on 8 A100 GPUs. Specifically, we adopt a set of learnable adaption prompts, and prepend them to the input text tokens at higher transformer layers. Then, a zero-init attention mechanism with zero gating is proposed, which adaptively injects the new instructional cues into LLaMA, while effectively preserves its pre-trained knowledge. With efficient training, LLaMA-Adapter generates high-quality responses, comparable to Alpaca with fully fine-tuned 7B parameters. Furthermore, our approach can be simply extended to multi-modal input, e.g., images, for image-conditioned LLaMA, which achieves superior reasoning capacity on ScienceQA. We release our code at https://github.com/ZrrSkywalker/LLaMA-Adapter*.## AdaptionPromptConfig[[autodoc]] tuners.adaption_prompt.config.AdaptionPromptConfig## AdaptionPromptModel[[autodoc]] tuners.adaption_prompt.model.AdaptionPromptModel\n",
      "Score\t 0.4378040851975304\n",
      "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n"
     ]
    }
   ],
   "source": [
    "for src in res.source_nodes:\n",
    "    print(\"Node ID\\t\", src.node_id)\n",
    "    print(\"Title\\t\", src.metadata[\"title\"])\n",
    "    print(\"Text\\t\", src.text)\n",
    "    print(\"Score\\t\", src.score)\n",
    "    print(\"-_\" * 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2y2AiInmpz7g"
   },
   "source": [
    "## GPT-4o-mini Multi-Step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "69kADAFilW1n",
    "outputId": "ea77b464-ca2c-4bea-e994-1d1e2e1cf47d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;33m> Current query: Write about Llama 3.1 Model, BERT and PEFT methods\n",
      "\u001b[0m\u001b[1;3;38;5;200m> New query: What are the key features of the Llama 3.1 Model?\n",
      "\u001b[0m\u001b[1;3;33m> Current query: Write about Llama 3.1 Model, BERT and PEFT methods\n",
      "\u001b[0m\u001b[1;3;38;5;200m> New query: What are the key features of the Llama 3.1 Model?\n",
      "\u001b[0m\u001b[1;3;33m> Current query: Write about Llama 3.1 Model, BERT and PEFT methods\n",
      "\u001b[0m\u001b[1;3;38;5;200m> New query: What are the key features of BERT?\n",
      "\u001b[0mThe Llama 3.1 model is a state-of-the-art language model developed by Meta, featuring significant advancements in scale and capabilities. It is the largest model from Meta, trained on over 15 trillion tokens using more than 16,000 H100 GPUs. One of its standout features is the extended context length of 128K, which allows it to process and understand longer texts effectively. The model demonstrates enhanced reasoning and coding abilities, excels in generating high-quality code, and showcases strong logical reasoning and problem-solving skills. Additionally, it supports zero-shot tool use and has a multilingual processing capability, with about 50% of its training data consisting of multilingual tokens. In benchmark tests, Llama 3.1 outperforms competing models in various areas, including mathematical reasoning and long text processing.\n",
      "\n",
      "BERT, or Bidirectional Encoder Representations from Transformers, is another influential model in the field of natural language processing. It is designed with a bidirectional processing capability, allowing it to capture context from both directions simultaneously, which enhances its understanding of word meanings based on surrounding text. Built on the transformer architecture, BERT utilizes an encoder-only model to generate contextualized word embeddings. It is pre-trained on two main tasks: Masked Language Modeling (MLM) and Next Sentence Prediction (NSP), which help the model learn contextual relationships and improve its performance on various language tasks. BERT comes in two variants, BERT BASE and BERT LARGE, offering different levels of complexity and performance.\n",
      "\n",
      "PEFT, or Parameter-Efficient Fine-Tuning, refers to methods that allow for fine-tuning large pre-trained models like Llama 3.1 and BERT with fewer parameters. This approach is particularly beneficial in scenarios where computational resources are limited or when adapting models to specific tasks without the need for extensive retraining. PEFT techniques can include methods such as adapters, prompt tuning, and low-rank adaptation, which enable efficient model customization while maintaining the benefits of the original pre-trained model. These methods are increasingly important in the evolving landscape of AI, where the demand for specialized applications continues to grow.\n"
     ]
    }
   ],
   "source": [
    "response = multi_step_query_engine.query(\"Write about Llama 3.1 Model, BERT and PEFT methods\")\n",
    "print(response.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wGmHP72Iu10s",
    "outputId": "14811d9a-107e-4fed-dda8-6df57d677d1a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**What are the key features of the Llama 3.1 Model?**\n",
      "The Llama 3.1 model boasts several key features, including:\n",
      "\n",
      "1. **Model Scale and Training**: It is the largest model from Meta, trained on over 15 trillion tokens using more than 16,000 H100 GPUs.\n",
      "\n",
      "2. **Extended Context Length**: The model supports a context length of 128K, enhancing its ability to handle longer inputs.\n",
      "\n",
      "3. **Enhanced Capabilities**: It demonstrates improved reasoning and coding abilities compared to its predecessors.\n",
      "\n",
      "4. **Tool Use and Agentic Behaviors**: The model supports zero-shot tool use, allowing it to perform tasks without prior training on specific tools.\n",
      "\n",
      "5. **Multilingual Processing**: Approximately 50% of its training data consists of multilingual tokens, enabling effective understanding and processing of multiple languages.\n",
      "\n",
      "6. **Programming and Reasoning Skills**: Llama 3.1 excels in generating high-quality code and exhibits strong logical reasoning, problem-solving, and analytical skills.\n",
      "\n",
      "7. **Benchmark Performance**: It outperforms competing models like GPT-4o and Claude 3.5 Sonnet in areas such as mathematical reasoning, complex reasoning, multilingual support, and long text processing.\n",
      "\n",
      "These features collectively position Llama 3.1 as a powerful and versatile language model in the AI landscape.\n",
      "\n",
      "**What are the key features of the Llama 3.1 Model?**\n",
      "The Llama 3.1 model boasts several key features, including:\n",
      "\n",
      "1. **Model Scale and Training**: It is the largest model from Meta, trained on over 15 trillion tokens using more than 16,000 H100 GPUs.\n",
      "\n",
      "2. **Extended Context Length**: The model supports a context length of 128K, enhancing its ability to process and understand longer texts.\n",
      "\n",
      "3. **Enhanced Capabilities**: It demonstrates improved reasoning and coding abilities compared to its predecessors.\n",
      "\n",
      "4. **Multilingual Processing**: Approximately 50% of its training data consists of multilingual tokens, allowing it to effectively understand and process multiple languages.\n",
      "\n",
      "5. **Programming and Reasoning Skills**: Llama 3.1 excels in generating high-quality code and exhibits strong logical reasoning, problem-solving, and analytical skills.\n",
      "\n",
      "6. **Support for Tool Use**: The model can engage in zero-shot tool use and develop agentic behaviors.\n",
      "\n",
      "7. **Benchmark Performance**: It outperforms competing models like GPT-4o and Claude 3.5 Sonnet in areas such as mathematical reasoning, complex reasoning, multilingual support, and long text processing.\n",
      "\n",
      "These features collectively position Llama 3.1 as a powerful and versatile language model.\n",
      "\n",
      "**What are the key features of BERT?**\n",
      "BERT, or Bidirectional Encoder Representations from Transformers, is characterized by several key features:\n",
      "\n",
      "1. **Bidirectional Processing**: BERT processes text in both directions simultaneously, allowing it to capture context more effectively than traditional unidirectional models. This capability enables the model to consider the entire context of a sentence, improving its understanding of word meanings based on surrounding text.\n",
      "\n",
      "2. **Transformer Architecture**: BERT is built on the transformer architecture, specifically utilizing an encoder-only model. This design allows for the generation of contextualized word embeddings, which are crucial for understanding language nuances.\n",
      "\n",
      "3. **Pre-training Tasks**: BERT is pre-trained on two main tasks:\n",
      "   - **Masked Language Modeling (MLM)**: This involves predicting masked tokens in sentences, helping the model learn contextualized representations of words.\n",
      "   - **Next Sentence Prediction (NSP)**: This task requires the model to predict whether one sentence is likely to follow another, enhancing its understanding of sentence relationships.\n",
      "\n",
      "4. **Model Variants**: BERT comes in two versions: BERT BASE, which has 12 layers and 110 million parameters, and BERT LARGE, which has 24 layers and 340 million parameters. These variants allow for different levels of complexity and performance.\n",
      "\n",
      "Overall, BERT's innovative architecture and training methods significantly contribute to its effectiveness in natural language processing tasks.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for query, response in response.metadata['sub_qa']:\n",
    "    print(f\"**{query}**\\n{response}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k5pJPBPRqjbG",
    "outputId": "70343d65-4b15-4dc8-c6dc-5a0a34563d1e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node ID\t a6ebe22b-5529-4e78-93a6-e7336d031600\n",
      "Text\t GPT-2  GPT-3  and GPT-4  which were decoder-only architectures. Another well-known example is BERT (Bidirectional Encoder Representations from Transformers)  an encoder-only transformer mode used as a component in sentence embedding models.   Lets talk about BERT!BERT stands for Bidirectional Encoder Representations from Transformers. It is a language model by Google that uses a transformer architecture to understand and generate human-like language. BERT is designed to simultaneously process text in both directions  allowing it to capture context more effectively than traditional unidirectional models  which read text sequentially from left to right or right to left.   Example of Bidirectional CapabilityConsider the sentence:    The bank is situated on the _______ of the river.   In a unidirectional model  understanding the blank would primarily rely on the words before it  potentially leading to ambiguity about whether bank refers to a financial institution or the side of a river.   However  BERTs bidirectional approach allows it to use the entire sentences context  including the words before and after the blank. Thus  the missing word is likely related to the river  resulting in a more accurate prediction  such as bank referring to the riverbank rather than a financial institution.   BERT has two versions:   BERT BASE with    Layers: 12Parameters: 110MAttention Heads: 12Hidden Units: 768BERT LARGE with    Layers: 24Parameters: 340MAttention Heads: 12Hidden Units: 1024DYK?BERT was pre-trained on 3.3 Billion words!   What was it pre-trained on? For what?BERT was pre-trained on two tasks:   Masked Language Modeling (MLM):The inputs are sentences that start with a special token called CLS (Classify Token) and end with a SEP (separator token).   Words  tokens (consider)   Around 15% of the input tokens are masked  and the model is trained to predict those masked tokens.   The model learns to produce contextualized vectors based on the surrounding words at this stage. Read the example above and reread this sentence.   Next Sentence Prediction (NSP):In this one  the model predicts if one sentence is likely to follow another.\n",
      "Score\t 0.47732872464031784\n",
      "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
      "Node ID\t 5c948017-64fe-4457-9d22-548f03306e1e\n",
      "Text\t # DeBERTa-v2## OverviewThe DeBERTa model was proposed in [DeBERTa: Decoding-enhanced BERT with Disentangled Attention](https://arxiv.org/abs/2006.03654) by Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen It is based on Google'sBERT model released in 2018 and Facebook's RoBERTa model released in 2019.It builds on RoBERTa with disentangled attention and enhanced mask decoder training with half of the data used inRoBERTa.The abstract from the paper is the following:*Recent progress in pre-trained neural language models has significantly improved the performance of many naturallanguage processing (NLP) tasks. In this paper we propose a new model architecture DeBERTa (Decoding-enhanced BERT withdisentangled attention) that improves the BERT and RoBERTa models using two novel techniques. The first is thedisentangled attention mechanism, where each word is represented using two vectors that encode its content andposition, respectively, and the attention weights among words are computed using disentangled matrices on theircontents and relative positions. Second, an enhanced mask decoder is used to replace the output softmax layer topredict the masked tokens for model pretraining. We show that these two techniques significantly improve the efficiencyof model pretraining and performance of downstream tasks. Compared to RoBERTa-Large, a DeBERTa model trained on half ofthe training data performs consistently better on a wide range of NLP tasks, achieving improvements on MNLI by +0.9%(90.2% vs. 91.1%), on SQuAD v2.0 by +2.3% (88.4% vs. 90.7%) and RACE by +3.6% (83.2% vs. 86.8%). The DeBERTa code andpre-trained models will be made publicly available at https://github.com/microsoft/DeBERTa.*The following information is visible directly on the [original implementationrepository](https://github.com/microsoft/DeBERTa). DeBERTa v2 is the second version of the DeBERTa model. It includesthe 1.5B model used for the SuperGLUE single-model submission and achieving 89.9, versus human baseline 89.8. You canfind more details about this\n",
      "Score\t 0.44709040168047093\n",
      "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n"
     ]
    }
   ],
   "source": [
    "for src in response.source_nodes:\n",
    "    print(\"Node ID\\t\", src.node_id)\n",
    "    print(\"Text\\t\", src.text)\n",
    "    print(\"Score\\t\", src.score)\n",
    "    print(\"-_\" * 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jwcSCiMhp4Uh"
   },
   "source": [
    "# Test gemini-1.5-flash Multi-Step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "id": "uH9gNfZuslHK",
    "outputId": "f9852cab-842f-4d0f-8036-9ee98111eba2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-43-46371d5420d8>:11: DeprecationWarning: Call to deprecated class method from_defaults. (ServiceContext is deprecated, please use `llama_index.settings.Settings` instead.) -- Deprecated since version 0.10.0.\n",
      "  service_context_gemini = ServiceContext.from_defaults(llm=llm)\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import ServiceContext\n",
    "from llama_index.core.indices.query.query_transform.base import (\n",
    "    StepDecomposeQueryTransform,\n",
    ")\n",
    "from llama_index.core.query_engine.multistep_query_engine import MultiStepQueryEngine\n",
    "\n",
    "from llama_index.llms.gemini import Gemini\n",
    "\n",
    "llm = Gemini(model=\"models/gemini-1.5-flash\")\n",
    "\n",
    "service_context_gemini = ServiceContext.from_defaults(llm=llm)\n",
    "\n",
    "step_decompose_transform = StepDecomposeQueryTransform(llm=llm, verbose=True)\n",
    "\n",
    "query_engine_gemini = vector_index.as_query_engine(\n",
    "    service_context=service_context_gemini\n",
    ")\n",
    "query_engine_gemini = MultiStepQueryEngine(\n",
    "    query_engine=query_engine_gemini,\n",
    "    query_transform=step_decompose_transform,\n",
    "    index_summary=\"Used to answer the Questions about RAG, Machine Learning, Deep Learning, and Generative AI. Note: Don't repeat the Same question\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "id": "9s6SkHI0p6VZ",
    "outputId": "43d7dbcc-792d-49cb-d04d-c4b318b12b44"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;33m> Current query: Write about Llama 3.1 Model, BERT and PEFT\n",
      "\u001b[0m\u001b[1;3;38;5;200m> New query: None\n",
      "\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "response_gemini = query_engine_gemini.query(\"Write about Llama 3.1 Model, BERT and PEFT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "FlgMkAhQsTIY",
    "outputId": "23617fb1-0219-412f-93dd-db90afb7b188"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Empty Response'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_gemini.response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DxOF2qth1gUC"
   },
   "source": [
    "## Test Retriever on Multistep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true,
    "id": "In9BZbU10KAz"
   },
   "outputs": [],
   "source": [
    "# import llama_index\n",
    "# from llama_index.core.indices.query.schema import QueryBundle\n",
    "\n",
    "# t = QueryBundle(\"How Retrieval Augmented Generation (RAG) work?\")\n",
    "# query_engine_gemini.retrieve(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f2xI2YPowYpd"
   },
   "source": [
    "## Subquestion Query Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a4JSpCzFwWG2",
    "outputId": "baa6649c-be2f-41ff-827f-25c322811a38"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 5 sub questions.\n",
      "\u001b[1;3;38;2;237;90;200m[LlamaIndex] Q: What are the key features and improvements of the Llama 3.1 model compared to its predecessors?\n",
      "\u001b[0m\u001b[1;3;38;2;90;149;237m[LlamaIndex] Q: How does the BERT model work and what are its main applications in natural language processing?\n",
      "\u001b[0m\u001b[1;3;38;2;11;159;203m[LlamaIndex] Q: What is PEFT (Parameter-Efficient Fine-Tuning) and how does it enhance the performance of models like BERT and Llama 3.1?\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m[LlamaIndex] Q: What are the differences in architecture between Llama 3.1 and BERT?\n",
      "\u001b[0m\u001b[1;3;38;2;237;90;200m[LlamaIndex] Q: In what scenarios is PEFT particularly beneficial for fine-tuning language models?\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m[LlamaIndex] A: The provided information does not detail the architectural differences between Llama 3.1 and BERT. It primarily focuses on the specifications, performance, and advantages of Llama 3.1, particularly its hardware requirements, benchmarking results, and open-source benefits. For a comprehensive comparison of architectures, additional resources would be needed.\n",
      "\u001b[0m\u001b[1;3;38;2;237;90;200m[LlamaIndex] A: PEFT is particularly beneficial for fine-tuning language models in scenarios where computational and storage costs are a concern. It allows for the adaptation of large pretrained models by fine-tuning only a small number of additional parameters, making it accessible for training and storing large language models on consumer hardware. This approach is advantageous in applications that require efficient resource usage while still achieving performance comparable to fully fine-tuned models. Additionally, PEFT is useful in situations where rapid integration with popular libraries like Transformers, Diffusers, and Accelerate is needed to streamline the training and inference processes.\n",
      "\u001b[0m\u001b[1;3;38;2;11;159;203m[LlamaIndex] A: PEFT (Parameter-Efficient Fine-Tuning) is a library designed to efficiently adapt large pretrained models to various downstream applications without the need to fine-tune all model parameters. It achieves this by only fine-tuning a small number of additional parameters, which significantly reduces both computational and storage costs. This approach allows for performance that is comparable to fully fine-tuned models, making it more accessible for training and storing large language models on consumer hardware.\n",
      "\n",
      "By integrating with popular libraries such as Transformers, Diffusers, and Accelerate, PEFT enhances the training and inference processes of large models. This integration facilitates a faster and easier way to load, train, and utilize models like BERT and Llama 3.1, ultimately improving their performance in various tasks while maintaining efficiency.\n",
      "\u001b[0m\u001b[1;3;38;2;237;90;200m[LlamaIndex] A: The Llama 3.1 model introduces several key features and improvements over its predecessors. Notably, it is the largest model developed by Meta, trained on over 15 trillion tokens using more than 16,000 H100 GPUs. This significant training scale enhances its capabilities.\n",
      "\n",
      "Key improvements include:\n",
      "\n",
      "1. **Context Length**: Llama 3.1 supports a 128K context length, allowing for more extensive input processing.\n",
      "\n",
      "2. **Enhanced Reasoning and Coding**: The model exhibits improved reasoning and coding capabilities, enabling it to generate high-quality code and perform complex programming tasks effectively.\n",
      "\n",
      "3. **Multilingual Processing**: With about 50% of its training data consisting of multilingual tokens, Llama 3.1 excels in understanding and processing multiple languages.\n",
      "\n",
      "4. **Tool Use and Agentic Behaviors**: The model supports zero-shot tool use and can develop agentic behaviors, enhancing its functionality in various applications.\n",
      "\n",
      "5. **Benchmark Performance**: Llama 3.1 outperforms previous models in areas such as mathematical reasoning, complex reasoning, and long text processing, achieving high scores in benchmark tests.\n",
      "\n",
      "Overall, these advancements position Llama 3.1 as a leading model in the AI landscape, surpassing its predecessors in multiple dimensions.\n",
      "\u001b[0m\u001b[1;3;38;2;90;149;237m[LlamaIndex] A: The BERT model operates using a transformer architecture that processes text bidirectionally, allowing it to capture context more effectively than traditional unidirectional models. This bidirectional processing enables BERT to consider the entire context of a sentence, which enhances its ability to predict missing words based on surrounding text. For instance, in a sentence with a blank, BERT can utilize both preceding and following words to make a more accurate prediction about the missing word.\n",
      "\n",
      "BERT is pre-trained on two main tasks: Masked Language Modeling (MLM) and Next Sentence Prediction (NSP). In MLM, a portion of the input tokens is masked, and the model learns to predict these masked tokens based on the context provided by the surrounding words. NSP involves predicting whether one sentence is likely to follow another, which helps the model understand relationships between sentences.\n",
      "\n",
      "The main applications of BERT in natural language processing include tasks such as sentiment analysis, question answering, named entity recognition, and language inference. Its ability to generate contextualized word embeddings makes it a powerful tool for various NLP applications, significantly improving performance across a range of tasks.\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
    "from llama_index.core.query_engine import SubQuestionQueryEngine\n",
    "\n",
    "query_engine = vector_index.as_query_engine()\n",
    "\n",
    "query_engine_tools = [\n",
    "    QueryEngineTool(\n",
    "        query_engine=query_engine,\n",
    "        metadata=ToolMetadata(\n",
    "            name=\"LlamaIndex\",\n",
    "            description=\"Used to answer the Questions about RAG, Machine Learning, Deep Learning, and Generative AI. Note: Don't repeat the Same question\",\n",
    "        ),\n",
    "    ),\n",
    "]\n",
    "\n",
    "sub_question_engine = SubQuestionQueryEngine.from_defaults(\n",
    "    query_engine_tools=query_engine_tools,\n",
    "    use_async=True,\n",
    ")\n",
    "\n",
    "response = sub_question_engine.query(\"Write about Llama 3.1 Model, BERT and PEFT\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "id": "IKxgCfJawjcj",
    "outputId": "aac7af72-9266-4ca8-b957-1d083813a105"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Llama 3.1 is a state-of-the-art language model developed by Meta, notable for being the largest model in its series, trained on over 15 trillion tokens with the help of more than 16,000 H100 GPUs. This extensive training enhances its capabilities significantly. Key features of Llama 3.1 include support for a 128K context length, improved reasoning and coding abilities, multilingual processing, and the capacity for zero-shot tool use and agentic behaviors. It also demonstrates superior performance in benchmark tests, particularly in mathematical reasoning and long text processing.\\n\\nBERT, on the other hand, utilizes a transformer architecture that processes text bidirectionally, allowing it to capture context more effectively than traditional models. It is pre-trained on tasks such as Masked Language Modeling and Next Sentence Prediction, which enable it to understand relationships between words and sentences. BERT is widely applied in natural language processing tasks, including sentiment analysis, question answering, and named entity recognition, thanks to its ability to generate contextualized word embeddings.\\n\\nPEFT, or Parameter-Efficient Fine-Tuning, is a library that enhances the performance of large pretrained models like BERT and Llama 3.1 by allowing for efficient adaptation to various applications. It fine-tunes only a small number of additional parameters, significantly reducing computational and storage costs while maintaining performance comparable to fully fine-tuned models. PEFT integrates seamlessly with popular libraries, facilitating faster training and inference processes, making it particularly beneficial in scenarios where resource efficiency is crucial.'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FCdPwVAQ6ixg"
   },
   "source": [
    "# HyDE Transform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "1x6He0T961Kg"
   },
   "outputs": [],
   "source": [
    "query_engine = vector_index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "0GgtfeBC6m0H"
   },
   "outputs": [],
   "source": [
    "from llama_index.core.indices.query.query_transform import HyDEQueryTransform\n",
    "from llama_index.core.query_engine.transform_query_engine import TransformQueryEngine\n",
    "\n",
    "hyde = HyDEQueryTransform(include_original=True)\n",
    "hyde_query_engine = TransformQueryEngine(query_engine, hyde)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "mm3nYnIE6mwl"
   },
   "outputs": [],
   "source": [
    "response = hyde_query_engine.query(\"Write about Llama 3.1 Model, BERT and PEFT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "id": "PjTJ2poc6mt5",
    "outputId": "2f9d1f3b-ed66-4103-8ae0-c635f284fa3b"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Llama 3.1 405B is a significant advancement in the field of AI, developed by Meta. It stands out as the largest open-source model to date, trained on over 15 trillion tokens using more than 16,000 H100 GPUs. This extensive training has enabled it to achieve a remarkable 128K context length, which enhances its ability to handle complex reasoning and long document summarization. The model has been designed to outperform proprietary models like GPT-4o and Claude 3.5 Sonnet in various benchmark tests, particularly in areas such as general knowledge, steerability, mathematical reasoning, and multilingual processing.\\n\\nIn terms of features, Llama 3.1 includes enhanced reasoning and coding capabilities, allowing it to generate high-quality code and perform well in programming tasks. It also supports multilingual processing, with about 50% of its training data consisting of multilingual tokens, enabling effective understanding and processing of multiple languages.\\n\\nBERT, or Bidirectional Encoder Representations from Transformers, is another influential model in the AI landscape, primarily used for natural language processing tasks. Unlike Llama 3.1, which is designed for a broader range of applications, BERT focuses on understanding the context of words in a sentence by looking at the words that come before and after them. This bidirectional approach allows BERT to excel in tasks such as sentiment analysis, question answering, and language inference.\\n\\nPEFT, or Parameter-Efficient Fine-Tuning, is a technique that allows models like BERT to be fine-tuned on specific tasks with fewer parameters, making the process more efficient. This method is particularly useful for adapting large models to specific applications without the need for extensive computational resources.\\n\\nIn summary, Llama 3.1 represents a leap forward in open-source AI capabilities, while BERT and PEFT contribute to the efficiency and effectiveness of natural language processing tasks. Each model has its unique strengths and applications within the AI ecosystem.'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "StgikqWZ6mrl",
    "outputId": "3358d551-20dd-4f13-fb01-bb9ac2f82653"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node ID\t 5624cdc8-2997-4e4d-82d1-c7383d389215\n",
      "Text\t 3.1 405B is Metas largest model  trained with over 15 trillion tokens. For this  Meta optimized the entire training stack and trained it on more than 16 000 H100 GPUs  making it the first Llama model trained at this scale.   According to Meta  this version of the original model (Llama 1 and Llama 2) has 128K context length  improved reasoning and coding capabilities. Meta has also upgraded both multilingual 8B and 70B models.   Key Features of Llama 3.1 40 5B:Llama 3.1 comes with a host of features and capabilities that appeal to The users  such as:   RAG & tool use  Meta states that you can use Llama system components to extend the model using zero-shot tool use and build agentic behaviors with RAG.   Multi-lingual  Llama 3 naturally supports multilingual processing. The pre-training data includes about 50% multilingual tokens and can process and understand multiple languages.   Programming and Reasoning  Llama 3 has powerful programming capabilities  generating high-quality code with a strong understanding of syntax and logic. It can create complex code structures and perform well in various programming tasks. Llama 3 excels in logical reasoning  problem-solving  analysis  and inference. It handles complex logical tasks and solves intricate problems effectively.   Multimodal Models  Multimodal models have been developed that support image recognition  video recognition  and speech understanding capabilities  but these models are still under development and have not yet been widely released.   Benchmark ResultsMeta compared the Llama 3.1 405B model with models such as GPT-4  GPT-4o  and Claude 3.5 sonnet. The results showed that Llama 3.1 performed better than GPT-4o and Claude 3.5 sonnet on test data sets such as mathematical reasoning  complex reasoning  and Multilingual support and its long text processing ability is also excellent  receiving 95.2 points in zero scrolls quality.   it falls short compared to Claude 3.5 sonnet in tool utilization ability\n",
      "Score\t 0.5475656108153438\n",
      "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
      "Node ID\t 309bb3c8-d52c-4571-b42d-7a2cde27fac4\n",
      "Text\t the AI news in the past 7 days has been insane  with so much happening in the world of AI   in this video  were diving into some of the latest AI developments from major players like Llama 3.1 405B  GPT-4o and Claude 3.5 Sonnet   Llama 3.1 405B is the first open-source model that performs on par with leading proprietary AI models in general knowledge  steerability  math  tool use  and multilingual translation  among other capabilities.   Meta announced the launch of Llama 3.1  which is the largest open-source AI model to date  and has surpassed OpenAIs GPT-4o and Anthropics Claude 3.5 Sonnet in multiple benchmark tests!   In this step-by-step guide  we will cover what Llama 3.1 405B is  how to use Llama 3.1 405B locally  and why Llama 3.1 405B is so much better than GPT-4o and Claude 3.5 Sonnet.   I highly recommend you watch this video to the end is a game changer in your chatbot that will realize the power of Llama 3.1 405B!   If you like this topic and you want to support me:   Clap my article 50 times; that will really help me out.U+1F44FFollow me on Medium and subscribe to get my latest articleU+1FAF6Follow me on my YouTube channelMore info on my discordLlama 3.1 405B is Metas largest model  trained with over 15 trillion tokens. For this  Meta optimized the entire training stack and trained it on more than 16 000 H100 GPUs  making it the first Llama model trained at this scale.   According to Meta  this version of the original model (Llama 1 and Llama 2) has 128K context length  improved reasoning and coding capabilities. Meta has also upgraded both multilingual 8B and 70B models.   Key Features of Llama 3.1 40 5B:Llama\n",
      "Score\t 0.5366529754003225\n",
      "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n"
     ]
    }
   ],
   "source": [
    "for src in response.source_nodes:\n",
    "    print(\"Node ID\\t\", src.node_id)\n",
    "    print(\"Text\\t\", src.text)\n",
    "    print(\"Score\\t\", src.score)\n",
    "    print(\"-_\" * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "17Jbo1FH6mjH"
   },
   "outputs": [],
   "source": [
    "query_bundle = hyde(\"Write about Llama 3.1 Model, BERT and PEFT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "UZEK63K77W7X"
   },
   "outputs": [],
   "source": [
    "hyde_doc = query_bundle.embedding_strs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "id": "wyzwkpSn7Yi1",
    "outputId": "61ee4b8d-54e1-4c04-ce47-7aca3f8be7b4"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"The Llama 3.1 model, developed by Meta, represents a significant advancement in the field of natural language processing (NLP). It builds upon the foundation laid by its predecessors, Llama 1 and Llama 2, by incorporating more extensive training data and improved architectural designs. Llama 3.1 is designed to enhance performance in various NLP tasks, such as text generation, summarization, and question-answering, while also being more efficient in terms of computational resources. This model is particularly notable for its ability to generate coherent and contextually relevant text, making it a valuable tool for developers and researchers in the AI community.\\n\\nIn contrast, BERT (Bidirectional Encoder Representations from Transformers), introduced by Google in 2018, revolutionized the way models understand language by employing a bidirectional approach to context. Unlike traditional models that read text sequentially, BERT processes words in relation to all the other words in a sentence simultaneously. This allows it to capture nuanced meanings and relationships, significantly improving performance on tasks such as sentiment analysis, named entity recognition, and other classification tasks. BERT's architecture is based on the transformer model, which utilizes self-attention mechanisms to weigh the importance of different words in a sentence, leading to a deeper understanding of context.\\n\\nPEFT, or Parameter-Efficient Fine-Tuning, is a technique that has gained traction in the NLP community as a means to adapt large pre-trained models like Llama 3.1 and BERT to specific tasks without the need for extensive computational resources. PEFT methods, such as adapters and prompt tuning, allow practitioners to fine-tune only a small subset of parameters in a model while keeping the majority of the pre-trained weights frozen. This approach not only reduces the computational burden but also enables faster training times and less data requirement, making it feasible to deploy powerful models in resource-constrained environments.\\n\\nIn summary, the Llama 3.1 model, BERT, and PEFT represent key innovations in the NLP landscape. Llama 3.1 enhances text generation capabilities, BERT provides a robust framework for understanding language context, and PEFT offers a practical solution for efficiently fine-tuning these models for specific applications. Together, they contribute to the ongoing evolution of AI-driven language understanding and generation.\""
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyde_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "rSHSMLQHmvh0"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "llamaindexkernel",
   "language": "python",
   "name": "llamaindexkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
