{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zE1h0uQV7uT"
      },
      "source": [
        "# Install Packages and Setup Variables\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "QPJzr-I9XQ7l",
        "outputId": "6ba01bd3-1c01-4511-9300-50d02f4dcf44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m455.6/455.6 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m602.1/602.1 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m431.4/431.4 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.9/79.9 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m266.8/266.8 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/41.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.8/65.8 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.7/55.7 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.2/196.2 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.5/118.5 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m49.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m45.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m782.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m62.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m304.6/304.6 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m67.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m453.1/453.1 kB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.3/129.3 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q llama-index==0.12.21 openai==1.59.8 tiktoken==0.7.0 chromadb==0.6.0 sentence-transformers==3.3.1 pydantic==2.10.5 llama-index-vector-stores-chroma==0.4.1 kaleido==0.2.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "riuXwpSPcvWC"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Set the following API Keys in the Python environment. Will be used later.\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"<YOUR_OPENAI_KEY>\"\n",
        "\n",
        "# from google.colab import userdata\n",
        "# os.environ[\"OPENAI_API_KEY\"] = userdata.get('openai_api_key')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "jIEeZzqLbz0J"
      },
      "outputs": [],
      "source": [
        "# Allows running asyncio in environments with an existing event loop, like Jupyter notebooks.\n",
        "\n",
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bkgi2OrYzF7q"
      },
      "source": [
        "# Load Models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "9oGT6crooSSj"
      },
      "outputs": [],
      "source": [
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "from llama_index.llms.openai import OpenAI\n",
        "from llama_index.core import Settings\n",
        "\n",
        "Settings.llm = OpenAI(temperature=1, model=\"gpt-4o-mini\")\n",
        "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWaT6rL7ksp8"
      },
      "source": [
        "# Load Indexes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "c65e37abc0ad4eba8ef0a1f4b5aafc52",
            "5de41aa48287411f837b8d8a63d80d8c",
            "597541035f35430481ed3ca89aef9f1b",
            "1469ef48d32b41fcaa4c6d07d1fead09",
            "b3b2aeda04ab4e37aba354792a2ca1db",
            "041a40bef9974a1e83f0c9004849de95",
            "53ca441ba94c415a95358be5ed3111ca",
            "5d0d5a3b0588457e8133e521b7b15748",
            "bd2342851c4148e2a029378cf1f1c42c",
            "24eb64b26a244874aaa5c0e0f8786771",
            "6dc2855fb4a04425a9a0813f3d1823b4"
          ]
        },
        "id": "60AA1iCrqApk",
        "outputId": "28595d63-850d-4f91-a04c-fa0c1ae3d045"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vectorstore.zip:   0%|          | 0.00/97.2M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c65e37abc0ad4eba8ef0a1f4b5aafc52"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Downloading Vector store from Hugging face hub\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "vectorstore = hf_hub_download(repo_id=\"jaiganesan/ai_tutor_knowledge\", filename=\"vectorstore.zip\", repo_type=\"dataset\", local_dir=\".\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SodY2Xpf_kxg",
        "outputId": "7c7bf1de-2448-4294-f570-5cd1ec5fc64d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  vectorstore.zip\n",
            "   creating: ai_tutor_knowledge/\n",
            "   creating: ai_tutor_knowledge/684af133-f877-4230-bde4-575cf53b6688/\n",
            "  inflating: ai_tutor_knowledge/684af133-f877-4230-bde4-575cf53b6688/length.bin  \n",
            "  inflating: ai_tutor_knowledge/684af133-f877-4230-bde4-575cf53b6688/index_metadata.pickle  \n",
            "  inflating: ai_tutor_knowledge/684af133-f877-4230-bde4-575cf53b6688/link_lists.bin  \n",
            "  inflating: ai_tutor_knowledge/684af133-f877-4230-bde4-575cf53b6688/header.bin  \n",
            "  inflating: ai_tutor_knowledge/684af133-f877-4230-bde4-575cf53b6688/data_level0.bin  \n",
            "  inflating: ai_tutor_knowledge/chroma.sqlite3  \n"
          ]
        }
      ],
      "source": [
        "!unzip -o vectorstore.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mXi56KTXk2sp",
        "outputId": "2b1b2e61-2894-40e8-8311-762dfba5586d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:chromadb.db.impl.sqlite:⚠️ It looks like you upgraded from a version below 0.6 and could benefit from vacuuming your database. Run chromadb utils vacuum --help for more information.\n"
          ]
        }
      ],
      "source": [
        "import chromadb\n",
        "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
        "from llama_index.core import VectorStoreIndex\n",
        "\n",
        "# Load the vector store from the local storage.\n",
        "db = chromadb.PersistentClient(path=\"./ai_tutor_knowledge\")\n",
        "chroma_collection = db.get_or_create_collection(\"ai_tutor_knowledge\")\n",
        "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
        "vector_index = VectorStoreIndex.from_vector_store(vector_store)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0m5rl195bcz"
      },
      "source": [
        "# Display result\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "4JpaHEmF5dSS"
      },
      "outputs": [],
      "source": [
        "# A simple function to show the response and the sources.\n",
        "def display_res(response):\n",
        "    print(\"Response:\\n\\t\", response.response.replace(\"\\n\", \"\"))\n",
        "\n",
        "    print(\"Sources:\")\n",
        "    if response.source_nodes:\n",
        "        for src in response.source_nodes:\n",
        "            print(\"\\tNode ID\\t\", src.node_id)\n",
        "            print(\"\\tText\\t\", src.text)\n",
        "            print(\"\\tScore\\t\", src.score)\n",
        "            print(\"\\t\" + \"-_\" * 20)\n",
        "    else:\n",
        "        print(\"\\tNo sources used!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbStjvUJ1cft"
      },
      "source": [
        "# Chat Engine\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "kwWlDpoR1cRI"
      },
      "outputs": [],
      "source": [
        "# define the chat_engine by using the index\n",
        "chat_engine = vector_index.as_chat_engine()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ER3Lb-oN46lJ",
        "outputId": "b6c6d282-4d01-45d4-bf68-35fb6e6f6c15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response:\n",
            "\t Parameter efficient fine-tuning (PEFT) works by making minimal adjustments to the weights of a pre-trained large language model (LLM) while avoiding the computational costs associated with full fine-tuning, which alters every weight in the model. This method capitalizes on the knowledge already embedded in a pretrained model and fine-tunes it using task-specific datasets.Key approaches in PEFT include:1. **Selective Fine-Tuning**: This involves choosing a subset of the model's parameters to fine-tune rather than the entire model, which reduces computational requirements.2. **Reparameterization**: This method adjusts model weights using low-rank representations to minimize the number of parameters that need to be trained, efficiently representing weight changes without overwhelming the model.3. **Additive Method**: This approach involves adding new parameters or structures to the model that facilitate fine-tuning without extensive alterations to the existing weights.Overall, PEFT aims to optimize the fine-tuning process by reducing complexity and computational load while enabling effective adaptation to specific tasks.\n",
            "Sources:\n",
            "\tNode ID\t 6be88fa3-2f8b-43e7-aba0-d874b39809fc\n",
            "\tText\t # FourierFT: Discrete Fourier Transformation Fine-Tuning[FourierFT](https://huggingface.co/papers/2405.03003) is a parameter-efficient fine-tuning technique that leverages Discrete Fourier Transform to compress the model's tunable weights. This method outperforms LoRA in the GLUE benchmark and common ViT classification tasks using much less parameters.FourierFT currently has the following constraints:- Only `nn.Linear` layers are supported.- Quantized layers are not supported.If these constraints don't work for your use case, consider other methods instead.The abstract from the paper is:> Low-rank adaptation (LoRA) has recently gained much interest in fine-tuning foundation models. It effectively reduces the number of trainable parameters by incorporating low-rank matrices A and B to represent the weight change, i.e., Delta W=BA. Despite LoRA's progress, it faces storage challenges when handling extensive customization adaptations or larger base models. In this work, we aim to further compress trainable parameters by enjoying the powerful expressiveness of the Fourier transform. Specifically, we introduce FourierFT, which treats Delta W as a matrix in the spatial domain and learns only a small fraction of its spectral coefficients. With the trained spectral coefficients, we implement the inverse discrete Fourier transform to recover Delta W. Empirically, our FourierFT method shows comparable or better performance with fewer parameters than LoRA on various tasks, including natural language understanding, natural language generation, instruction tuning, and image classification. For example, when performing instruction tuning on the LLaMA2-7B model, FourierFT surpasses LoRA with only 0.064M trainable parameters, compared to LoRA's 33.5M.## FourierFTConfig[[autodoc]] tuners.fourierft.config.FourierFTConfig## FourierFTModel[[autodoc]] tuners.fourierft.model.FourierFTModel\n",
            "\tScore\t 0.42088463893369404\n",
            "\t-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "\tNode ID\t 2b4590b8-4c9d-4d51-a81f-9a13979593a8\n",
            "\tText\t SAMSUM is one of the datasets that FLAN T5 uses. There are several pre-trained FLAN T5 models that have been fine-tuned on SAMSUM  including Phil Schmid/flan-t5-base-samsum and jasonmcaffee/flan-t5-large-samsum on Hugging Face. If we want to fine-tune the FLAN T5 model specifically for formal dialogue conversations  we can do so using the DIALOGUESUM dataset.   Models fine-tuned on DialogSum can be applied to areas like customer support  meeting minutes generation  chatbot summarization  and more.   2. PEFT (Parameter efficient fine tuning)Training LLMs is computationally intensive. Full finetuning is computationally expensive as it might change each weight in the model. First  we start with a pretrained LLM like GPT-3. This model already has a vast amount of knowledge and understanding of language. Then we provide task-specific datasets  which could be data for question answering or sentiment analysis or any other customer dataset. During training  full finetuning process makes slight adjustments to every weight in the pretrained model. While the model weights are substantial  we have other important aspects during training like Optimizer  which adds up to the cost. For example  Optimizer States  gradients  forward activation  and temporary memory. These additional components add up to the training cost.   Three main approaches are used in PEFT: Selective / reparameterization/additive.   1. SelectiveHere  we select a subset of initial LLM parameters to fine-tune.   2. ReparameterizationWe reparameterize model weights using a low-rank representation. We will discuss LoRA in detail below.   LORA: Low Rank Representation:   Each layer in a transformer architecture has multiple weight matrices for different operations  like self-attention or feed-forward networks. These matrices can have different sizes depending on the specific layer and configuration. Let us take an example by picking a matrix of size 512 x 64 = 32 768 parameters. Let us now see LoRA with rank = 8.   Original Weight Matrix: Dimensions: 512 x 64  Parameters: 32 768 (512 x 64)Matrix A (Rank Decomposition):\n",
            "\tScore\t 0.3833038456089033\n",
            "\t-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n"
          ]
        }
      ],
      "source": [
        "# First Question:\n",
        "response = chat_engine.chat(\"Use the tool to answer, how does parameter efficient finetuning work?\")\n",
        "\n",
        "display_res(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3RRmiJEQ5R1Q",
        "outputId": "b5e2fa94-b213-436c-cd48-46cfe95d66c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response:\n",
            "\t Sure! Here’s a joke for you:Why did the scarecrow win an award?Because he was outstanding in his field!\n",
            "Sources:\n",
            "\tNo sources used!\n"
          ]
        }
      ],
      "source": [
        "# Second Question:\n",
        "response = chat_engine.chat(\"Could you tell me a joke?\")\n",
        "display_res(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8eOzp5Xc5Vbj",
        "outputId": "fd41d662-5dfc-443d-884c-904dbe320dde"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response:\n",
            "\t The first question you asked was about how parameter efficient fine-tuning works.\n",
            "Sources:\n",
            "\tNo sources used!\n"
          ]
        }
      ],
      "source": [
        "# Third Question: (check if it can recall previous interactions)\n",
        "response = chat_engine.chat(\"What was the first question I asked?\")\n",
        "display_res(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "7jfiLpru5VZT"
      },
      "outputs": [],
      "source": [
        "# Reset the session to clear the memory\n",
        "chat_engine.reset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jt0q8RW25VXN",
        "outputId": "adce109f-313f-4817-8bd1-42ed8f8800ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response:\n",
            "\t Your first question was, \"What was the first question I asked?\"\n",
            "Sources:\n",
            "\tNo sources used!\n"
          ]
        }
      ],
      "source": [
        "# Fourth Question: (don't recall the previous interactions.)\n",
        "response = chat_engine.chat(\"What was the first question I asked?\")\n",
        "display_res(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Egsib7yPJGR"
      },
      "source": [
        "# Streaming\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zanJeMbaPJcq",
        "outputId": "492d9770-6d51-485a-d422-46125b66997b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieval-Augmented Generation (RAG) and Parameter-Efficient Fine-Tuning (PEFT) are both methodologies in the realm of natural language processing, but they serve different purposes and employ distinct approaches. RAG combines a retrieval system with a generative model to improve output quality on knowledge-intensive tasks. It features two main components: the retrieval aspect, which extracts relevant information from external databases (like indexed documents), and the generation aspect, which produces coherent responses using the retrieved content. This dual mechanism allows RAG to leverage both parametric memory from the generative model and non-parametric memory from the retrieval system, addressing the limitations of traditional models by providing more accurate and up-to-date information. In contrast, PEFT focuses on the efficient adaptation of large pre-trained models to specific tasks by only fine-tuning a small number of additional parameters, rather than adjusting all model parameters. This method reduces computational costs and storage requirements, making it feasible to work with large models on consumer hardware. In summary, while RAG excels at integrating information retrieval with generative capabilities for improved performance, PEFT aims to make the fine-tuning process more efficient, thereby enabling broader access to advanced language models."
          ]
        }
      ],
      "source": [
        "# Stream the words as soon as they are available instead of waiting for the model to finish generation.\n",
        "streaming_response = chat_engine.stream_chat(\n",
        "    \"Write a paragraph explaining how RAG and PEFT work, and highlight the differences between them.\"\n",
        ")\n",
        "streaming_response.print_response_stream()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DuRgOJ2AHMJh"
      },
      "source": [
        "## Condense Question\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yb2Lt41jq145"
      },
      "source": [
        "Enhance the input prompt by looking at the previous chat history along with the present question. The refined prompt can then be used to fetch the nodes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "v0gmM5LGIaRl"
      },
      "outputs": [],
      "source": [
        "# Define GPT-4 model that will be used by the chat_engine to improve the query.\n",
        "gpt4 = OpenAI(temperature=0.9, model=\"gpt-4o\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "EDWsaBTBIhK7"
      },
      "outputs": [],
      "source": [
        "chat_engine = vector_index.as_chat_engine(\n",
        "    chat_mode=\"condense_question\", llm=gpt4, verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h4c--hJ75VU2",
        "outputId": "237c62c0-da84-4055-bcff-dcb9a5fa70c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Querying with: How does Retrieval-Augmented Generation (RAG) work, and which problem does it solve?\n",
            "Response:\n",
            "\t Retrieval-Augmented Generation (RAG) works by integrating pretraining and retrieval-based models to enhance the performance of large language models (LLMs). It involves key steps such as query classification, retrieval of relevant documents, reranking to refine these documents' order, repacking to organize them for better generation, and summarization to extract key information. This process enables RAG to address challenges faced by LLMs, such as producing outdated information and fabricating facts, thus improving the accuracy and reliability of generated content. Additionally, RAG allows for the rapid deployment of applications without needing to update model parameters, provided relevant documents are available.\n",
            "Sources:\n",
            "\tNode ID\t 2aa05360-f43a-4819-bce7-0acf7b897eab\n",
            "\tText\t Generative large language models are prone to producing outdated information or fabricating facts, although they were aligned with human preferences by reinforcement learning [1] or lightweight alternatives [2–5]. Retrieval-augmented generation (RAG) techniques address these issues by combining the strengths of pretraining and retrieval-based models, thereby providing a robust framework for enhancing model performance [6]. Furthermore, RAG enables rapid deployment of applications for specific organizations and domains without necessitating updates to the model parameters, as long as query-related documents are provided. Many RAG approaches have been proposed to enhance large language models (LLMs) through query-dependent retrievals [6–8]. A typical RAG workflow usually contains multiple intervening processing steps: query classification (determining whether retrieval is necessary for a given input query), retrieval (efficiently obtaining relevant documents for the query), reranking (refining the order of retrieved documents based on their relevance to the query), repacking (organizing the retrieved documents into a structured one for better generation), summarization (extracting key information for response generation from the repacked document and eliminating redundancies) and models. Implementing RAG also requires decisions on the ways to properly split documents into chunks, the types of embeddings to use for semantically representing these chunks, the choice of\n",
            "\tScore\t 0.5942003378877615\n",
            "\t-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "\tNode ID\t 1c324686-fad7-4a41-bfd3-d44f9612ca91\n",
            "\tText\t Authors: Hao Yu, Aoran Gan, Kai Zhang, Shiwei Tong, Qi Liu, and Zhaofeng Liu.Numerous studies of Retrieval-Augmented Generation (RAG) systems have emerged from various perspectives since the advent of Large Language Models (LLMs). The RAG system comprises two primary components: Retrieval and Generation. The retrieval component aims to extract relevant information from various external knowledge sources. It involves two main phases, indexing and searching. Indexing organizes documents to facilitate efficient retrieval, using either inverted indexes for sparse retrieval or dense vector encoding for dense retrieval. The searching component utilizes these indexes to fetch relevant documents based on the user's query, often incorporating the optional rerankers to refine the ranking of the retrieved documents. The generation component utilizes the retrieved content and question query to formulate coherent and contextually relevant responses with the prompting and inferencing phases. As the \"Emerging\" ability of LLMs and the breakthrough in aligning human commands, LLMs are the best performance choices model for the generation stage. Prompting methods like Chain of Thought, Tree of Thought, Rephrase and Respond guide better generation results. In the inferencing step, LLMs interpret the prompted input to generate accurate and in-depth responses that align with the query's intent and integrate the extracted information without further finetuning, such as fully finetuning or LoRA. Appendix A details the complete RAG structure. Figure 1 illustrates the structure of the RAG systems as mentioned.\n",
            "\tScore\t 0.5742996423371074\n",
            "\t-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n"
          ]
        }
      ],
      "source": [
        "response = chat_engine.chat(\n",
        "    \"How does Retrieval-Augmented Generation (RAG) work, and which problem does it solve?\"\n",
        ")\n",
        "display_res(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysL9ONePOsGB"
      },
      "source": [
        "## ReAct\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KiEFmxAtrmF-"
      },
      "source": [
        "ReAct is an agent-based chat mode that uses a loop to decide on querying a data engine during interactions, offering flexibility but relying on the Large Language Model's quality for effective responses, requiring careful management to avoid inaccurate answers.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "-M1jWoKXOs2t"
      },
      "outputs": [],
      "source": [
        "chat_engine = vector_index.as_chat_engine(chat_mode=\"react\", verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UZkEW1SSOs0H",
        "outputId": "2e73b01d-1cab-4e6f-ca14-80477f685671"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Added user message to memory: Which company developed Claude 3.5 Sonnet, and what is its primary application?\n",
            "=== Calling Function ===\n",
            "Calling function: query_engine_tool with args: {\"input\":\"Which company developed Claude 3.5 Sonnet, and what is its primary application?\"}\n",
            "Got output: The information provided does not specify which company developed Claude 3.5 Sonnet or its primary application.\n",
            "========================\n",
            "\n",
            "=== Calling Function ===\n",
            "Calling function: query_engine_tool with args: {\"input\":\"Claude 3.5 Sonnet developer and primary application\"}\n",
            "Got output: The information provided does not cover details about Claude 3.5, its development, or its primary application. Please provide more specific context or related information for a focused response.\n",
            "========================\n",
            "\n",
            "=== Calling Function ===\n",
            "Calling function: query_engine_tool with args: {\"input\":\"Claude AI model details including Claude 3.5 Sonnet, its developer and application.\"}\n",
            "Got output: Claude AI, developed by Anthropic, is a generative AI model that emphasizes safety, explainability, and performance. Within the Claude AI family, the Sonnet model serves as a free-tier option that balances cost and features, making it suitable for tasks like creative writing and answering questions. Claude AI is designed to process large amounts of text, with Sonnet offering functionality appropriate for users who need reliable assistance without a premium subscription. The broader Claude family includes other models as well, such as Haiku, which is budget-friendly, and Opus, which requires a premium subscription and is geared towards more complex tasks.\n",
            "========================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "response = chat_engine.chat(\n",
        "    \"Which company developed Claude 3.5 Sonnet, and what is its primary application?\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eW5P1lD4Osxf",
        "outputId": "2d59631b-2856-4ece-e67e-4f34dbaa3c75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response:\n",
            "\t Claude 3.5 Sonnet was developed by Anthropic. Its primary application is in generative AI tasks, particularly for creative writing and answering questions, providing users with reliable assistance in a free-tier option that balances cost and features.\n",
            "Sources:\n",
            "\tNode ID\t 55740ef4-3809-4dfa-ad06-e85bac4e165f\n",
            "\tText\t seeing. Most visual perception is handled by low-level processes that merely tell your brain \"that\\'s a water droplet\" without telling you details like where the lightest and darkest points are, or \"that\\'s a bush\" without telling you the shape and position of every leaf. This is a feature of brains, not a bug. In everyday life it would be distracting to notice every leaf on every bush. But when you have to paint something, you have to look more closely, and when you do there\\'s a lot to see. You can still be noticing new things after days of trying to paint something people usually take for granted, just as you can after days of trying to write an essay about something people usually take for granted.\\n\\nThis is not the only way to paint. I\\'m not 100% sure it\\'s even a good way to paint. But it seemed a good enough bet to be worth trying.\\n\\nOur teacher, professor Ulivi, was a nice guy. He could see I worked hard, and gave me a good grade, which he wrote down in a sort of passport each student had. But the Accademia wasn\\'t teaching me anything except Italian, and my money was running out, so at the end of the first year I went back to the US.\\n\\nI wanted to go back to RISD, but I was now broke and RISD was very expensive, so I decided to get a job for a year and then return to RISD the next fall. I got one at a company called Interleaf, which made software for creating documents. You mean like Microsoft Word? Exactly. That was how I learned that low end software tends to eat high end software. But Interleaf still had a few years to live yet. [5]\\n\\nInterleaf had done something pretty bold. Inspired by Emacs, they\\'d added a scripting language, and even made the scripting language a dialect of Lisp. Now they wanted a Lisp hacker to write things in it. This was the closest thing I\\'ve had to a normal job, and I hereby apologize to my boss and coworkers, because I was a bad employee. Their Lisp was the thinnest icing on a giant C cake, and since I didn\\'t know C and didn\\'t want to learn it, I never understood most of the software.\n",
            "\tScore\t 0.2510596247298863\n",
            "\t-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "\tNode ID\t 03b1c975-f71f-4300-bf73-bbb602b8ee69\n",
            "\tText\t and had \"hardly any plot.\"2. Programming on an IBM 1401 computer in 9th grade, using an early version of Fortran language.3. Building simple games, a program to predict the height of model rockets, and a word processor for his father.4. Reading science fiction novels, such as \"The Moon is a Harsh Mistress\" by Heinlein, which inspired him to work on AI.5. Living in Florence, Italy, and walking through the city's streets to the Accademia.Please note that these activities are mentioned in the text and are not based on prior knowledge or assumptions.</b>### Streaming Support```pythonquery_engine = index.as_query_engine(streaming=True)response = query_engine.query(\"What happened at interleaf?\")for token in response.response_gen:    print(token, end=\"\")```     Based on the context information provided, it appears that the author worked at Interleaf, a company that made software for creating and managing documents. The author mentions that Interleaf was \"on the way down\" and that the company's Release Engineering group was large compared to the group that actually wrote the software. It is inferred that Interleaf was experiencing financial difficulties and that the author was nervous about money. However, there is no explicit mention of what specifically happened at Interleaf.\n",
            "\tScore\t 0.24688669688641435\n",
            "\t-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "\tNode ID\t f4c4e058-cea6-4c20-b8fb-956ff06c137b\n",
            "\tText\t still had a few years to live yet. [5]\\n\\nInterleaf had done something pretty bold. Inspired by Emacs, they\\'d added a scripting language, and even made the scripting language a dialect of Lisp. Now they wanted a Lisp hacker to write things in it. This was the closest thing I\\'ve had to a normal job, and I hereby apologize to my boss and coworkers, because I was a bad employee. Their Lisp was the thinnest icing on a giant C cake, and since I didn\\'t know C and didn\\'t want to learn it, I never understood most of the software. Plus I was terribly irresponsible. This was back when a programming job meant showing up every day during certain working hours.', start_char_idx=14179, end_char_idx=18443, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=1.0986518859863281)]\n",
            "\tScore\t 0.2521247635272654\n",
            "\t-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "\tNode ID\t 55740ef4-3809-4dfa-ad06-e85bac4e165f\n",
            "\tText\t seeing. Most visual perception is handled by low-level processes that merely tell your brain \"that\\'s a water droplet\" without telling you details like where the lightest and darkest points are, or \"that\\'s a bush\" without telling you the shape and position of every leaf. This is a feature of brains, not a bug. In everyday life it would be distracting to notice every leaf on every bush. But when you have to paint something, you have to look more closely, and when you do there\\'s a lot to see. You can still be noticing new things after days of trying to paint something people usually take for granted, just as you can after days of trying to write an essay about something people usually take for granted.\\n\\nThis is not the only way to paint. I\\'m not 100% sure it\\'s even a good way to paint. But it seemed a good enough bet to be worth trying.\\n\\nOur teacher, professor Ulivi, was a nice guy. He could see I worked hard, and gave me a good grade, which he wrote down in a sort of passport each student had. But the Accademia wasn\\'t teaching me anything except Italian, and my money was running out, so at the end of the first year I went back to the US.\\n\\nI wanted to go back to RISD, but I was now broke and RISD was very expensive, so I decided to get a job for a year and then return to RISD the next fall. I got one at a company called Interleaf, which made software for creating documents. You mean like Microsoft Word? Exactly. That was how I learned that low end software tends to eat high end software. But Interleaf still had a few years to live yet. [5]\\n\\nInterleaf had done something pretty bold. Inspired by Emacs, they\\'d added a scripting language, and even made the scripting language a dialect of Lisp. Now they wanted a Lisp hacker to write things in it. This was the closest thing I\\'ve had to a normal job, and I hereby apologize to my boss and coworkers, because I was a bad employee. Their Lisp was the thinnest icing on a giant C cake, and since I didn\\'t know C and didn\\'t want to learn it, I never understood most of the software.\n",
            "\tScore\t 0.25009613001459613\n",
            "\t-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "\tNode ID\t 4558b7d3-7f77-4f55-b0e7-64d385820117\n",
            "\tText\t 2. Dedicated to safety and security   It is a well-known fact that Anthropic prioritizes responsible AI development the most  and it is clearly seen in Claudes design. This generative AI model is trained on a carefully curated dataset thus it minimizes biases and factual errors to a large extent. On top of that  Claude also undergoes rigorous safety checks to prevent the generation of harmful and misleading content.   3. Emphasizes Explainability   While many of the AI and LLMs currently operate as black boxes  Claude offers a high level of explainability surpassing other models. This means it can explain the reasoning and decision-making process behind all of its responses. Therefore  it helps users to use this model confidently and they can be assured about the credibility of the information provided.   Claude FamilyClaude AI comes in a family of 3 generative AI models. Users can choose from these three models based on their power requirements and budget.   1. Haiku: It is the most budget-friendly option and offers fast response times. This can be perfect for simple tasks that require short context. This is yet to be launched but users can expect it to be highly cost-effective and cheaper as compared to other models.   2. Sonnet: This is a free-tier model and serves as an excellent starting point by offering a balance between cost and features. It can effectively handle tasks like writing different creative text formats and answering questions  just like Open AIs ChatGPT.   3. Opus: This is the most powerful generative AI model by Claude AI; however  users require a premium subscription to use this AI Chatbot. It can perform complex tasks easily that require a large context window. So  if you are looking for a generative AI that can do research  summarize lengthy documents  or help with consistent lengthy conversations  then this model will be the best option.   ChatGPT vs. Claude AI: How do they differ?Claude AI and OpenAIs ChatGPT both are very powerful LLM models. But they are designed for various purposes. Lets compare.   Strengths:    Claude: It is great in performing tasks requiring long-term context as discussed above. They can maintain consistency throughout their response in extended conversations. Also  their explainability\n",
            "\tScore\t 0.46193427112940716\n",
            "\t-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "\tNode ID\t 0e97549b-bf5a-4e4f-9f5a-d6a478126874\n",
            "\tText\t Claude AI and ChatGPT are both powerful and popular generative AI models revolutionizing various aspects of our lives. Here  let us learn more about Claude AI and its benefits   Ever since the launch of ChatGPT  many other companies have also joined the race to bring excellent generative AI models into the world that not only help users create realistic content but are also safe to use  and free from bias. While Open AIs ChatGPT and Googles Bard  now Gemini  get most of the limelight  Claude AI stands out for its impressive features and being the most reliable and ethical Large Language Model.   In this article  we will learn more about what Claude AI is and what are its unique features. We will also discuss how it differs from the most popular generative AI tool ChatGPT.   Claude AI is developed by Anthropic  an AI startup company backed by Google and Amazon  and is dedicated to developing safe and beneficial AI. Claude AI is an LLM based on the powerful transformer architecture and like OpenAIs ChatGPT  it can generate text  translate languages  as well as write different kinds of compelling content. It can interact with users like a normal AI chatbot; however  it also boasts some unique features that make it different from others.   1. Larger Context Window   One of the Claude AIs biggest capabilities is that it can process huge chunks of text as compared to ChatGPT. While ChatGPT struggles to process and keep track of information in long conversations  Claudes context window is huge (spanning up to 150 pages)  which helps users to do more coherent and consistent conversations  especially when it comes to long documents.   2. Dedicated to safety and security   It is a well-known fact that Anthropic prioritizes responsible AI development the most  and it is clearly seen in Claudes design. This generative AI model is trained on a carefully curated dataset thus it minimizes biases and factual errors to a large extent. On top of that  Claude also undergoes rigorous safety checks to prevent the generation of harmful and misleading content.   3. Emphasizes Explainability   While many of the AI and LLMs currently operate as black boxes  Claude offers a high level of explainability surpassing other models. This means it\n",
            "\tScore\t 0.4306906716725645\n",
            "\t-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n"
          ]
        }
      ],
      "source": [
        "display_res(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zf6r2AmFOsca"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "llamaindexkernel",
      "language": "python",
      "name": "llamaindexkernel"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c65e37abc0ad4eba8ef0a1f4b5aafc52": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5de41aa48287411f837b8d8a63d80d8c",
              "IPY_MODEL_597541035f35430481ed3ca89aef9f1b",
              "IPY_MODEL_1469ef48d32b41fcaa4c6d07d1fead09"
            ],
            "layout": "IPY_MODEL_b3b2aeda04ab4e37aba354792a2ca1db"
          }
        },
        "5de41aa48287411f837b8d8a63d80d8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_041a40bef9974a1e83f0c9004849de95",
            "placeholder": "​",
            "style": "IPY_MODEL_53ca441ba94c415a95358be5ed3111ca",
            "value": "vectorstore.zip: 100%"
          }
        },
        "597541035f35430481ed3ca89aef9f1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5d0d5a3b0588457e8133e521b7b15748",
            "max": 97198458,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bd2342851c4148e2a029378cf1f1c42c",
            "value": 97198458
          }
        },
        "1469ef48d32b41fcaa4c6d07d1fead09": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_24eb64b26a244874aaa5c0e0f8786771",
            "placeholder": "​",
            "style": "IPY_MODEL_6dc2855fb4a04425a9a0813f3d1823b4",
            "value": " 97.2M/97.2M [00:00&lt;00:00, 239MB/s]"
          }
        },
        "b3b2aeda04ab4e37aba354792a2ca1db": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "041a40bef9974a1e83f0c9004849de95": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "53ca441ba94c415a95358be5ed3111ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5d0d5a3b0588457e8133e521b7b15748": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bd2342851c4148e2a029378cf1f1c42c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "24eb64b26a244874aaa5c0e0f8786771": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6dc2855fb4a04425a9a0813f3d1823b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}