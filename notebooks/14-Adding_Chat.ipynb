{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-zE1h0uQV7uT"
   },
   "source": [
    "# Install Packages and Setup Variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "QPJzr-I9XQ7l",
    "outputId": "839fff95-4a0b-49ed-d7c4-274a136a1525"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m455.6/455.6 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m602.1/602.1 kB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m431.4/431.4 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.9/79.9 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m68.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m65.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m61.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m87.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.4/183.4 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.2/65.2 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.9/118.9 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.6/79.6 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m60.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m67.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.3/251.3 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.3/302.3 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m73.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m452.6/452.6 kB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "!pip install -q llama-index==0.12.21 openai==1.59.8 tiktoken==0.7.0 chromadb==0.6.0 sentence-transformers==3.3.1 pydantic==2.10.5 llama-index-vector-stores-chroma==0.4.1 kaleido==0.2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "riuXwpSPcvWC"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set the following API Keys in the Python environment. Will be used later.\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"<YOUR_OPENAI_KEY>\"\n",
    "\n",
    "# from google.colab import userdata\n",
    "# os.environ[\"OPENAI_API_KEY\"] = userdata.get('openai_api_key')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "jIEeZzqLbz0J"
   },
   "outputs": [],
   "source": [
    "# Allows running asyncio in environments with an existing event loop, like Jupyter notebooks.\n",
    "\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bkgi2OrYzF7q"
   },
   "source": [
    "# Load Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "9oGT6crooSSj"
   },
   "outputs": [],
   "source": [
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core import Settings\n",
    "\n",
    "Settings.llm = OpenAI(temperature=1, model=\"gpt-4o-mini\")\n",
    "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OWaT6rL7ksp8"
   },
   "source": [
    "# Load Indexes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "6d535e93468f42e0b4457c489ab47b01",
      "6195e847572044d69e1d18d86c94952b",
      "26ecd9baabe44b61beba36135f254caa",
      "cdeba624476448b680845e64cd9a2094",
      "fba61bd1343c41f4b79523c03c06d9c6",
      "6442b80972904afcbcb11d28c62412ba",
      "a07f744bcad34274b308addcdf72665b",
      "1d1947446c564041b89b67549f34313b",
      "d8c426f7339e4bd58bbdd63c3418f6a5",
      "eed23fb0b6ea4cb191d2fda2e73ad129",
      "50ac80f1552f46fa98a911e0475666b1"
     ]
    },
    "id": "60AA1iCrqApk",
    "outputId": "0b8c3e62-c0ce-413b-c889-0c202e81dd29"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d535e93468f42e0b4457c489ab47b01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vectorstore.zip:   0%|          | 0.00/97.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Downloading Vector store from Hugging face hub\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "vectorstore = hf_hub_download(repo_id=\"jaiganesan/ai_tutor_knowledge\", filename=\"vectorstore.zip\", repo_type=\"dataset\", local_dir=\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SodY2Xpf_kxg",
    "outputId": "b9037c03-c9bd-4f35-8519-71a4112032b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  vectorstore.zip\n",
      "   creating: ai_tutor_knowledge/\n",
      "   creating: ai_tutor_knowledge/684af133-f877-4230-bde4-575cf53b6688/\n",
      "  inflating: ai_tutor_knowledge/684af133-f877-4230-bde4-575cf53b6688/length.bin  \n",
      "  inflating: ai_tutor_knowledge/684af133-f877-4230-bde4-575cf53b6688/index_metadata.pickle  \n",
      "  inflating: ai_tutor_knowledge/684af133-f877-4230-bde4-575cf53b6688/link_lists.bin  \n",
      "  inflating: ai_tutor_knowledge/684af133-f877-4230-bde4-575cf53b6688/header.bin  \n",
      "  inflating: ai_tutor_knowledge/684af133-f877-4230-bde4-575cf53b6688/data_level0.bin  \n",
      "  inflating: ai_tutor_knowledge/chroma.sqlite3  \n"
     ]
    }
   ],
   "source": [
    "!unzip -o vectorstore.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mXi56KTXk2sp",
    "outputId": "2a5bd4ab-d0ec-4a34-f4a8-5385e7935a31"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:chromadb.db.impl.sqlite:⚠️ It looks like you upgraded from a version below 0.6 and could benefit from vacuuming your database. Run chromadb utils vacuum --help for more information.\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "# Load the vector store from the local storage.\n",
    "db = chromadb.PersistentClient(path=\"./ai_tutor_knowledge\")\n",
    "chroma_collection = db.get_or_create_collection(\"ai_tutor_knowledge\")\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "vector_index = VectorStoreIndex.from_vector_store(vector_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q0m5rl195bcz"
   },
   "source": [
    "# Display result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "4JpaHEmF5dSS"
   },
   "outputs": [],
   "source": [
    "# A simple function to show the response and the sources.\n",
    "def display_res(response):\n",
    "    print(\"Response:\\n\\t\", response.response.replace(\"\\n\", \"\"))\n",
    "\n",
    "    print(\"Sources:\")\n",
    "    if response.source_nodes:\n",
    "        for src in response.source_nodes:\n",
    "            print(\"\\tNode ID\\t\", src.node_id)\n",
    "            print(\"\\tText\\t\", src.text)\n",
    "            print(\"\\tScore\\t\", src.score)\n",
    "            print(\"\\t\" + \"-_\" * 20)\n",
    "    else:\n",
    "        print(\"\\tNo sources used!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hbStjvUJ1cft"
   },
   "source": [
    "# Chat Engine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "kwWlDpoR1cRI"
   },
   "outputs": [],
   "source": [
    "# define the chat_engine by using the index\n",
    "chat_engine = vector_index.as_chat_engine(llm=Settings.llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ER3Lb-oN46lJ",
    "outputId": "3e1673b7-a4b9-4851-da96-54f817d1182c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "\t Parameter Efficient Fine Tuning (PEFT) works by making adjustments to a subset of parameters in large language models (LLMs) to optimize their performance without requiring full fine-tuning of all model weights. This approach significantly reduces computational costs and resource demands.Here are three main strategies utilized in PEFT:1. **Selective Fine-Tuning**: Only a selected subset of parameters in the LLM is fine-tuned, allowing for targeted adjustments while leaving the majority of the model unchanged.2. **Reparameterization**: This involves using low-rank representations to modify model weights. Techniques like Low Rank Adaptation (LoRA) decompose weight matrices to maintain performance with fewer parameters.3. **Additive Method**: While not extensively detailed, this approach refers to adding new parameters or mechanisms to enhance the model's capabilities without significantly altering existing weights.By employing these methods, PEFT enables effective fine-tuning of models while conserving computational resources, making it a practical choice for many applications.\n",
      "Sources:\n",
      "\tNode ID\t 6be88fa3-2f8b-43e7-aba0-d874b39809fc\n",
      "\tText\t # FourierFT: Discrete Fourier Transformation Fine-Tuning[FourierFT](https://huggingface.co/papers/2405.03003) is a parameter-efficient fine-tuning technique that leverages Discrete Fourier Transform to compress the model's tunable weights. This method outperforms LoRA in the GLUE benchmark and common ViT classification tasks using much less parameters.FourierFT currently has the following constraints:- Only `nn.Linear` layers are supported.- Quantized layers are not supported.If these constraints don't work for your use case, consider other methods instead.The abstract from the paper is:> Low-rank adaptation (LoRA) has recently gained much interest in fine-tuning foundation models. It effectively reduces the number of trainable parameters by incorporating low-rank matrices A and B to represent the weight change, i.e., Delta W=BA. Despite LoRA's progress, it faces storage challenges when handling extensive customization adaptations or larger base models. In this work, we aim to further compress trainable parameters by enjoying the powerful expressiveness of the Fourier transform. Specifically, we introduce FourierFT, which treats Delta W as a matrix in the spatial domain and learns only a small fraction of its spectral coefficients. With the trained spectral coefficients, we implement the inverse discrete Fourier transform to recover Delta W. Empirically, our FourierFT method shows comparable or better performance with fewer parameters than LoRA on various tasks, including natural language understanding, natural language generation, instruction tuning, and image classification. For example, when performing instruction tuning on the LLaMA2-7B model, FourierFT surpasses LoRA with only 0.064M trainable parameters, compared to LoRA's 33.5M.## FourierFTConfig[[autodoc]] tuners.fourierft.config.FourierFTConfig## FourierFTModel[[autodoc]] tuners.fourierft.model.FourierFTModel\n",
      "\tScore\t 0.4208613591390603\n",
      "\t-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
      "\tNode ID\t 2b4590b8-4c9d-4d51-a81f-9a13979593a8\n",
      "\tText\t SAMSUM is one of the datasets that FLAN T5 uses. There are several pre-trained FLAN T5 models that have been fine-tuned on SAMSUM  including Phil Schmid/flan-t5-base-samsum and jasonmcaffee/flan-t5-large-samsum on Hugging Face. If we want to fine-tune the FLAN T5 model specifically for formal dialogue conversations  we can do so using the DIALOGUESUM dataset.   Models fine-tuned on DialogSum can be applied to areas like customer support  meeting minutes generation  chatbot summarization  and more.   2. PEFT (Parameter efficient fine tuning)Training LLMs is computationally intensive. Full finetuning is computationally expensive as it might change each weight in the model. First  we start with a pretrained LLM like GPT-3. This model already has a vast amount of knowledge and understanding of language. Then we provide task-specific datasets  which could be data for question answering or sentiment analysis or any other customer dataset. During training  full finetuning process makes slight adjustments to every weight in the pretrained model. While the model weights are substantial  we have other important aspects during training like Optimizer  which adds up to the cost. For example  Optimizer States  gradients  forward activation  and temporary memory. These additional components add up to the training cost.   Three main approaches are used in PEFT: Selective / reparameterization/additive.   1. SelectiveHere  we select a subset of initial LLM parameters to fine-tune.   2. ReparameterizationWe reparameterize model weights using a low-rank representation. We will discuss LoRA in detail below.   LORA: Low Rank Representation:   Each layer in a transformer architecture has multiple weight matrices for different operations  like self-attention or feed-forward networks. These matrices can have different sizes depending on the specific layer and configuration. Let us take an example by picking a matrix of size 512 x 64 = 32 768 parameters. Let us now see LoRA with rank = 8.   Original Weight Matrix: Dimensions: 512 x 64  Parameters: 32 768 (512 x 64)Matrix A (Rank Decomposition):\n",
      "\tScore\t 0.3832778241128017\n",
      "\t-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n"
     ]
    }
   ],
   "source": [
    "# First Question:\n",
    "response = chat_engine.chat(\"Use the tool to answer, how does parameter efficient finetuning work?\")\n",
    "\n",
    "display_res(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3RRmiJEQ5R1Q",
    "outputId": "1d2990c6-fc1a-4c3e-edb6-7b4ec85818aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "\t Sure! Here's a joke for you:Why don't scientists trust atoms?Because they make up everything!\n",
      "Sources:\n",
      "\tNo sources used!\n"
     ]
    }
   ],
   "source": [
    "# Second Question:\n",
    "response = chat_engine.chat(\"Could you tell me a joke?\")\n",
    "display_res(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8eOzp5Xc5Vbj",
    "outputId": "d8a94677-1d26-4d7b-8c77-7da312a6358a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "\t The first question you asked was, \"how does parameter efficient finetuning work?\"\n",
      "Sources:\n",
      "\tNo sources used!\n"
     ]
    }
   ],
   "source": [
    "# Third Question: (check if it can recall previous interactions)\n",
    "response = chat_engine.chat(\"What was the first question I asked?\")\n",
    "display_res(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "7jfiLpru5VZT"
   },
   "outputs": [],
   "source": [
    "# Reset the session to clear the memory\n",
    "chat_engine.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jt0q8RW25VXN",
    "outputId": "1669605d-cff3-4b56-d9ea-549e13a08e9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "\t The first question you asked was, \"What was the first question I asked?\"\n",
      "Sources:\n",
      "\tNo sources used!\n"
     ]
    }
   ],
   "source": [
    "# Fourth Question: (don't recall the previous interactions.)\n",
    "response = chat_engine.chat(\"What was the first question I asked?\")\n",
    "display_res(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Egsib7yPJGR"
   },
   "source": [
    "# Streaming\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zanJeMbaPJcq",
    "outputId": "9ebf0fbc-e489-4ad1-8bd3-0bb67100fa6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieval-Augmented Generation (RAG) and Parameter-Efficient Fine-Tuning (PEFT) are two distinct methodologies in the realm of machine learning, particularly for natural language processing tasks.\n",
      "\n",
      "RAG operates by combining a retrieval component and a generation component. The retrieval component extracts relevant information from external knowledge sources (like databases or documents) by indexing them for efficient access, followed by searching to find the most pertinent data based on user queries. The generation component then utilizes this retrieved information along with the user's input to produce coherent, contextually relevant responses. This method enhances the accuracy and relevance of generated text by integrating timely external information and employing sophisticated prompting techniques, such as Chain of Thought or Tree of Thought, to improve the reasoning in responses.\n",
      "\n",
      "On the other hand, PEFT focuses on adapting large pretrained models for specific tasks by fine-tuning only a minimal number of additional parameters, making it much more resource-efficient compared to traditional fine-tuning approaches. It allows models to maintain their performance while requiring significantly less computational and storage resources, thus improving accessibility for training on consumer hardware. PEFT is also designed to work seamlessly with popular machine learning frameworks, offering guides and tools for users to implement it across various tasks, like image classification or language modeling.\n",
      "\n",
      "In summary, while RAG enhances generative models by incorporating relevant external information into responses, PEFT streamlines the adaptation of large models to new tasks by minimizing the resources needed for fine-tuning."
     ]
    }
   ],
   "source": [
    "# Stream the words as soon as they are available instead of waiting for the model to finish generation.\n",
    "streaming_response = chat_engine.stream_chat(\n",
    "    \"Write a paragraph explaining how RAG and PEFT work, and highlight the differences between them.\"\n",
    ")\n",
    "streaming_response.print_response_stream()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DuRgOJ2AHMJh"
   },
   "source": [
    "## Condense Question\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yb2Lt41jq145"
   },
   "source": [
    "Enhance the input prompt by looking at the previous chat history along with the present question. The refined prompt can then be used to fetch the nodes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "v0gmM5LGIaRl"
   },
   "outputs": [],
   "source": [
    "# Define GPT-4 model that will be used by the chat_engine to improve the query.\n",
    "gpt4 = OpenAI(temperature=0.9, model=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "EDWsaBTBIhK7"
   },
   "outputs": [],
   "source": [
    "chat_engine = vector_index.as_chat_engine(\n",
    "    chat_mode=\"condense_question\", llm=gpt4, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h4c--hJ75VU2",
    "outputId": "9ef2dd4b-6b21-41f8-a637-56ebb7dce0b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Querying with: How does Retrieval-Augmented Generation (RAG) work, and which problem does it solve?\n",
      "Response:\n",
      "\t Retrieval-Augmented Generation (RAG) combines the strengths of pretraining and retrieval-based models to enhance the performance of generative language models. It operates by integrating information retrieval techniques with the generative capabilities of large language models (LLMs). The process involves several key steps: classifying queries to determine the need for retrieval, retrieving relevant documents, reranking them for relevance, repacking the information for structured input, and summarizing key points for response generation. RAG addresses the problems of producing outdated information and fabricating facts that are common in LLMs by ensuring the generated content is based on up-to-date and relevant information from external sources.\n",
      "Sources:\n",
      "\tNode ID\t 2aa05360-f43a-4819-bce7-0acf7b897eab\n",
      "\tText\t Generative large language models are prone to producing outdated information or fabricating facts, although they were aligned with human preferences by reinforcement learning [1] or lightweight alternatives [2–5]. Retrieval-augmented generation (RAG) techniques address these issues by combining the strengths of pretraining and retrieval-based models, thereby providing a robust framework for enhancing model performance [6]. Furthermore, RAG enables rapid deployment of applications for specific organizations and domains without necessitating updates to the model parameters, as long as query-related documents are provided. Many RAG approaches have been proposed to enhance large language models (LLMs) through query-dependent retrievals [6–8]. A typical RAG workflow usually contains multiple intervening processing steps: query classification (determining whether retrieval is necessary for a given input query), retrieval (efficiently obtaining relevant documents for the query), reranking (refining the order of retrieved documents based on their relevance to the query), repacking (organizing the retrieved documents into a structured one for better generation), summarization (extracting key information for response generation from the repacked document and eliminating redundancies) and models. Implementing RAG also requires decisions on the ways to properly split documents into chunks, the types of embeddings to use for semantically representing these chunks, the choice of\n",
      "\tScore\t 0.594250348937533\n",
      "\t-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
      "\tNode ID\t 1c324686-fad7-4a41-bfd3-d44f9612ca91\n",
      "\tText\t Authors: Hao Yu, Aoran Gan, Kai Zhang, Shiwei Tong, Qi Liu, and Zhaofeng Liu.Numerous studies of Retrieval-Augmented Generation (RAG) systems have emerged from various perspectives since the advent of Large Language Models (LLMs). The RAG system comprises two primary components: Retrieval and Generation. The retrieval component aims to extract relevant information from various external knowledge sources. It involves two main phases, indexing and searching. Indexing organizes documents to facilitate efficient retrieval, using either inverted indexes for sparse retrieval or dense vector encoding for dense retrieval. The searching component utilizes these indexes to fetch relevant documents based on the user's query, often incorporating the optional rerankers to refine the ranking of the retrieved documents. The generation component utilizes the retrieved content and question query to formulate coherent and contextually relevant responses with the prompting and inferencing phases. As the \"Emerging\" ability of LLMs and the breakthrough in aligning human commands, LLMs are the best performance choices model for the generation stage. Prompting methods like Chain of Thought, Tree of Thought, Rephrase and Respond guide better generation results. In the inferencing step, LLMs interpret the prompted input to generate accurate and in-depth responses that align with the query's intent and integrate the extracted information without further finetuning, such as fully finetuning or LoRA. Appendix A details the complete RAG structure. Figure 1 illustrates the structure of the RAG systems as mentioned.\n",
      "\tScore\t 0.5743425352893081\n",
      "\t-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n"
     ]
    }
   ],
   "source": [
    "response = chat_engine.chat(\n",
    "    \"How does Retrieval-Augmented Generation (RAG) work, and which problem does it solve?\"\n",
    ")\n",
    "display_res(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ysL9ONePOsGB"
   },
   "source": [
    "## ReAct\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KiEFmxAtrmF-"
   },
   "source": [
    "ReAct is an agent-based chat mode that uses a loop to decide on querying a data engine during interactions, offering flexibility but relying on the Large Language Model's quality for effective responses, requiring careful management to avoid inaccurate answers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "-M1jWoKXOs2t"
   },
   "outputs": [],
   "source": [
    "chat_engine = vector_index.as_chat_engine(chat_mode=\"react\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UZkEW1SSOs0H",
    "outputId": "88a45cf4-96e0-4fc3-f6b6-35f13f5c142a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Which company developed Claude 3.5 Sonnet, and what is its primary application?\n",
      "=== Calling Function ===\n",
      "Calling function: query_engine_tool with args: {\"input\":\"Which company developed Claude 3.5 Sonnet, and what is its primary application?\"}\n",
      "Got output: The context does not provide information about the company that developed Claude 3.5 Sonnet or its primary application.\n",
      "========================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = chat_engine.chat(\n",
    "    \"Which company developed Claude 3.5 Sonnet, and what is its primary application?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eW5P1lD4Osxf",
    "outputId": "32f16ef2-1691-44be-9b6e-385f5129ca56"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "\t I couldn't find specific information about the company that developed Claude 3.5 Sonnet or its primary application. If you have any other questions or need further assistance, feel free to ask!\n",
      "Sources:\n",
      "\tNode ID\t 55740ef4-3809-4dfa-ad06-e85bac4e165f\n",
      "\tText\t seeing. Most visual perception is handled by low-level processes that merely tell your brain \"that\\'s a water droplet\" without telling you details like where the lightest and darkest points are, or \"that\\'s a bush\" without telling you the shape and position of every leaf. This is a feature of brains, not a bug. In everyday life it would be distracting to notice every leaf on every bush. But when you have to paint something, you have to look more closely, and when you do there\\'s a lot to see. You can still be noticing new things after days of trying to paint something people usually take for granted, just as you can after days of trying to write an essay about something people usually take for granted.\\n\\nThis is not the only way to paint. I\\'m not 100% sure it\\'s even a good way to paint. But it seemed a good enough bet to be worth trying.\\n\\nOur teacher, professor Ulivi, was a nice guy. He could see I worked hard, and gave me a good grade, which he wrote down in a sort of passport each student had. But the Accademia wasn\\'t teaching me anything except Italian, and my money was running out, so at the end of the first year I went back to the US.\\n\\nI wanted to go back to RISD, but I was now broke and RISD was very expensive, so I decided to get a job for a year and then return to RISD the next fall. I got one at a company called Interleaf, which made software for creating documents. You mean like Microsoft Word? Exactly. That was how I learned that low end software tends to eat high end software. But Interleaf still had a few years to live yet. [5]\\n\\nInterleaf had done something pretty bold. Inspired by Emacs, they\\'d added a scripting language, and even made the scripting language a dialect of Lisp. Now they wanted a Lisp hacker to write things in it. This was the closest thing I\\'ve had to a normal job, and I hereby apologize to my boss and coworkers, because I was a bad employee. Their Lisp was the thinnest icing on a giant C cake, and since I didn\\'t know C and didn\\'t want to learn it, I never understood most of the software.\n",
      "\tScore\t 0.2511663130633601\n",
      "\t-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
      "\tNode ID\t 03b1c975-f71f-4300-bf73-bbb602b8ee69\n",
      "\tText\t and had \"hardly any plot.\"2. Programming on an IBM 1401 computer in 9th grade, using an early version of Fortran language.3. Building simple games, a program to predict the height of model rockets, and a word processor for his father.4. Reading science fiction novels, such as \"The Moon is a Harsh Mistress\" by Heinlein, which inspired him to work on AI.5. Living in Florence, Italy, and walking through the city's streets to the Accademia.Please note that these activities are mentioned in the text and are not based on prior knowledge or assumptions.</b>### Streaming Support```pythonquery_engine = index.as_query_engine(streaming=True)response = query_engine.query(\"What happened at interleaf?\")for token in response.response_gen:    print(token, end=\"\")```     Based on the context information provided, it appears that the author worked at Interleaf, a company that made software for creating and managing documents. The author mentions that Interleaf was \"on the way down\" and that the company's Release Engineering group was large compared to the group that actually wrote the software. It is inferred that Interleaf was experiencing financial difficulties and that the author was nervous about money. However, there is no explicit mention of what specifically happened at Interleaf.\n",
      "\tScore\t 0.24692122208344394\n",
      "\t-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n"
     ]
    }
   ],
   "source": [
    "display_res(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "zf6r2AmFOsca"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "llamaindexkernel",
   "language": "python",
   "name": "llamaindexkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
