{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6j1vDKF4HpMR"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/towardsai/ai-tutor-rag-system/blob/main/notebooks/14-Adding_Chat.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-zE1h0uQV7uT"
   },
   "source": [
    "## Install Packages and Setup Variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "s36CGBwAIeUc",
    "outputId": "e0f11299-d801-4426-8a58-c4061a6f0a7d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.8/19.8 MB\u001b[0m \u001b[31m86.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m57.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m72.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.9/11.9 MB\u001b[0m \u001b[31m99.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m303.3/303.3 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.3/103.3 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m85.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.9/131.9 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.7/65.7 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m208.0/208.0 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.8/510.8 kB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.1/56.1 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.9/63.9 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.5/310.5 kB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m87.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m452.2/452.2 kB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.4/144.4 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "!pip install -q llama-index==0.14.0 openai==1.107.0 chromadb==1.0.21 llama-index-vector-stores-chroma==0.5.3 jedi==0.19.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "riuXwpSPcvWC"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set the following API Keys in the Python environment. Will be used later.\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"<YOUR_OPENAI_KEY>\"\n",
    "\n",
    "from google.colab import userdata\n",
    "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jIEeZzqLbz0J"
   },
   "outputs": [],
   "source": [
    "# Allows running asyncio in environments with an existing event loop, like Jupyter notebooks.\n",
    "\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bkgi2OrYzF7q"
   },
   "source": [
    "# Load Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9oGT6crooSSj"
   },
   "outputs": [],
   "source": [
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core import Settings\n",
    "\n",
    "Settings.llm = OpenAI(model=\"gpt-5-mini\", additional_kwrgs={'reasoning_effort':'minimal'})\n",
    "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OWaT6rL7ksp8"
   },
   "source": [
    "# Load Indexes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "29756ffb4ba243a5af7b3e135f353f11",
      "7aab0bf598da4d229229979ecdc76314",
      "b10b953b884b44b49f6f695a18ca88b3",
      "8d6bcea686b24b84a87ded7b65dbf65d",
      "267b804021514565bbb407b443364b3d",
      "3f512da11d0c4ec385b8fb2667e2e3dc",
      "6905b6bf8d2a4e2aa160e521e1a53a11",
      "e8e74694d8624cea93d854cffdfb77b0",
      "ab2dccb9721c4680af8b83b68a02b646",
      "4c5c7caa997f4b0ea91f4dabcfd8fc1d",
      "64b6b39d598f4ddebeee7324d51cbded"
     ]
    },
    "id": "60AA1iCrqApk",
    "outputId": "b8e2d017-7436-4f6b-a9e3-dcdc8cbebee4"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29756ffb4ba243a5af7b3e135f353f11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vectorstore.zip:   0%|          | 0.00/97.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Downloading Vector store from Hugging face hub\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "vectorstore = hf_hub_download(repo_id=\"jaiganesan/ai_tutor_knowledge\", filename=\"vectorstore.zip\", repo_type=\"dataset\", local_dir=\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SodY2Xpf_kxg",
    "outputId": "25486ef4-2781-499e-b895-34e3dd7e04a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  vectorstore.zip\n",
      "   creating: ai_tutor_knowledge/\n",
      "   creating: ai_tutor_knowledge/684af133-f877-4230-bde4-575cf53b6688/\n",
      "  inflating: ai_tutor_knowledge/684af133-f877-4230-bde4-575cf53b6688/length.bin  \n",
      "  inflating: ai_tutor_knowledge/684af133-f877-4230-bde4-575cf53b6688/index_metadata.pickle  \n",
      "  inflating: ai_tutor_knowledge/684af133-f877-4230-bde4-575cf53b6688/link_lists.bin  \n",
      "  inflating: ai_tutor_knowledge/684af133-f877-4230-bde4-575cf53b6688/header.bin  \n",
      "  inflating: ai_tutor_knowledge/684af133-f877-4230-bde4-575cf53b6688/data_level0.bin  \n",
      "  inflating: ai_tutor_knowledge/chroma.sqlite3  \n"
     ]
    }
   ],
   "source": [
    "!unzip -o vectorstore.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mXi56KTXk2sp"
   },
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "# Load the vector store from the local storage.\n",
    "db = chromadb.PersistentClient(path=\"./ai_tutor_knowledge\")\n",
    "chroma_collection = db.get_or_create_collection(\"ai_tutor_knowledge\")\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "vector_index = VectorStoreIndex.from_vector_store(vector_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q0m5rl195bcz"
   },
   "source": [
    "# Display result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4JpaHEmF5dSS"
   },
   "outputs": [],
   "source": [
    "# A simple function to show the response and the sources.\n",
    "def display_res(response):\n",
    "    print(\"Response:\\n\\t\", response.response.replace(\"\\n\", \"\"))\n",
    "\n",
    "    print(\"Sources:\")\n",
    "    if response.source_nodes:\n",
    "        for src in response.source_nodes:\n",
    "            print(\"\\tNode ID\\t\", src.node_id)\n",
    "            print(\"\\tText\\t\", src.text)\n",
    "            print(\"\\tScore\\t\", src.score)\n",
    "            print(\"\\t\" + \"-_\" * 20)\n",
    "    else:\n",
    "        print(\"\\tNo sources used!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hbStjvUJ1cft"
   },
   "source": [
    "# Chat Engine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kwWlDpoR1cRI"
   },
   "outputs": [],
   "source": [
    "# define the chat_engine by using the index\n",
    "chat_engine = vector_index.as_chat_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ER3Lb-oN46lJ",
    "outputId": "e24b7658-c237-4ffd-83a8-52cee9cc23fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "\t Short answerParameter-efficient fine-tuning (PEFT) keeps the large pretrained base model mostly frozen and only trains a small, compact set of extra parameters (adapter / update parameters). The extra parameters are designed so a small number can express the task-specific weight change, so you get near-full-model performance while vastly reducing trainable parameters, memory, and storage for many downstream tasks.How it works (mechanisms shown in the provided documents)- General idea  - Represent the desired change to the pretrained weights (Delta W) using a compact parameterization instead of updating every weight directly.  - Add or compose the compact update with the frozen base weights during inference/training.  - Train only those compact parameters, leaving the main model weights frozen.- Low-rank updates (LoRA — mentioned for comparison)  - Delta W is factorized as BA where A and B are low-rank matrices. Training A and B is far cheaper than training the full weight matrix.- FourierFT (Discrete Fourier Transform fine-tuning)  - Treats Delta W as a matrix in the spatial domain and parameterizes it in the spectral domain (via Discrete Fourier Transform).  - Only a small fraction of spectral coefficients are learned; the inverse DFT reconstructs Delta W for use with the frozen base weights.  - This compresses the tunable weights aggressively and can match or beat LoRA on benchmarks while using far fewer trainable parameters.  - Constraints from the docs: currently supports only nn.Linear layers, and it does not support quantized layers.  - Example: on instruction tuning for LLaMA2-7B the paper reports FourierFT using 0.064M trainable parameters versus LoRA’s 33.5M.- BOFT (Orthogonal Butterfly Finetuning)  - Builds on Orthogonal Finetuning (OFT) but uses a butterfly-structured orthogonal parameterization inspired by the Cooley–Tukey FFT to transmit information more efficiently.  - The butterfly parameterization greatly reduces the number of parameters needed to represent orthogonal transforms compared to naive orthogonal matrices, improving parameter efficiency.  - BOFT subsumes OFT as a special case and is evaluated across large vision transformers, large language models, and text-to-image diffusion models.Practical benefits and uses (from documents)- Dramatically fewer trainable parameters (and lower storage for many customized adapters).- Comparable or better performance vs. larger-update methods on tasks such as natural language understanding, generation, instruction tuning, and image classification.- Works well for adapting very large foundation models without full fine-tuning.Practical constraints to watch for (from documents)- Some PEFT methods are layer-type limited (e.g., FourierFT only supports nn.Linear).- Some methods do not support quantized layers (FourierFT explicitly excludes quantized layers).- Different parameterizations trade expressiveness, compute, and storage; pick the method that matches your model architecture and deployment constraints.Convenience tooling- PEFT libraries provide model wrappers (e.g., AutoPeftModel / PeftModel classes) that let you load and apply these compact parameterizations to a base model easily, automatically inferring appropriate PEFT model classes where possible.If you want, I can:- Compare a few PEFT methods side-by-side (LoRA, FourierFT, BOFT) with more concrete pros/cons.- Suggest which method to try given a particular base model (architecture, quantization) and task.\n",
      "Sources:\n",
      "\tNode ID\t 6be88fa3-2f8b-43e7-aba0-d874b39809fc\n",
      "\tText\t # FourierFT: Discrete Fourier Transformation Fine-Tuning[FourierFT](https://huggingface.co/papers/2405.03003) is a parameter-efficient fine-tuning technique that leverages Discrete Fourier Transform to compress the model's tunable weights. This method outperforms LoRA in the GLUE benchmark and common ViT classification tasks using much less parameters.FourierFT currently has the following constraints:- Only `nn.Linear` layers are supported.- Quantized layers are not supported.If these constraints don't work for your use case, consider other methods instead.The abstract from the paper is:> Low-rank adaptation (LoRA) has recently gained much interest in fine-tuning foundation models. It effectively reduces the number of trainable parameters by incorporating low-rank matrices A and B to represent the weight change, i.e., Delta W=BA. Despite LoRA's progress, it faces storage challenges when handling extensive customization adaptations or larger base models. In this work, we aim to further compress trainable parameters by enjoying the powerful expressiveness of the Fourier transform. Specifically, we introduce FourierFT, which treats Delta W as a matrix in the spatial domain and learns only a small fraction of its spectral coefficients. With the trained spectral coefficients, we implement the inverse discrete Fourier transform to recover Delta W. Empirically, our FourierFT method shows comparable or better performance with fewer parameters than LoRA on various tasks, including natural language understanding, natural language generation, instruction tuning, and image classification. For example, when performing instruction tuning on the LLaMA2-7B model, FourierFT surpasses LoRA with only 0.064M trainable parameters, compared to LoRA's 33.5M.## FourierFTConfig[[autodoc]] tuners.fourierft.config.FourierFTConfig## FourierFTModel[[autodoc]] tuners.fourierft.model.FourierFTModel\n",
      "\tScore\t 0.3667481992485351\n",
      "\t-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
      "\tNode ID\t dee706b6-2345-4e7f-b850-1d80e233d998\n",
      "\tText\t # BOFT[Orthogonal Butterfly (BOFT)](https://hf.co/papers/2311.06243) is a generic method designed for finetuning foundation models. It improves the paramter efficiency of the finetuning paradigm -- Orthogonal Finetuning (OFT), by taking inspiration from Cooley-Tukey fast Fourier transform, showing favorable results across finetuning different foundation models, including large vision transformers, large language models and text-to-image diffusion models.The abstract from the paper is:*Large foundation models are becoming ubiquitous, but training them from scratch is prohibitively expensive. Thus, efficiently adapting these powerful models to downstream tasks is increasingly important. In this paper, we study a principled finetuning paradigm -- Orthogonal Finetuning (OFT) -- for downstream task adaptation. Despite demonstrating good generalizability, OFT still uses a fairly large number of trainable parameters due to the high dimensionality of orthogonal matrices. To address this, we start by examining OFT from an information transmission perspective, and then identify a few key desiderata that enable better parameter-efficiency. Inspired by how the Cooley-Tukey fast Fourier transform algorithm enables efficient information transmission, we propose an efficient orthogonal parameterization using butterfly structures. We apply this parameterization to OFT, creating a novel parameter-efficient finetuning method, called Orthogonal Butterfly (BOFT). By subsuming OFT as a special case, BOFT introduces a generalized orthogonal finetuning framework. Finally, we conduct an extensive empirical study of adapting large vision transformers, large language models, and text-to-image diffusion models to various downstream tasks in vision and language*.## BOFTConfig[[autodoc]] tuners.boft.config.BOFTConfig## BOFTModel[[autodoc]] tuners.boft.model.BOFTModel\n",
      "\tScore\t 0.34777707617854275\n",
      "\t-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n"
     ]
    }
   ],
   "source": [
    "# First Question:\n",
    "response = chat_engine.chat(\"Use the tool to answer, how does parameter efficient finetuning work?\")\n",
    "\n",
    "display_res(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3RRmiJEQ5R1Q",
    "outputId": "354e8e45-e725-4397-ed96-fdf08fe7dca1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "\t Absolutely — love that you're in the mood for a laugh! Here you go:Why did the kangaroo cross the road?  To prove it wasn't chicken.Want another one (animal, robot, or New York-themed)?\n",
      "Sources:\n",
      "\tNode ID\t c4d6c614-e8ba-4aa4-92df-a2a2d4456717\n",
      "\tText\t the possum it could be done.\\n\\n- It was on its way to a poultry farmers\\' convention.\\n\\nThe joke plays on the double meaning of \"the other side\" - literally crossing the road to the other side, or the \"other side\" meaning the afterlife. So it\\'s an anti-joke, with a silly or unexpected pun as the answer.' additional_kwargs={} example=FalseWe can use our \"LLM with Fallbacks\" as we would a normal LLM.```pythonfrom langchain_core.prompts import ChatPromptTemplateprompt = ChatPromptTemplate.from_messages(    [        (            \"system\",            \"You're a nice assistant who always includes a compliment in your response\",        ),        (\"human\", \"Why did the {animal} cross the road\"),    ])chain = prompt | llmwith patch(\"openai.resources.chat.completions.Completions.create\", side_effect=error):    try:        print(chain.invoke({\"animal\": \"kangaroo\"}))    except RateLimitError:        print(\"Hit error\")```    content=\" I don't actually know why the kangaroo crossed the road, but I can take a guess! Here are some possible reasons:\\n\\n- To get to the other side (the classic joke answer!)\\n\\n- It was trying to find some food or water \\n\\n- It was trying to find a mate during mating season\\n\\n- It was fleeing from a predator or perceived threat\\n\\n- It was disoriented and crossed accidentally \\n\\n- It was following a herd of other kangaroos who were crossing\\n\\n- It wanted a change of scenery or environment \\n\\n- It was trying to reach a new habitat or territory\\n\\nThe real reason is unknown without more context, but hopefully one of those potential explanations does the joke justice! Let me know if you have any other animal jokes I can try to decipher.\" additional_kwargs={} example=False## Fallback for SequencesWe can also create fallbacks for sequences, that are sequences themselves. Here we do that with two different models: ChatOpenAI\n",
      "\tScore\t 0.2612993224799625\n",
      "\t-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
      "\tNode ID\t 12b881d2-1566-484f-89eb-40fb30c9ec4f\n",
      "\tText\t alright, I'll give you the lowdown. But don't say I didn't warn you, I'm a robot, not a tour guide!So, you wanna know what's fun to do in the Big Apple? Well, let me tell you, there's a million things to do, but I'll give you the highlights. First off, you gotta see the sights: the Statue of Liberty, Central Park, Times Square... you know, the usual tourist traps. But if you're lookin' for something a little more... unusual, I'd recommend checkin' out the Museum of Modern Art. It's got some wild stuff, like that Warhol guy's soup cans and all that jazz.And if you're feelin' adventurous, take a walk across the Brooklyn Bridge. Just watch out for those pesky pigeons, they're like little feathered thieves! (laughs) Get it? Thieves? Ah, never mind.Now, if you're lookin' for some serious fun, hit up the comedy clubs in Greenwich Village. You might even catch a glimpse of some up-and-coming comedians... or a bunch of wannabes tryin' to make it big. (winks)And finally, if you're feelin' like a real New Yorker, grab a slice of pizza from one of the many amazingpizzerias around the city. Just don't try to order a \"robot-sized\" slice, trust me, it won't end well. (laughs)So, there you have it, pal! That's my expert advice on what to do in New York. Now, if you'llexcuse me, I've got some oil changes to attend to. (winks)```You can continue the chat by appending your own response to it. The`response` object returned by the pipeline actually contains the entire chat so far, so we can simply appenda message and pass it back:```pythonchat = response[0]['generated_text']chat.append(    {\"role\": \"user\", \"content\": \"Wait, what's so wild about soup cans?\"})response = pipe(chat, max_new_tokens=512)print(response[0]['generated_text'][-1]['content'])```And you'll get:```text(laughs) Oh, you're killin' me, pal! You don't get it, do you? Warhol's soup cans are like,\n",
      "\tScore\t 0.24664256823972958\n",
      "\t-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n"
     ]
    }
   ],
   "source": [
    "# Second Question:\n",
    "response = chat_engine.chat(\"Could you tell me a joke?\")\n",
    "display_res(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8eOzp5Xc5Vbj",
    "outputId": "a9a0ef44-1b9e-4688-98f3-ed7712eda054"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "\t Your first question was: \"Use the tool to answer, how does parameter efficient finetuning work?\"\n",
      "Sources:\n",
      "\tNode ID\t 0a13c990-4b3e-4a61-99e0-85deddf1452a\n",
      "\tText\t holidays. Yearly calendar showing months for the year 2022. Calendars – online and print friendly – for any year ...[0m[32;1m[1;3mShiver me timbers, it looks like this be a question about the year 2022. Let me search one more time.    Action: Search    Action Input: \"What be happenin' in 2022?\"[0m        Observation:[36;1m[1;3m8. Humanitarian Crises Deepen · 7. Latin America Moves Left. · 6. Iranians Protest. · 5. COVID Eases. · 4. Inflation Returns. · 3. Climate Change ...[0m[32;1m[1;3mAvast ye, it looks like the same results be comin' up. I reckon there be no clear answer to this question.    Final Answer: Arg, I be sorry matey, but I can't give ye a clear answer to that question.[0m        [1m> Finished chain.[0m    \"Arg, I be sorry matey, but I can't give ye a clear answer to that question.\"## LLM Agent with HistoryExtend the LLM Agent with the ability to retain a [memory](https://python.langchain.com/en/latest/modules/agents/agents/custom_llm_agent.html#adding-memory) and use it as context as it continues the conversation.We use a simple ```ConversationBufferWindowMemory``` for this example that keeps a rolling window of the last two conversation turns. LangChain has other [memory options](https://python.langchain.com/en/latest/modules/memory.html), with different tradeoffs suitable for different use cases.```python# Set up a prompt template which can interpolate the historytemplate_with_history = \"\"\"You are SearchGPT, a professional search engine who provides informative answers to users. Answer the following questions as best you can. You have access to the following tools:{tools}Use the following format:Question: the input question you must answerThought: you should always think about what to doAction: the action to take, should be one of [{tool_names}]Action Input: the input to the actionObservation: the result\n",
      "\tScore\t 0.29808698336781597\n",
      "\t-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
      "\tNode ID\t 91ee09f9-e626-4e26-93e9-1dc846ec265f\n",
      "\tText\t import WebBaseLoaderfrom langchain_community.vectorstores import FAISSfrom langchain_openai.chat_models import ChatOpenAIfrom langchain_openai.embeddings import OpenAIEmbeddingsfrom langchain_text_splitters import RecursiveCharacterTextSplitterloader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")data = loader.load()# Splittext_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)all_splits = text_splitter.split_documents(data)# Store splitsvectorstore = FAISS.from_documents(documents=all_splits, embedding=OpenAIEmbeddings())# LLMllm = ChatOpenAI()```## Legacy<details open>```pythonfrom langchain.chains import ConversationalRetrievalChainfrom langchain_core.prompts import ChatPromptTemplatecondense_question_template = \"\"\"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question.Chat History:{chat_history}Follow Up Input: {question}Standalone question:\"\"\"condense_question_prompt = ChatPromptTemplate.from_template(condense_question_template)qa_template = \"\"\"You are an assistant for question-answering tasks.Use the following pieces of retrieved context to answerthe question. If you don't know the answer, say that youdon't know. Use three sentences maximum and keep theanswer concise.Chat History:{chat_history}Other context:{context}Question: {question}\"\"\"qa_prompt = ChatPromptTemplate.from_template(qa_template)convo_qa_chain = ConversationalRetrievalChain.from_llm(    llm,    vectorstore.as_retriever(),    condense_question_prompt=condense_question_prompt,    combine_docs_chain_kwargs={        \"prompt\": qa_prompt,    },)convo_qa_chain(    {        \"question\": \"What are autonomous agents?\",        \"chat_history\": \"\",    })```    {'question': 'What are autonomous agents?',     'chat_history': '',     'answer': 'Autonomous agents are entities empowered with capabilities like planning, task decomposition, and memory to perform complex tasks independently. These agents can leverage tools like browsing the\n",
      "\tScore\t 0.2975534453857518\n",
      "\t-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n"
     ]
    }
   ],
   "source": [
    "# Third Question: (check if it can recall previous interactions)\n",
    "response = chat_engine.chat(\"What was the first question I asked?\")\n",
    "display_res(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7jfiLpru5VZT"
   },
   "outputs": [],
   "source": [
    "# Reset the session to clear the memory\n",
    "chat_engine.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jt0q8RW25VXN",
    "outputId": "42aece9c-1ea3-4829-a05c-264ad6ff30f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "\t Don't know — the excerpts show example queries \"What was a hard moment for the author?\" and \"What did the author do growing up?\", but they don't indicate which one was asked first.\n",
      "Sources:\n",
      "\tNode ID\t b71bd703-51b9-4ff9-9694-b0823ad1f178\n",
      "\tText\t Node ID: adb6b7ce-49bb-4961-8506-37082c02a389    Text: What I Worked On  February 2021  Before college the two main    things I worked on, outside of school, were writing and programming. I    didn't write essays. I wrote what beginning writers were supposed to    write then, and probably still are: short stories. My stories were    awful. They had hardly any plot, just characters with strong feelings,    which I ...    Score:  0.802        Node ID: e39be1fe-32d0-456e-b211-4efabd191108    Text: Except for a few officially anointed thinkers who went to the    right parties in New York, the only people allowed to publish essays    were specialists writing about their specialties. There were so many    essays that had never been written, because there had been no way to    publish them. Now they could be, and I was going to write them. [12]    I've wor...    Score:  0.799    ```pythonresponse = query_engine.query(\"What was a hard moment for the author?\")print(textwrap.fill(str(response), 100))```    19:39:29 httpx INFO   HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"    19:39:29 llama_index.vector_stores.redis.base INFO   Querying index llama_index with filters *    19:39:29 llama_index.vector_stores.redis.base INFO   Found 2 results for query with id ['llama_index/vector_adb6b7ce-49bb-4961-8506-37082c02a389', 'llama_index/vector_e39be1fe-32d0-456e-b211-4efabd191108']    19:39:31 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"    A hard moment\n",
      "\tScore\t 0.276704321396143\n",
      "\t-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
      "\tNode ID\t 569a1dac-ce00-4114-8f65-f08cc8f53dff\n",
      "\tText\t llama_index.core import VectorStoreIndexfrom llama_index.core import StorageContext, ServiceContextfrom llama_index.vector_stores.firestore import FirestoreVectorStore# Create a Firestore vector storestore = FirestoreVectorStore(collection_name=COLLECTION_NAME)storage_context = StorageContext.from_defaults(vector_store=store)service_context = ServiceContext.from_defaults(    llm=None, embed_model=embed_model)index = VectorStoreIndex.from_documents(    documents, storage_context=storage_context, service_context=service_context)```    /var/folders/mh/cqn7wzgs3j79rbg243_gfcx80000gn/T/ipykernel_29666/1668628626.py:10: DeprecationWarning: Call to deprecated class method from_defaults. (ServiceContext is deprecated, please use `llama_index.settings.Settings` instead.) -- Deprecated since version 0.10.0.      service_context = ServiceContext.from_defaults(llm=None, embed_model=embed_model)    LLM is explicitly disabled. Using MockLLM.### Perform searchYou can use the `FirestoreVectorStore` to perform similarity searches on the vectors you have stored. This is useful for finding similar documents or text.```pythonquery_engine = index.as_query_engine()res = query_engine.query(\"What did the author do growing up?\")print(str(res.source_nodes[0].text))```    None    What I Worked On        February 2021        Before college the two main things I worked on, outside of school, were writing and programming. I didn't write essays. I wrote what beginning writers were supposed to write then, and probably still are: short stories. My stories were awful. They had hardly any plot, just characters with strong feelings, which I imagined made them deep.        The first programs I tried writing were on the IBM 1401 that our school district used for what was then called \"data processing.\" This was in 9th grade, so I was 13 or 14. The school district's 1401 happened to be in the basement of our junior high school, and my friend Rich Draves and I got permission to use it. It was like a mini Bond villain's lair down there,\n",
      "\tScore\t 0.27561842357678606\n",
      "\t-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n"
     ]
    }
   ],
   "source": [
    "# Fourth Question: (don't recall the previous interactions.)\n",
    "response = chat_engine.chat(\"What was the first question I asked?\")\n",
    "display_res(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Egsib7yPJGR"
   },
   "source": [
    "# Streaming\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zanJeMbaPJcq",
    "outputId": "cf91f4aa-020e-4a61-e02b-b581e8a524ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG (Retrieval-Augmented Generation) combines a pre-trained seq2seq generator (parametric memory) with a non-parametric dense vector index accessed by a neural retriever: relevant documents are retrieved from the index, passed to the seq2seq model, and the model marginalizes over them to generate answers. Two common RAG formulations either condition the whole generated sequence on the same retrieved passages or allow different passages per token; the retriever and generator can be initialized from pretrained models and fine-tuned jointly for downstream, knowledge-intensive tasks, yielding more specific, diverse, and factual outputs than parametric-only seq2seq baselines. The provided documents do not contain information about PEFT (parameter-efficient fine-tuning), so I don't know how PEFT is described in these sources and cannot reliably highlight differences between RAG and PEFT from the given excerpts."
     ]
    }
   ],
   "source": [
    "# Stream the words as soon as they are available instead of waiting for the model to finish generation.\n",
    "streaming_response = chat_engine.stream_chat(\n",
    "    \"Write a paragraph explaining how RAG and PEFT work, and highlight the differences between them.\"\n",
    ")\n",
    "streaming_response.print_response_stream()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DuRgOJ2AHMJh"
   },
   "source": [
    "## Condense Question\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yb2Lt41jq145"
   },
   "source": [
    "Enhance the input prompt by looking at the previous chat history along with the present question. The refined prompt can then be used to fetch the nodes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v0gmM5LGIaRl"
   },
   "outputs": [],
   "source": [
    "# Define GPT-5 model that will be used by the chat_engine to improve the query.\n",
    "gpt5 = OpenAI(model=\"gpt-5\", additional_kwrgs={'reasoning_effort':'minimal'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EDWsaBTBIhK7"
   },
   "outputs": [],
   "source": [
    "chat_engine = vector_index.as_chat_engine(\n",
    "    chat_mode=\"condense_question\", llm=gpt5, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h4c--hJ75VU2",
    "outputId": "9d31f966-38f2-424e-f7ff-59eed24857f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Querying with: How does Retrieval-Augmented Generation (RAG) work, and which problem does it solve?\n",
      "Response:\n",
      "\t Retrieval-Augmented Generation (RAG) augments a generative language model with on-demand access to external documents so answers are grounded in relevant, up-to-date evidence. It tackles a core limitation of large language models: they can produce outdated information and fabricate facts. RAG reduces these issues and enables rapid, domain-specific deployment without updating model parameters, as long as relevant documents are available.How it works:- Query classification: Decide whether a given query needs retrieval.- Retrieval: Index a corpus (e.g., inverted indexes for sparse retrieval or dense vector encodings for dense retrieval), search for relevant documents, and optionally rerank them to improve relevance.- Repacking: Organize the retrieved documents into a structured context for generation.- Summarization: Extract key information and remove redundancy from the repacked content.- Generation: The LLM takes the user query plus the processed evidence and, guided by prompting methods such as Chain of Thought, Tree of Thought, or Rephrase-and-Respond, produces a coherent, contextually relevant answer without additional fine-tuning.Implementation choices typically include how to chunk documents and which embeddings to use for semantic representation.\n",
      "Sources:\n",
      "\tNode ID\t 2aa05360-f43a-4819-bce7-0acf7b897eab\n",
      "\tText\t Generative large language models are prone to producing outdated information or fabricating facts, although they were aligned with human preferences by reinforcement learning [1] or lightweight alternatives [2–5]. Retrieval-augmented generation (RAG) techniques address these issues by combining the strengths of pretraining and retrieval-based models, thereby providing a robust framework for enhancing model performance [6]. Furthermore, RAG enables rapid deployment of applications for specific organizations and domains without necessitating updates to the model parameters, as long as query-related documents are provided. Many RAG approaches have been proposed to enhance large language models (LLMs) through query-dependent retrievals [6–8]. A typical RAG workflow usually contains multiple intervening processing steps: query classification (determining whether retrieval is necessary for a given input query), retrieval (efficiently obtaining relevant documents for the query), reranking (refining the order of retrieved documents based on their relevance to the query), repacking (organizing the retrieved documents into a structured one for better generation), summarization (extracting key information for response generation from the repacked document and eliminating redundancies) and models. Implementing RAG also requires decisions on the ways to properly split documents into chunks, the types of embeddings to use for semantically representing these chunks, the choice of\n",
      "\tScore\t 0.5942003378877615\n",
      "\t-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
      "\tNode ID\t 1c324686-fad7-4a41-bfd3-d44f9612ca91\n",
      "\tText\t Authors: Hao Yu, Aoran Gan, Kai Zhang, Shiwei Tong, Qi Liu, and Zhaofeng Liu.Numerous studies of Retrieval-Augmented Generation (RAG) systems have emerged from various perspectives since the advent of Large Language Models (LLMs). The RAG system comprises two primary components: Retrieval and Generation. The retrieval component aims to extract relevant information from various external knowledge sources. It involves two main phases, indexing and searching. Indexing organizes documents to facilitate efficient retrieval, using either inverted indexes for sparse retrieval or dense vector encoding for dense retrieval. The searching component utilizes these indexes to fetch relevant documents based on the user's query, often incorporating the optional rerankers to refine the ranking of the retrieved documents. The generation component utilizes the retrieved content and question query to formulate coherent and contextually relevant responses with the prompting and inferencing phases. As the \"Emerging\" ability of LLMs and the breakthrough in aligning human commands, LLMs are the best performance choices model for the generation stage. Prompting methods like Chain of Thought, Tree of Thought, Rephrase and Respond guide better generation results. In the inferencing step, LLMs interpret the prompted input to generate accurate and in-depth responses that align with the query's intent and integrate the extracted information without further finetuning, such as fully finetuning or LoRA. Appendix A details the complete RAG structure. Figure 1 illustrates the structure of the RAG systems as mentioned.\n",
      "\tScore\t 0.5742996423371074\n",
      "\t-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n"
     ]
    }
   ],
   "source": [
    "response = chat_engine.chat(\n",
    "    \"How does Retrieval-Augmented Generation (RAG) work, and which problem does it solve?\"\n",
    ")\n",
    "display_res(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ysL9ONePOsGB"
   },
   "source": [
    "## ReAct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s2BNgZYMM8AC"
   },
   "outputs": [],
   "source": [
    "from llama_index.core.agent.workflow import ReActAgent\n",
    "from llama_index.core.workflow import Context\n",
    "from llama_index.core.tools import QueryEngineTool\n",
    "\n",
    "query_engine = vector_index.as_query_engine()\n",
    "\n",
    "tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=query_engine,\n",
    "    name=\"ReAct Agent\",\n",
    "    description=\"Answer questions using the vector index; pass plain text queries.\",\n",
    ")\n",
    "\n",
    "agent = ReActAgent(\n",
    "    tools=[tool],\n",
    "    verbose=True\n",
    "\n",
    ")\n",
    "\n",
    "# context to hold this session/state\n",
    "ctx = Context(agent)\n",
    "\n",
    "handler = agent.run(\"Which company developed Claude 3.5 Sonnet, and what is its primary application?\", ctx=ctx, max_iterations=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3MdtJriFfUeb",
    "outputId": "103d7077-376e-4456-ee56-1fc4e68ded92"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running step init_run\n",
      "Step init_run produced event AgentInput\n",
      "Running step setup_agent\n",
      "Step setup_agent produced event AgentSetup\n",
      "Running step run_agent_step\n",
      "Step run_agent_step produced event AgentOutput\n",
      "Running step parse_agent_output\n",
      "Step parse_agent_output produced no event\n",
      "Running step call_tool\n",
      "Step call_tool produced event ToolCallResult\n",
      "Running step aggregate_tool_results\n",
      "Step aggregate_tool_results produced event AgentInput\n",
      "Running step setup_agent\n",
      "Step setup_agent produced event AgentSetup\n",
      "Running step run_agent_step\n",
      "Step run_agent_step produced event AgentOutput\n",
      "Running step parse_agent_output\n",
      "Step parse_agent_output produced event StopEvent\n",
      "Claude 3.5 Sonnet was developed by Anthropic. Its primary application is as a general-purpose large language model for conversational AI and text-generation tasks — i.e., chatbots and virtual assistants, summarization, content and code generation, and other assistant-style workflows (with the Sonnet variant positioned for efficient, production-oriented chat use).\n"
     ]
    }
   ],
   "source": [
    "response = await handler\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N0uNaUDZflA-"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "llamaindexkernel",
   "language": "python",
   "name": "llamaindexkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
