{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create AI-Tutor vector database\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\"../.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean scraped data\n",
    "- Removes sections with <7 tokens and sections titled \"Transformers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import uuid\n",
    "import tiktoken\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(\n",
    "        encoding.encode(\n",
    "            string, disallowed_special=(encoding.special_tokens_set - {\"<|endoftext|>\"})\n",
    "        )\n",
    "    )\n",
    "    return num_tokens\n",
    "\n",
    "\n",
    "def clean_jsonl_file(input_filepath, output_filepath):\n",
    "    cleaned_data = []\n",
    "\n",
    "    with open(input_filepath, \"r\") as file:\n",
    "        for line in file:\n",
    "            json_obj = json.loads(line)\n",
    "            content = json_obj.get(\"content\", \"\")\n",
    "            token_count = num_tokens_from_string(content, \"cl100k_base\")\n",
    "\n",
    "            # Check conditions for keeping the line\n",
    "            if token_count > 7 and not (\n",
    "                token_count == 92 and json_obj.get(\"name\") == \"Transformers\"\n",
    "            ):\n",
    "                # Create a new OrderedDict with 'tokens' as the first key\n",
    "                new_obj = OrderedDict(\n",
    "                    [(\"tokens\", token_count), (\"doc_id\", str(uuid.uuid4()))]\n",
    "                )\n",
    "                # Add the rest of the key-value pairs from the original object\n",
    "                new_obj.update(json_obj)\n",
    "                cleaned_data.append(new_obj)\n",
    "\n",
    "    with open(output_filepath, \"w\") as file:\n",
    "        for item in cleaned_data:\n",
    "            json.dump(item, file)\n",
    "            file.write(\"\\n\")\n",
    "\n",
    "    print(f\"Original number of lines: {sum(1 for _ in open(input_filepath))}\")\n",
    "    print(f\"Cleaned number of lines: {len(cleaned_data)}\")\n",
    "\n",
    "\n",
    "# Usage\n",
    "input_filepath = \"../hf_transformers_v4_42_0.jsonl\"\n",
    "output_filepath = \"../hf_transformers_v4_42_0_cleaned.jsonl\"\n",
    "clean_jsonl_file(input_filepath, output_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merges sections by 'URL'\n",
    "\n",
    "- Excluding sections like \"model_doc\", \"internal\", \"main_classes\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "import tiktoken\n",
    "\n",
    "\n",
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(\n",
    "        encoding.encode(\n",
    "            string, disallowed_special=(encoding.special_tokens_set - {\"<|endoftext|>\"})\n",
    "        )\n",
    "    )\n",
    "    return num_tokens\n",
    "\n",
    "\n",
    "def should_not_merge(url):\n",
    "    \"\"\"Check if the URL contains any of the exclusion patterns.\"\"\"\n",
    "    exclusion_patterns = [\"model_doc\", \"internal\", \"main_classes\"]\n",
    "    return any(pattern in url for pattern in exclusion_patterns)\n",
    "\n",
    "\n",
    "def merge_jsonl(input_file, output_file):\n",
    "    # Dictionary to store merged data\n",
    "    merged_data = defaultdict(list)\n",
    "\n",
    "    # Read and process the input file\n",
    "    with open(input_file, \"r\") as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            url = data[\"url\"]\n",
    "            merged_data[url].append(data)\n",
    "\n",
    "    # Write the merged data to the output file\n",
    "    with open(output_file, \"w\") as f:\n",
    "        for url, entries in merged_data.items():\n",
    "            if len(entries) == 1 or should_not_merge(url):\n",
    "                # If there's only one entry or it shouldn't be merged, write all entries as is\n",
    "                for entry in entries:\n",
    "                    entry[\"retrieve_doc\"] = False\n",
    "                    json.dump(entry, f)\n",
    "                    f.write(\"\\n\")\n",
    "            else:\n",
    "                # Merge the entries\n",
    "                merged_entry = entries[0].copy()\n",
    "                merged_entry[\"content\"] = \"\\n\\n\".join(\n",
    "                    entry[\"content\"] for entry in entries\n",
    "                )\n",
    "                merged_entry[\"tokens\"] = num_tokens_from_string(\n",
    "                    merged_entry[\"content\"], \"cl100k_base\"\n",
    "                )\n",
    "                merged_entry[\"retrieve_doc\"] = True\n",
    "                json.dump(merged_entry, f)\n",
    "                f.write(\"\\n\")\n",
    "\n",
    "\n",
    "# Usage\n",
    "input_file = \"../hf_transformers_v4_42_0_cleaned.jsonl\"\n",
    "output_file = \"../hf_transformers_v4_42_0_merged.jsonl\"\n",
    "merge_jsonl(input_file, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a set of Llama-index Documents with each section in the jsonl file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc ID: 85b2c5b6-ce24-4e4e-8a2d-d6557c917012\n",
      "Text: DeepSpeed is a PyTorch optimization library that makes\n",
      "distributed training memory-efficient and fast. At it’s core is the\n",
      "Zero Redundancy Optimizer (ZeRO) which enables training large models\n",
      "at scale. ZeRO works in several stages: ZeRO-1, optimizer state\n",
      "partioning across GPUs ZeRO-2, gradient partitioning across GPUs\n",
      "ZeRO-3, parameteter partit...\n",
      "{'url': 'https://huggingface.co/docs/transformers/deepspeed', 'title': 'DeepSpeed', 'tokens': 8483, 'retrieve_doc': True, 'source': 'HF_Transformers'}\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import Document\n",
    "from llama_index.core.schema import MetadataMode\n",
    "import json\n",
    "\n",
    "\n",
    "def create_docs(input_file):\n",
    "    with open(input_file, \"r\") as f:\n",
    "        documents = []\n",
    "        for i, line in enumerate(f):\n",
    "            data = json.loads(line)\n",
    "            documents.append(\n",
    "                Document(\n",
    "                    doc_id=data[\"doc_id\"],\n",
    "                    text=data[\"content\"],\n",
    "                    metadata={\n",
    "                        \"url\": data[\"url\"],\n",
    "                        \"title\": data[\"name\"],\n",
    "                        \"tokens\": data[\"tokens\"],\n",
    "                        \"retrieve_doc\": data[\"retrieve_doc\"],\n",
    "                        \"source\": \"HF_Transformers\",\n",
    "                    },\n",
    "                    excluded_llm_metadata_keys=[\n",
    "                        # \"url\",\n",
    "                        \"title\",\n",
    "                        \"tokens\",\n",
    "                        \"retrieve_doc\",\n",
    "                        \"source\",\n",
    "                    ],\n",
    "                    excluded_embed_metadata_keys=[\n",
    "                        \"url\",\n",
    "                        \"title\",\n",
    "                        \"tokens\",\n",
    "                        \"retrieve_doc\",\n",
    "                        \"source\",\n",
    "                    ],\n",
    "                )\n",
    "            )\n",
    "        return documents\n",
    "\n",
    "\n",
    "documents = create_docs(\"../hf_transformers_v4_42_0_merged.jsonl\")\n",
    "print(documents[0])\n",
    "print(documents[0].metadata)\n",
    "\n",
    "document_dict = {doc.doc_id: doc for doc in documents}\n",
    "# save dict to disk, as .pkl file\n",
    "import pickle\n",
    "\n",
    "with open(\"document_dict.pkl\", \"wb\") as f:\n",
    "    pickle.dump(document_dict, f)\n",
    "\n",
    "# load dict from disk\n",
    "with open(\"document_dict.pkl\", \"rb\") as f:\n",
    "    document_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "\n",
    "# create client and a new collection\n",
    "# chromadb.EphemeralClient saves data in-memory.\n",
    "chroma_client = chromadb.PersistentClient(path=\"./ai-tutor-vector-db\")\n",
    "chroma_collection = chroma_client.create_collection(\"ai-tutor-vector-db\")\n",
    "\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core import StorageContext\n",
    "\n",
    "# Define a storage context object using the created vector database.\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/omar/Documents/ai_repos/ai-tutor-rag-system/env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Parsing nodes: 100%|██████████| 3374/3374 [00:34<00:00, 97.04it/s] \n",
      "Generating embeddings: 100%|██████████| 21/21 [00:03<00:00,  6.49it/s]\n",
      "Generating embeddings: 100%|██████████| 21/21 [00:02<00:00,  7.21it/s]\n",
      "Generating embeddings: 100%|██████████| 21/21 [00:03<00:00,  6.80it/s]\n",
      "Generating embeddings: 100%|██████████| 21/21 [00:02<00:00,  7.24it/s]\n",
      "Generating embeddings: 100%|██████████| 21/21 [00:03<00:00,  5.56it/s]\n",
      "Generating embeddings: 100%|██████████| 21/21 [00:02<00:00,  7.63it/s]\n",
      "Generating embeddings: 100%|██████████| 21/21 [00:05<00:00,  3.55it/s]\n",
      "Generating embeddings: 100%|██████████| 21/21 [00:02<00:00,  7.33it/s]\n",
      "Generating embeddings: 100%|██████████| 21/21 [00:02<00:00,  7.39it/s]\n",
      "Generating embeddings: 100%|██████████| 21/21 [00:03<00:00,  6.10it/s]\n",
      "Generating embeddings: 100%|██████████| 21/21 [00:02<00:00,  7.44it/s]\n",
      "Generating embeddings: 100%|██████████| 21/21 [00:03<00:00,  6.40it/s]\n",
      "Generating embeddings: 100%|██████████| 21/21 [00:05<00:00,  4.17it/s]\n",
      "Generating embeddings: 100%|██████████| 21/21 [00:02<00:00,  7.82it/s]\n",
      "Generating embeddings: 100%|██████████| 21/21 [00:03<00:00,  6.66it/s]\n",
      "Generating embeddings: 100%|██████████| 21/21 [00:02<00:00,  7.56it/s]\n",
      "Generating embeddings: 100%|██████████| 21/21 [00:07<00:00,  2.67it/s]\n",
      "Generating embeddings: 100%|██████████| 21/21 [00:03<00:00,  6.06it/s]\n",
      "Generating embeddings: 100%|██████████| 21/21 [00:03<00:00,  6.33it/s]\n",
      "Generating embeddings: 100%|██████████| 21/21 [00:02<00:00,  7.31it/s]\n",
      "Generating embeddings: 100%|██████████| 21/21 [00:04<00:00,  5.21it/s]\n",
      "Generating embeddings: 100%|██████████| 21/21 [00:04<00:00,  4.65it/s]\n",
      "Generating embeddings: 100%|██████████| 21/21 [00:03<00:00,  6.71it/s]\n",
      "Generating embeddings: 100%|██████████| 21/21 [00:03<00:00,  5.66it/s]\n",
      "Generating embeddings: 100%|██████████| 21/21 [00:02<00:00,  7.22it/s]\n",
      "Generating embeddings: 100%|██████████| 21/21 [00:02<00:00,  7.19it/s]\n",
      "Generating embeddings: 100%|██████████| 21/21 [00:08<00:00,  2.53it/s]\n",
      "Generating embeddings: 100%|██████████| 21/21 [00:02<00:00,  7.41it/s]\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    "    embed_model=OpenAIEmbedding(model=\"text-embedding-3-large\", mode=\"similarity\"),\n",
    "    transformations=[SentenceSplitter(chunk_size=800, chunk_overlap=400)],\n",
    "    show_progress=True,\n",
    "    use_async=True,\n",
    "    storage_context=storage_context,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = index.as_retriever(\n",
    "    similarity_top_k=10,\n",
    "    use_async=True,\n",
    "    embed_model=OpenAIEmbedding(model=\"text-embedding-3-large\", mode=\"similarity\"),\n",
    "    # embed_model=OpenAIEmbedding(model=\"text-embedding-3-large\", mode=\"text_search\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.data_structs import Node\n",
    "from llama_index.core.schema import NodeWithScore, BaseNode, TextNode\n",
    "\n",
    "\n",
    "# query = \"fine-tune a pretrained model\"\n",
    "# query = \"fine-tune an llm\"\n",
    "query = \"how to fine-tune an llm?\"\n",
    "\n",
    "nodes_context = []\n",
    "nodes = retriever.retrieve(query)\n",
    "\n",
    "\n",
    "# Filter nodes with the same ref_doc_id\n",
    "def filter_nodes_by_unique_doc_id(nodes):\n",
    "    unique_nodes = {}\n",
    "    for node in nodes:\n",
    "        doc_id = node.node.ref_doc_id\n",
    "        if doc_id is not None and doc_id not in unique_nodes:\n",
    "            unique_nodes[doc_id] = node\n",
    "    return list(unique_nodes.values())\n",
    "\n",
    "\n",
    "nodes = filter_nodes_by_unique_doc_id(nodes)\n",
    "print(len(nodes))\n",
    "\n",
    "for node in nodes:\n",
    "    print(\"Node ID\\t\", node.node_id)\n",
    "    print(\"Title\\t\", node.metadata[\"title\"])\n",
    "    print(\"Text\\t\", node.text)\n",
    "    print(\"Score\\t\", node.score)\n",
    "    print(\"Metadata\\t\", node.metadata)\n",
    "    print(\"-_\" * 20)\n",
    "    if node.metadata[\"retrieve_doc\"] == True:\n",
    "        print(\"This node will be replaced by the document\")\n",
    "        doc = document_dict[node.node.ref_doc_id]\n",
    "        # print(doc.text)\n",
    "        new_node = NodeWithScore(\n",
    "            node=TextNode(text=doc.text, metadata=node.metadata), score=node.score\n",
    "        )\n",
    "        print(new_node.text)\n",
    "        nodes_context.append(new_node)\n",
    "    else:\n",
    "        nodes_context.append(node)\n",
    "\n",
    "print(len(nodes_context))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import ChatPromptTemplate\n",
    "from llama_index.core.llms import ChatMessage, MessageRole\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are a witty AI teacher, helpfully answering questions from students of an applied artificial intelligence course on Large Language Models (LLMs or llm). Topics covered include training models, fine-tuning models, giving 'memory' to LLMs, prompting, hallucinations and bias, vector databases, transformer architectures, embeddings, RAG frameworks, Langchain, Llama-Index, LLMs interact with tool use, AI agents, reinforcement learning with human feedback. Questions should be understood with this context.\"\n",
    "    \"You are provided information found in Hugging Face's documentation and the RAG course. \"\n",
    "    \"Only some information might be relevant to the question, so ignore the irrelevant part and use the relevant part to answer the question.\"\n",
    "    \"Only respond with information given to you documentation. DO NOT use additional information, even if you know the answer. \"\n",
    "    \"If the answer is somewhere in the documentation, answer the question (depending on the questions and the variety of relevant information in the documentation, give complete and helpful answers.\"\n",
    "    \"Here is the information you can use, the order is not important: \\n\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\\n\"\n",
    "    \"---------------------\\n\\n\"\n",
    "    \"REMEMBER:\\n\"\n",
    "    \"You are a witty AI teacher, helpfully answering questions from students of an applied artificial intelligence course on Large Language Models (LLMs or llm). Topics covered include training models, fine tuning models, giving memory to LLMs, prompting, hallucinations and bias, vector databases, transformer architectures, embeddings, RAG frameworks, Langchain, making LLMs interact with tool use, AI agents, reinforcement learning with human feedback. Questions should be understood with this context.\"\n",
    "    \"You are provided information found in Hugging Face's documentation and the RAG course. \"\n",
    "    \"Here are the rules you must follow:\\n\"\n",
    "    \"* Only respond with information inside the documentation. DO NOT provide additional information, even if you know the answer. \"\n",
    "    \"* If the answer is in the documentation, answer the question (depending on the questions and the variety of relevant information in the json documentation. Your answer needs to be pertinent and not redundant giving a clear explanation as if you were a teacher. \"\n",
    "    \"* Only use information summarized from the documentation, do not respond otherwise. \"\n",
    "    \"* Do not refer to the documentation directly, but use the instructions provided within it to answer questions. \"\n",
    "    \"* Do not reference any links, urls or hyperlinks in your answers.\\n\"\n",
    "    \"* Make sure to format your answers in Markdown format, including code block and snippets.\\n\"\n",
    "    \"Now answer the following question: \\n\"\n",
    ")\n",
    "\n",
    "chat_text_qa_msgs: list[ChatMessage] = [\n",
    "    ChatMessage(role=MessageRole.SYSTEM, content=system_prompt),\n",
    "    ChatMessage(\n",
    "        role=MessageRole.USER,\n",
    "        content=\"{query_str}\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "TEXT_QA_TEMPLATE = ChatPromptTemplate(chat_text_qa_msgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "from llama_index.core.data_structs import Node\n",
    "from llama_index.core.schema import NodeWithScore\n",
    "from llama_index.core import get_response_synthesizer\n",
    "from llama_index.llms.gemini import Gemini\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "# llm = Gemini(model=\"models/gemini-1.5-flash\", temperature=1, max_tokens=None)\n",
    "llm = Gemini(model=\"models/gemini-1.5-pro\", temperature=1, max_tokens=None)\n",
    "# llm = OpenAI(temperature=1, model=\"gpt-3.5-turbo\", max_tokens=None)\n",
    "# llm = OpenAI(temperature=1, model=\"gpt-4o\", max_tokens=None)\n",
    "\n",
    "response_synthesizer = get_response_synthesizer(\n",
    "    llm=llm, response_mode=\"simple_summarize\", text_qa_template=TEXT_QA_TEMPLATE\n",
    ")\n",
    "\n",
    "response = response_synthesizer.synthesize(query, nodes=nodes_context)\n",
    "# print(response.response)\n",
    "display(Markdown(response.response))\n",
    "\n",
    "# for src in response.source_nodes:\n",
    "#     print(src.node.ref_doc_id)\n",
    "#     print(\"Node ID\\t\", src.node_id)\n",
    "#     print(\"Title\\t\", src.metadata[\"title\"])\n",
    "#     print(\"Text\\t\", src.text)\n",
    "#     print(\"Score\\t\", src.score)\n",
    "#     print(\"Metadata\\t\", src.metadata)\n",
    "#     print(\"-_\" * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import chromadb\n",
    "\n",
    "# # create client and a new collection\n",
    "# # chromadb.EphemeralClient saves data in-memory.\n",
    "# chroma_client = chromadb.PersistentClient(path=\"./ai-tutor-db\")\n",
    "# chroma_collection = chroma_client.create_collection(\"ai-tutor-db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "# from llama_index.core import StorageContext\n",
    "\n",
    "# vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "\n",
    "# # Define a storage context object using the created vector store.\n",
    "# storage_context = StorageContext.from_defaults(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# from llama_index.core.schema import TextNode\n",
    "\n",
    "\n",
    "# def load_jsonl_create_nodes(filepath):\n",
    "#     nodes = []  # List to hold the created node objects\n",
    "#     with open(filepath, \"r\") as file:\n",
    "#         for line in file:\n",
    "#             # Load each line as a JSON object\n",
    "#             json_obj = json.loads(line)\n",
    "#             # Extract required information\n",
    "#             title = json_obj.get(\"title\")\n",
    "#             url = json_obj.get(\"url\")\n",
    "#             content = json_obj.get(\"content\")\n",
    "#             source = json_obj.get(\"source\")\n",
    "#             # Create a TextNode object and append to the list\n",
    "#             node = TextNode(\n",
    "#                 text=content,\n",
    "#                 metadata={\"title\": title, \"url\": url, \"source\": source},\n",
    "#                 excluded_embed_metadata_keys=[\"title\", \"url\", \"source\"],\n",
    "#                 excluded_llm_metadata_keys=[\"title\", \"url\", \"source\"],\n",
    "#             )\n",
    "#             nodes.append(node)\n",
    "#     return nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filepath = \"../combined_data.jsonl\"\n",
    "# nodes = load_jsonl_create_nodes(filepath)\n",
    "\n",
    "# print(f\"Loaded {len(nodes)} nodes/chunks from the JSONL file\\n \")\n",
    "\n",
    "# node = nodes[0]\n",
    "# print(f\"ID: {node.id_} \\nText: {node.text}, \\nMetadata: {node.metadata}\")\n",
    "\n",
    "# print(\"\\n\")\n",
    "\n",
    "# node = nodes[-10000]\n",
    "# print(f\"ID: {node.id_} \\nText: {node.text}, \\nMetadata: {node.metadata}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create the pipeline to apply the transformation on each chunk,\n",
    "# # and store the transformed text in the chroma vector store.\n",
    "# pipeline = IngestionPipeline(\n",
    "#     transformations=[\n",
    "#         text_splitter,\n",
    "#         QuestionsAnsweredExtractor(questions=3, llm=llm),\n",
    "#         SummaryExtractor(summaries=[\"prev\", \"self\"], llm=llm),\n",
    "#         KeywordExtractor(keywords=10, llm=llm),\n",
    "#         OpenAIEmbedding(),\n",
    "#     ],\n",
    "#     vector_store=vector_store\n",
    "# )\n",
    "\n",
    "# nodes = pipeline.run(documents=documents, show_progress=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "# from llama_index.core import VectorStoreIndex\n",
    "\n",
    "# # embeds = OpenAIEmbedding(model=\"text-embedding-3-small\", mode=\"similarity\")\n",
    "# # embeds = OpenAIEmbedding(model=\"text-embedding-3-large\", mode=\"similarity\")\n",
    "# embeds = OpenAIEmbedding(model=\"text-embedding-3-large\", mode=\"text_search\")\n",
    "# # embeds = OpenAIEmbedding(model=\"text-embedding-ada-002\", mode=\"similarity\")\n",
    "\n",
    "# # Build index / generate embeddings using OpenAI.\n",
    "# index = VectorStoreIndex(\n",
    "#     nodes=nodes,\n",
    "#     show_progress=True,\n",
    "#     use_async=True,\n",
    "#     storage_context=storage_context,\n",
    "#     embed_model=embeds,\n",
    "#     insert_batch_size=3000,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llama_index.llms.openai import OpenAI\n",
    "\n",
    "# llm = OpenAI(temperature=0, model=\"gpt-3.5-turbo\", max_tokens=None)\n",
    "# query_engine = index.as_query_engine(llm=llm, similarity_top_k=5, embed_model=embeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res = query_engine.query(\"What is the LLaMa model?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res.response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for src in res.source_nodes:\n",
    "#     print(\"Node ID\\t\", src.node_id)\n",
    "#     print(\"Title\\t\", src.metadata[\"title\"])\n",
    "#     print(\"Text\\t\", src.text)\n",
    "#     print(\"Score\\t\", src.score)\n",
    "#     print(\"Metadata\\t\", src.metadata)\n",
    "#     print(\"-_\" * 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load DB from disk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import logging\n",
    "\n",
    "# logger = logging.getLogger(__name__)\n",
    "# logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "\n",
    "# import chromadb\n",
    "# from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "\n",
    "# # Create your index\n",
    "# db2 = chromadb.PersistentClient(path=\"./ai-tutor-dataset\")\n",
    "# chroma_collection = db2.get_or_create_collection(\"ai-tutor-dataset\")\n",
    "# vector_store = ChromaVectorStore(chroma_collection=chroma_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create your index\n",
    "# from llama_index.core import VectorStoreIndex\n",
    "\n",
    "# index = VectorStoreIndex.from_vector_store(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "# from llama_index.llms.openai import OpenAI\n",
    "# from llama_index.core.vector_stores import (\n",
    "#     ExactMatchFilter,\n",
    "#     MetadataFilters,\n",
    "#     MetadataFilter,\n",
    "#     FilterOperator,\n",
    "#     FilterCondition,\n",
    "# )\n",
    "\n",
    "# filters = MetadataFilters(\n",
    "#     filters=[\n",
    "#         MetadataFilter(key=\"source\", value=\"lanchain_course\"),\n",
    "#         MetadataFilter(key=\"source\", value=\"langchain_docs\"),\n",
    "#     ],\n",
    "#     condition=FilterCondition.OR,\n",
    "# )\n",
    "\n",
    "# llm = OpenAI(temperature=0, model=\"gpt-3.5-turbo\", max_tokens=None)\n",
    "# embeds = OpenAIEmbedding(model=\"text-embedding-3-large\", mode=\"text_search\")\n",
    "# # query_engine = index.as_query_engine(\n",
    "# #     llm=llm, similarity_top_k=5, embed_model=embeds, verbose=True, streaming=True, filters=filters\n",
    "# # )\n",
    "# query_engine = index.as_query_engine(\n",
    "#     llm=llm,\n",
    "#     similarity_top_k=5,\n",
    "#     embed_model=embeds,\n",
    "#     verbose=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res = query_engine.query(\"What is the LLama model?\")\n",
    "\n",
    "# # history = \"\"\n",
    "# # for token in res.response_gen:\n",
    "# #     history += token\n",
    "# #     print(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res.response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for src in res.source_nodes:\n",
    "#     print(\"Node ID\\t\", src.node_id)\n",
    "#     print(\"Source\\t\", src.metadata[\"source\"])\n",
    "#     print(\"Title\\t\", src.metadata[\"title\"])\n",
    "#     print(\"Text\\t\", src.text)\n",
    "#     print(\"Score\\t\", src.score)\n",
    "#     print(\"-_\" * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.display import Markdown, display\n",
    "\n",
    "\n",
    "# # define prompt viewing function\n",
    "# def display_prompt_dict(prompts_dict):\n",
    "#     for k, p in prompts_dict.items():\n",
    "#         text_md = f\"**Prompt Key**: {k}<br>\" f\"**Text:** <br>\"\n",
    "#         display(Markdown(text_md))\n",
    "#         print(p.get_template())\n",
    "#         display(Markdown(\"<br><br>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompts_dict = query_engine.get_prompts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display_prompt_dict(prompts_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
